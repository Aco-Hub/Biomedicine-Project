{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by exercises 8\n",
    "from abc import abstractmethod, ABC\n",
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from IPython.display import clear_output\n",
    "#from backbones.fcnet import FCNet\n",
    "from fcnet import FCNet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.prot.utils import get_samples_using_ic, check_min_samples, get_mode_ids, encodings, get_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaTemplate(nn.Module):\n",
    "    def __init__(self, backbone, n_way, n_support):\n",
    "        super(MetaTemplate, self).__init__()\n",
    "        self.n_way = n_way\n",
    "        self.n_support = n_support\n",
    "        self.n_query = -1  # (change depends on input)\n",
    "        self.feature = backbone\n",
    "        self.feat_dim = self.feature.final_feat_dim\n",
    "\n",
    "        # n_way = nb_classes per episode\n",
    "        # n_support = nb_samples per class for support set\n",
    "        # n_query = nb_samples per class for query set\n",
    "        # backbone = feature extractor (embedding network) ie. function f\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_forward(self, x, is_feature=False):\n",
    "        \"\"\"\n",
    "        forward pass, returns score (probabilities for query set)\n",
    "        output is logits for all query set (prob of each class)\n",
    "        first dim of output is n_way * n_query, nb of samples in query set\n",
    "        second dim is prob to belong to each class\n",
    "        x: [n_way, n_support + n_query, **embedding_dim]\n",
    "        out: [n_way * n_query, n_way]\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_forward_loss(self, x):\n",
    "        \"\"\"\n",
    "        takes the episode and compute loss of episode\n",
    "        x: [n_way, n_support + n_query, **embedding_dim]\n",
    "        out: loss (scalar)\n",
    "        \"\"\"\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.feature.forward(x)\n",
    "        return out\n",
    "\n",
    "    def parse_feature(self, x):\n",
    "        '''\n",
    "        create embeddings for support and query set\n",
    "        :param x: [n_way, n_support + n_query, **embedding_dim]\n",
    "        out: z_supp, z_queryÂ¨\n",
    "            z_supp: [n_way, n_support, feat_dim]\n",
    "            z_query: [n_way, n_query, feat_dim]\n",
    "        '''\n",
    "        x = Variable(x.to(self.device))\n",
    "        # reshape x to create one batch of size n_way * (n_support + n_query) and of dim whatever is dim of x\n",
    "        # x is of shape [n_way, n_support + n_query, **embedding_dim] originally, we have to reshape it to pass it to the NN ie. shape (batch_size, dim_size)\n",
    "        # note: x.contigous is used to make sure that is in the same place in mem (more efficient)\n",
    "        x = x.contiguous().view(self.n_way * (self.n_support + self.n_query), * x.size()[2:])\n",
    "        # Compute support and query feature.\n",
    "        z_all = self.forward(x)\n",
    "\n",
    "        # Reverse the transformation to distribute the samples based on the dimensions of their individual categories and flatten the embeddings.\n",
    "        # transformation is the transformation just above to one batch, ie. transform to original shape [n_way, n_support + n_query, **embedding_dim]\n",
    "        z_all = z_all.view(self.n_way, self.n_support + self.n_query, -1)\n",
    "\n",
    "        # Extract the support and query features.\n",
    "        z_support = z_all[:, :self.n_support, :]\n",
    "        z_query = z_all[:, self.n_support:, :]\n",
    "\n",
    "        return z_support, z_query\n",
    "\n",
    "    def correct(self, x):\n",
    "        # Compute the predictions scores.\n",
    "        scores = self.set_forward(x)\n",
    "\n",
    "        # Compute the top1 elements.\n",
    "        topk_scores, topk_labels = scores.data.topk(k=1, dim=1, largest=True, sorted=True)\n",
    "\n",
    "        # Detach the variables (transforming to numpy also detach the tensor)\n",
    "        topk_ind = topk_labels.cpu().numpy()\n",
    "\n",
    "        # Create the category labels for the queries, this is unique for the few shot learning setup\n",
    "        y_query = np.repeat(range(self.n_way), self.n_query)\n",
    "\n",
    "        #>>> np.repeat(range(10), 2)\n",
    "        #array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9])\n",
    "\n",
    "        # Compute number of elements that are correctly classified.\n",
    "        top1_correct = np.sum(topk_ind[:, 0] == y_query)\n",
    "        return float(top1_correct), len(y_query)\n",
    "\n",
    "    def train_loop(self, epoch, train_loader, optimizer):\n",
    "        print_freq = 10\n",
    "\n",
    "        avg_loss = 0\n",
    "        for i, (x, _) in enumerate(train_loader):\n",
    "            self.n_query = x.size(1) - self.n_support\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.set_forward_loss(x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss = avg_loss + loss.item()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                print('Epoch {:d} | Batch {:d}/{:d} | Loss {:f}'.format(epoch, i, len(train_loader),\n",
    "                                                                        avg_loss / float(i + 1)))\n",
    "        return avg_loss/len(train_loader)\n",
    "    \n",
    "    def test_loop(self, epoch, test_loader, record=None, return_std=False):\n",
    "        acc_all = []\n",
    "\n",
    "        iter_num = len(test_loader)\n",
    "        for i, (x, _) in enumerate(test_loader):\n",
    "            self.n_query = x.size(1) - self.n_support\n",
    "            correct_this, count_this = self.correct(x)\n",
    "            acc_all.append(correct_this / count_this * 100)\n",
    "\n",
    "        acc_all = np.asarray(acc_all)\n",
    "        acc_mean = np.mean(acc_all)\n",
    "        acc_std = np.std(acc_all)\n",
    "        print(f'Epoch {epoch} | Test Acc = {acc_mean:4.2f}% +- {1.96 * acc_std / np.sqrt(iter_num):4.2f}%')\n",
    "\n",
    "        if return_std:\n",
    "            return acc_mean, acc_std\n",
    "        else:\n",
    "            return acc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropyLoss, self).__init__()\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs, targets): # inputs: torch.Size([75, 59, 64]), targets: torch.Size([75])\n",
    "        inputs = inputs.view(inputs.size(0), inputs.size(1), -1)\n",
    "        # inputs: torch.Size([75, 59, 64])\n",
    "        log_probs = self.logsoftmax(inputs)\n",
    "\n",
    "        # below = problematic line\n",
    "        # torch zeros (75, 59)\n",
    "        targets = torch.zeros(inputs.size(0), inputs.size(1)).scatter_(1, targets.unsqueeze(1).data.cpu(), 1)\n",
    "        targets = targets.unsqueeze(-1)\n",
    "        targets = targets.cuda()\n",
    "        loss = (- targets * log_probs).mean(0).sum() \n",
    "        return loss / inputs.size(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can network 1 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is modified from https://github.com/blue-blue272/fewshot-CAN\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from methods.meta_template import MetaTemplate\n",
    "\n",
    "\n",
    "class CanNet(MetaTemplate):\n",
    "    def __init__(self, backbone, n_way, n_support, reduction_ratio=6, temperature=0.025, scale_cls=7, num_classes=7195):\n",
    "        super(CanNet, self).__init__(backbone, n_way, n_support)\n",
    "        self.m = self.feat_dim\n",
    "        self.loss_fn = CrossEntropyLoss()\n",
    "        self.num_classes = num_classes\n",
    "        #self.linear = nn.Linear(self.m, n_way)\n",
    "        #self.linear = nn.Linear(self.m, self.num_classes)\n",
    "        self.linear = nn.Linear(1, self.num_classes)\n",
    "        self.fusion_conv = nn.Conv1d(self.feat_dim, 1, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm1d(int(self.feat_dim / reduction_ratio))\n",
    "        self.w1 = nn.Linear(self.m, int(self.m / reduction_ratio))\n",
    "        self.activation = nn.ReLU()\n",
    "        self.w2 = nn.Linear(int(self.m / reduction_ratio), self.m)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.temperature = temperature\n",
    "        self.scale_cls = scale_cls\n",
    "\n",
    "    def set_forward(self, x, is_feature=False):\n",
    "        z_support, z_query = self.parse_feature(x, is_feature)\n",
    "\n",
    "        z_support = z_support.contiguous()\n",
    "        z_proto = z_support.view(self.n_way, self.n_support, self.m).mean(1)  # the shape of z is [n_data, n_dim]\n",
    "        z_query = z_query.contiguous().view(self.n_way * self.n_query, self.m)\n",
    "        z_proto_attention = torch.zeros((self.n_support,self.n_way * self.n_query, self.m)).cuda() # shape: [n_way , n-query, n_dim]\n",
    "        z_query_attention = torch.zeros((self.n_support,self.n_way * self.n_query, self.m)).cuda() # shape: [n_way , n_query, n_dim]\n",
    "        z_proto_attention, z_query_attention = self.cross_attention_module(z_proto, z_query)\n",
    "        ftrain = z_proto_attention # torch.Size([5, 75, 64])\n",
    "        ftest = z_query_attention #torch.Size([5, 75, 64])\n",
    "\n",
    "        ftrain = ftrain.mean(2) # torch.Size([5, 75])\n",
    "\n",
    "        #ftrain = ftrain.T # torch.Size([75, 5])\n",
    "        #ftest = ftest.transpose(0, 1) # torch.Size([75, 5, 64])\n",
    "\n",
    "        if not self.training:\n",
    "            return self.test(ftrain, ftest)\n",
    "    \n",
    "        # Normalize ftest and ftrain along the feature dimension\n",
    "        #ftest_norm = F.normalize(ftest, p=2, dim=1, eps=1e-12) # torch.Size([5, 75, 64]\n",
    "        #ftrain_norm = F.normalize(ftrain, p=2, dim=1, eps=1e-12) # torch.Size([5, 75])    \n",
    "\n",
    "        ftrain = ftrain.unsqueeze(2) # torch.Size([5, 75, 1])\n",
    "        # Calculate cls_scores by taking the matrix product of ftest_norm and ftrain_norm (transposed)\n",
    "        #cls_scores = self.scale_cls * torch.sum(ftest_norm * ftrain_norm, dim=2) # torch.Size([5, 64])\n",
    "        cls_scores = ftest * ftrain\n",
    "        cls_scores = cls_scores.transpose(0, 1)\n",
    "        #cls_scores = cls_scores.view(self.n_way * self.n_query, self.n_way)\n",
    "\n",
    "        return cls_scores, ftest\n",
    "    \n",
    "    def set_forward_loss(self, x, y_true_query):\n",
    "        y_query = torch.from_numpy(np.repeat(range( self.n_way ), self.n_query ))\n",
    "        y_query = Variable(y_query.cuda())\n",
    "\n",
    "\n",
    "        def one_hot(labels_train):\n",
    "            \"\"\"\n",
    "            Turn the labels_train to one-hot encoding.\n",
    "            Args:\n",
    "                labels_train: [batch_size, num_train_examples]\n",
    "            Return:\n",
    "                labels_train_1hot: [batch_size, num_train_examples, K]\n",
    "            \"\"\"\n",
    "            labels_train = labels_train.cpu()\n",
    "            nKnovel = 1 + labels_train.max()\n",
    "            labels_train_1hot_size = list(labels_train.size()) + [nKnovel,]\n",
    "            labels_train_unsqueeze = labels_train.unsqueeze(dim=labels_train.dim())\n",
    "            labels_train_1hot = torch.zeros(labels_train_1hot_size).scatter_(len(labels_train_1hot_size) - 1, labels_train_unsqueeze, 1)\n",
    "            return labels_train_1hot\n",
    "\n",
    "        y_query_one_hot = one_hot(y_query).cuda()\n",
    "\n",
    "        cls_scores, ftest = self.set_forward(x)\n",
    "\n",
    "        # ftest is of shape (5, 75, 64), change it to (1, 75, 64, 5) to be able to do matmul\n",
    "        ftest = ftest.unsqueeze(0) # torch.Size([1, 5, 75, 64])\n",
    "        ftest = ftest.transpose(2, 3) # torch.Size([1, 5, 64, 75])\n",
    "        ftest = ftest.transpose(1, 3) # torch.Size([1, 75, 64, 5])\n",
    "\n",
    "        # computation for the second loss\n",
    "\n",
    "\n",
    "        # this matmul is incorrect should be ftest: (1, 75, 64, 5) and y_query_one_hot: (1, 75, 5, 1)\n",
    "        y_query_one_hot = y_query_one_hot.unsqueeze(0) # torch.Size([1, 5, 75, 5])\n",
    "        y_query_one_hot = y_query_one_hot.unsqueeze(3) # torch.Size([1, 5, 75, 5, 1])\n",
    "        ftest = torch.matmul(ftest, y_query_one_hot) # torch.Size([1, 75, 64, 1])\n",
    "        ftest = ftest.view(-1, self.m) # torch.Size([75, 64])\n",
    "\n",
    "        ftest = ftest.unsqueeze(2) # torch.Size([75, 64, 1])\n",
    "\n",
    "        # goal: ytest = (75, 59, 64)\n",
    "        ytest = self.linear(ftest) # torch.Size([75, 64, 59])\n",
    "        ytest = ytest.transpose(2, 1) # torch.Size([75, 59, 64])\n",
    "\n",
    "        # cls scores: torch.Size([75, 5, 64]), y_query: torch.Size([75])\n",
    "        l1 = self.loss_fn(cls_scores, y_query )\n",
    "\n",
    "        y_true_query = y_true_query.reshape(-1) #torch.Size([75])\n",
    "        l2 = self.loss_fn(ytest, y_true_query)\n",
    "        loss = (l1 + l2) / 2\n",
    "        \n",
    "\n",
    "        return loss\n",
    "\n",
    "    \"\"\"\n",
    "    def set_forward_loss(self, x, y_true_query):\n",
    "        y_query = torch.from_numpy(np.repeat(range( self.n_way ), self.n_query ))\n",
    "        y_query = Variable(y_query.cuda())\n",
    "\n",
    "        cls_scores, ftest = self.set_forward(x)\n",
    "\n",
    "        y_true_query = y_true_query.contiguous().view(-1)\n",
    "        criterion = CrossEntropyLoss()\n",
    "        l1 = criterion(cls_scores, y_query)\n",
    "        loss = l1\n",
    "        \n",
    "\n",
    "        return loss\n",
    "    \"\"\"\n",
    "    def fusion_layer(self, z):\n",
    "        \"\"\"\n",
    "        Generates cross attention map A\n",
    "        :param R: [n_dim,n_dim]\n",
    "        :return: A  [n_dim]\n",
    "        \"\"\"\n",
    "\n",
    "        GAP = torch.mean(z, dim=-2)\n",
    "\n",
    "        w = self.w2(self.activation(self.w1(GAP)))\n",
    "\n",
    "\n",
    "        fusion = z * w.unsqueeze(2)\n",
    "\n",
    "        conv = torch.mean(fusion,dim=-1)\n",
    "\n",
    "        A = self.softmax(conv/self.temperature)\n",
    "\n",
    "        return A\n",
    "\n",
    "    def cross_attention_module(self, z_support, z_query):\n",
    "        \"\"\"\n",
    "        TODO: do this operation for all pairs at once instead of looping, look at base code\n",
    "        Takes 1 support embedding and 1 query embedding and returns cross-attentioned embeddings\n",
    "        :param z_support: [n_dim]\n",
    "        :param z_query: [n_dim]\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        def correlation_layer(z_support, z_query): \n",
    "            \"\"\"\n",
    "            Takes 1 support embedding and 1 query embedding and returns correlation map\n",
    "            ie. P and Q in the paper. P is [P1, P2, ..., Pn] where n is the dimension of the embeddings, same for Q.\n",
    "            :param z_support: [n_dim] ie. P\n",
    "            :param z_query: [n_dim] ie. Q\n",
    "            :return: correlation_map: [n_dim, n_dim]. Note: we use R^q = correlation_map and R^p = correlation_map.T \n",
    "            \"\"\"\n",
    "\n",
    "            # compute cosine similarity between support and query embeddings\n",
    "            #print(\"z_support_shape\", z_support.shape)\n",
    "            #print(\"z_query_shape\", z_query.shape)\n",
    "\n",
    "            #P = z_support / torch.linalg.norm(z_support, ord=2)\n",
    "            #Q = z_query / torch.linalg.norm(z_query, ord=2)\n",
    "            #P = F.normalize(z_support, p=2, dim=-1, eps=1e-12)\n",
    "            #Q = F.normalize(z_query, p=2, dim=-1, eps=1e-12)\n",
    "            P = z_support\n",
    "            Q = z_query\n",
    "\n",
    "            #print(\"P shape after norm\", P.shape)\n",
    "            #print(\"Q shape after norm\", Q.shape)\n",
    "            # P is of dim (n_dim) and Q is of dim (ndim)\n",
    "            # we need to change P to (n_dim, 1) and Q to (ndim, 1)\n",
    "            #P = P.reshape(P.shape, 1)\n",
    "            #Q = Q.reshape(Q.shape, 1)\n",
    "            #print(\"P shape after reshape\", P.shape)\n",
    "            #print(\"Q shape after reshape\", Q.shape)\n",
    "            correlation_map = torch.einsum(\"ij,kl->ikjl\",P,Q)  # dim: [n_dim, n_dim]\n",
    "            #print(\"correlation shape\", correlation_map.shape)\n",
    "\n",
    "            return correlation_map\n",
    "\n",
    "        P_k = z_support\n",
    "        Q_b = z_query\n",
    "\n",
    "        # compute correlation map\n",
    "        R_p = correlation_layer(P_k, Q_b)\n",
    "        R_q = R_p.transpose(2, 3)\n",
    "\n",
    "        # compute fusion layer\n",
    "        A_p = self.fusion_layer(R_p)\n",
    "        A_q = self.fusion_layer(R_q)\n",
    "\n",
    "        \"\"\"\n",
    "        A_p = A_p * P_k\n",
    "        P_bk = A_p + P_k\n",
    "\n",
    "        A_q = A_q * Q_b\n",
    "        Q_bk = A_q + Q_b\n",
    "        \"\"\"\n",
    "        P_bk = P_k.unsqueeze(1) * (1 + A_p)\n",
    "        Q_bk = Q_b.unsqueeze(0) * (1 + A_q)\n",
    "\n",
    "        return P_bk, Q_bk\n",
    "    \n",
    "    def correct(self, x):\n",
    "        scores = self.set_forward(x)\n",
    "        y_query = np.repeat(range(self.n_way), self.n_query)\n",
    "\n",
    "        topk_scores, topk_labels = scores.data.topk(k=1, dim=1, largest=True, sorted=True)\n",
    "        topk_ind = topk_labels.cpu().numpy()\n",
    "        top1_correct = np.sum(topk_ind[:, 0] == y_query)\n",
    "        return float(top1_correct), len(y_query)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    def train_loop(self, epoch, train_loader, optimizer):\n",
    "        print_freq = 10\n",
    "\n",
    "        avg_loss = 0\n",
    "        self.train()\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            y_all = y.cuda()\n",
    "    \n",
    "            # y true query is the global labels for the query set\n",
    "            y_true_query = y_all[:, self.n_support:]\n",
    "\n",
    "            self.n_query = x.size(1) - self.n_support\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.set_forward_loss(x, y_true_query)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss = avg_loss + loss.item()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                print('Epoch {:d} | Batch {:d}/{:d} | Loss {:f}'.format(epoch, i, len(train_loader),\n",
    "                                                                        avg_loss / float(i + 1)))\n",
    "        return avg_loss/len(train_loader)\n",
    "    \n",
    "    def test_loop(self, epoch, test_loader, record=None, return_std=False):\n",
    "        acc_all = []\n",
    "\n",
    "        iter_num = len(test_loader)\n",
    "\n",
    "        self.eval()\n",
    "        for i, (x, _) in enumerate(test_loader):\n",
    "            self.n_query = x.size(1) - self.n_support\n",
    "            correct_this, count_this = self.correct(x)\n",
    "            acc_all.append(correct_this / count_this * 100)\n",
    "\n",
    "        acc_all = np.asarray(acc_all)\n",
    "        acc_mean = np.mean(acc_all)\n",
    "        acc_std = np.std(acc_all)\n",
    "        print(f'Epoch {epoch} | Test Acc = {acc_mean:4.2f}% +- {1.96 * acc_std / np.sqrt(iter_num):4.2f}%')\n",
    "\n",
    "        if return_std:\n",
    "            return acc_mean, acc_std\n",
    "        else:\n",
    "            return acc_mean\n",
    "        \n",
    "    def test(self, ftrain, ftest):\n",
    "        #print(\"using this function\")\n",
    "\n",
    "        ftest = ftest.mean(2)\n",
    "        #ftest = F.normalize(ftest, p=2, dim=1, eps=1e-12)\n",
    "        #ftrain = F.normalize(ftrain, p=2, dim=1, eps=1e-12)\n",
    "        #print(\"ftrain: \", ftrain.shape)\n",
    "        #print(\"ftest: \", ftest.shape)\n",
    "        #ftrain:  torch.Size([5, 75])\n",
    "        #ftest:  torch.Size([5, 75])\n",
    "        \n",
    "\n",
    "        #scores = self.scale_cls * torch.sum(ftest * ftrain, dim=0)\n",
    "        #print(\"scores: \", scores.shape)\n",
    "        scores = ftest * ftrain\n",
    "\n",
    "        scores = scores.view(self.n_way * self.n_query, self.n_way)\n",
    "        \n",
    "        return scores\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is modified from https://github.com/blue-blue272/fewshot-CAN\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from methods.meta_template import MetaTemplate\n",
    "\n",
    "\n",
    "class CanNet(MetaTemplate):\n",
    "    def __init__(self, backbone, n_way, n_support, reduction_ratio=6, temperature=0.025, scale_cls=7, num_classes=7195):\n",
    "        super(CanNet, self).__init__(backbone, n_way, n_support)\n",
    "        self.m = self.feat_dim\n",
    "        self.loss_fn = CrossEntropyLoss()\n",
    "        self.num_classes = num_classes\n",
    "        #self.linear = nn.Linear(self.m, n_way)\n",
    "        #self.linear = nn.Linear(self.m, self.num_classes)\n",
    "        self.linear = nn.Linear(1, self.num_classes)\n",
    "        self.fusion_conv = nn.Conv1d(self.feat_dim, 1, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm1d(int(self.feat_dim / reduction_ratio))\n",
    "        self.w1 = nn.Linear(self.m, int(self.m / reduction_ratio))\n",
    "        self.activation = nn.ReLU()\n",
    "        self.w2 = nn.Linear(int(self.m / reduction_ratio), self.m)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.temperature = temperature\n",
    "        self.scale_cls = scale_cls\n",
    "\n",
    "    def set_forward(self, x, is_feature=False):\n",
    "        z_support, z_query = self.parse_feature(x, is_feature)\n",
    "\n",
    "        z_support = z_support.contiguous()\n",
    "        z_proto = z_support.view(self.n_way, self.n_support, self.m).mean(1)  # the shape of z is [n_data, n_dim]\n",
    "        z_query = z_query.contiguous().view(self.n_way * self.n_query, self.m)\n",
    "        z_proto_attention = torch.zeros((self.n_support,self.n_way * self.n_query, self.m)).cuda() # shape: [n_way , n-query, n_dim]\n",
    "        z_query_attention = torch.zeros((self.n_support,self.n_way * self.n_query, self.m)).cuda() # shape: [n_way , n_query, n_dim]\n",
    "        z_proto_attention, z_query_attention = self.cross_attention_module(z_proto, z_query)\n",
    "        ftrain = z_proto_attention # torch.Size([5, 75, 64])\n",
    "        ftest = z_query_attention #torch.Size([5, 75, 64])\n",
    "\n",
    "        #ftrain = ftrain.mean(2) # torch.Size([5, 75])\n",
    "\n",
    "        #ftrain = ftrain.T # torch.Size([75, 5])\n",
    "        #ftest = ftest.transpose(0, 1) # torch.Size([75, 5, 64])\n",
    "\n",
    "        if not self.training:\n",
    "            return self.test(ftrain, ftest)\n",
    "    \n",
    "        # Normalize ftest and ftrain along the feature dimension\n",
    "        ftest_norm = F.normalize(ftest, p=2, dim=1, eps=1e-12) # torch.Size([5, 75, 64]\n",
    "        ftrain_norm = F.normalize(ftrain, p=2, dim=1, eps=1e-12) # torch.Size([5, 75])    \n",
    "\n",
    "        #ftrain_norm = ftrain_norm.unsqueeze(2) # torch.Size([5, 75, 1])\n",
    "        # Calculate cls_scores by taking the matrix product of ftest_norm and ftrain_norm (transposed)\n",
    "        #cls_scores = self.scale_cls * torch.sum(ftest_norm * ftrain_norm, dim=2) # torch.Size([5, 64])\n",
    "        cls_scores = ftest_norm * ftrain_norm\n",
    "        cls_scores = 7 * torch.sum(cls_scores, dim=2)\n",
    "        cls_scores = cls_scores.transpose(0, 1)\n",
    "        #cls_scores = cls_scores.view(self.n_way * self.n_query, self.n_way)\n",
    "\n",
    "        return cls_scores, ftest\n",
    "    \n",
    "    def set_forward_loss(self, x, y_true_query):\n",
    "        y_query = torch.from_numpy(np.repeat(range( self.n_way ), self.n_query ))\n",
    "        y_query = Variable(y_query.cuda())\n",
    "\n",
    "\n",
    "        def one_hot(labels_train):\n",
    "            \"\"\"\n",
    "            Turn the labels_train to one-hot encoding.\n",
    "            Args:\n",
    "                labels_train: [batch_size, num_train_examples]\n",
    "            Return:\n",
    "                labels_train_1hot: [batch_size, num_train_examples, K]\n",
    "            \"\"\"\n",
    "            labels_train = labels_train.cpu()\n",
    "            nKnovel = 1 + labels_train.max()\n",
    "            labels_train_1hot_size = list(labels_train.size()) + [nKnovel,]\n",
    "            labels_train_unsqueeze = labels_train.unsqueeze(dim=labels_train.dim())\n",
    "            labels_train_1hot = torch.zeros(labels_train_1hot_size).scatter_(len(labels_train_1hot_size) - 1, labels_train_unsqueeze, 1)\n",
    "            return labels_train_1hot\n",
    "\n",
    "        y_query_one_hot = one_hot(y_query).cuda()\n",
    "\n",
    "        cls_scores, ftest = self.set_forward(x)\n",
    "\n",
    "        # ftest is of shape (5, 75, 64), change it to (1, 75, 64, 5) to be able to do matmul\n",
    "        ftest = ftest.unsqueeze(0) # torch.Size([1, 5, 75, 64])\n",
    "        ftest = ftest.transpose(2, 3) # torch.Size([1, 5, 64, 75])\n",
    "        ftest = ftest.transpose(1, 3) # torch.Size([1, 75, 64, 5])\n",
    "\n",
    "        # computation for the second loss\n",
    "\n",
    "\n",
    "        # this matmul is incorrect should be ftest: (1, 75, 64, 5) and y_query_one_hot: (1, 75, 5, 1)\n",
    "        y_query_one_hot = y_query_one_hot.unsqueeze(0) # torch.Size([1, 5, 75, 5])\n",
    "        y_query_one_hot = y_query_one_hot.unsqueeze(3) # torch.Size([1, 5, 75, 5, 1])\n",
    "        ftest = torch.matmul(ftest, y_query_one_hot) # torch.Size([1, 75, 64, 1])\n",
    "        ftest = ftest.view(-1, self.m) # torch.Size([75, 64])\n",
    "\n",
    "        ftest = ftest.unsqueeze(2) # torch.Size([75, 64, 1])\n",
    "\n",
    "        # goal: ytest = (75, 59, 64)\n",
    "        ytest = self.linear(ftest) # torch.Size([75, 64, 59])\n",
    "        ytest = ytest.transpose(2, 1) # torch.Size([75, 59, 64])\n",
    "\n",
    "        # cls scores: torch.Size([75, 5, 64]), y_query: torch.Size([75])\n",
    "        l1 = self.loss_fn(cls_scores, y_query )\n",
    "\n",
    "        y_true_query = y_true_query.reshape(-1) #torch.Size([75])\n",
    "        l2 = self.loss_fn(ytest, y_true_query)\n",
    "        loss = (l1 + l2) / 2\n",
    "        loss = l2\n",
    "\n",
    "        return loss\n",
    "\n",
    "    \"\"\"\n",
    "    def set_forward_loss(self, x, y_true_query):\n",
    "        y_query = torch.from_numpy(np.repeat(range( self.n_way ), self.n_query ))\n",
    "        y_query = Variable(y_query.cuda())\n",
    "\n",
    "        cls_scores, ftest = self.set_forward(x)\n",
    "\n",
    "        y_true_query = y_true_query.contiguous().view(-1)\n",
    "        criterion = CrossEntropyLoss()\n",
    "        l1 = criterion(cls_scores, y_query)\n",
    "        loss = l1\n",
    "        \n",
    "\n",
    "        return loss\n",
    "    \"\"\"\n",
    "    def fusion_layer(self, z):\n",
    "        \"\"\"\n",
    "        Generates cross attention map A\n",
    "        :param R: [n_dim,n_dim]\n",
    "        :return: A  [n_dim]\n",
    "        \"\"\"\n",
    "\n",
    "        GAP = torch.mean(z, dim=-2)\n",
    "\n",
    "        w = self.w2(self.activation(self.w1(GAP)))\n",
    "\n",
    "\n",
    "        fusion = z * w.unsqueeze(2)\n",
    "\n",
    "        conv = torch.mean(fusion,dim=-1)\n",
    "\n",
    "        A = self.softmax(conv/self.temperature)\n",
    "\n",
    "        return A\n",
    "\n",
    "    def cross_attention_module(self, z_support, z_query):\n",
    "        \"\"\"\n",
    "        TODO: do this operation for all pairs at once instead of looping, look at base code\n",
    "        Takes 1 support embedding and 1 query embedding and returns cross-attentioned embeddings\n",
    "        :param z_support: [n_dim]\n",
    "        :param z_query: [n_dim]\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        def correlation_layer(z_support, z_query): \n",
    "            \"\"\"\n",
    "            Takes 1 support embedding and 1 query embedding and returns correlation map\n",
    "            ie. P and Q in the paper. P is [P1, P2, ..., Pn] where n is the dimension of the embeddings, same for Q.\n",
    "            :param z_support: [n_dim] ie. P\n",
    "            :param z_query: [n_dim] ie. Q\n",
    "            :return: correlation_map: [n_dim, n_dim]. Note: we use R^q = correlation_map and R^p = correlation_map.T \n",
    "            \"\"\"\n",
    "\n",
    "            # compute cosine similarity between support and query embeddings\n",
    "            #print(\"z_support_shape\", z_support.shape)\n",
    "            #print(\"z_query_shape\", z_query.shape)\n",
    "\n",
    "            #P = z_support / torch.linalg.norm(z_support, ord=2)\n",
    "            #Q = z_query / torch.linalg.norm(z_query, ord=2)\n",
    "            P = F.normalize(z_support, p=2, dim=-1, eps=1e-12)\n",
    "            Q = F.normalize(z_query, p=2, dim=-1, eps=1e-12)\n",
    "            P = z_support\n",
    "            Q = z_query\n",
    "\n",
    "            #print(\"P shape after norm\", P.shape)\n",
    "            #print(\"Q shape after norm\", Q.shape)\n",
    "            # P is of dim (n_dim) and Q is of dim (ndim)\n",
    "            # we need to change P to (n_dim, 1) and Q to (ndim, 1)\n",
    "            #P = P.reshape(P.shape, 1)\n",
    "            #Q = Q.reshape(Q.shape, 1)\n",
    "            #print(\"P shape after reshape\", P.shape)\n",
    "            #print(\"Q shape after reshape\", Q.shape)\n",
    "            correlation_map = torch.einsum(\"ij,kl->ikjl\",P,Q)  # dim: [n_dim, n_dim]\n",
    "            #print(\"correlation shape\", correlation_map.shape)\n",
    "\n",
    "            return correlation_map\n",
    "\n",
    "        P_k = z_support\n",
    "        Q_b = z_query\n",
    "\n",
    "        # compute correlation map\n",
    "        R_p = correlation_layer(P_k, Q_b)\n",
    "        R_q = R_p.transpose(2, 3)\n",
    "\n",
    "        # compute fusion layer\n",
    "        A_p = self.fusion_layer(R_p)\n",
    "        A_q = self.fusion_layer(R_q)\n",
    "\n",
    "        \"\"\"\n",
    "        A_p = A_p * P_k\n",
    "        P_bk = A_p + P_k\n",
    "\n",
    "        A_q = A_q * Q_b\n",
    "        Q_bk = A_q + Q_b\n",
    "        \"\"\"\n",
    "        P_bk = P_k.unsqueeze(1) * (1 + A_p)\n",
    "        Q_bk = Q_b.unsqueeze(0) * (1 + A_q)\n",
    "\n",
    "        return P_bk, Q_bk\n",
    "    \n",
    "    def correct(self, x):\n",
    "        scores = self.set_forward(x)\n",
    "        y_query = np.repeat(range(self.n_way), self.n_query)\n",
    "\n",
    "        topk_scores, topk_labels = scores.data.topk(k=1, dim=1, largest=True, sorted=True)\n",
    "        topk_ind = topk_labels.cpu().numpy()\n",
    "        top1_correct = np.sum(topk_ind[:, 0] == y_query)\n",
    "        return float(top1_correct), len(y_query)\n",
    "    \n",
    "    \n",
    "    def train_loop(self, epoch, train_loader, optimizer):\n",
    "        print_freq = 10\n",
    "\n",
    "        avg_loss = 0\n",
    "        self.train()\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            y_all = y.cuda()\n",
    "    \n",
    "            # y true query is the global labels for the query set\n",
    "            y_true_query = y_all[:, self.n_support:]\n",
    "\n",
    "            self.n_query = x.size(1) - self.n_support\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.set_forward_loss(x, y_true_query)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss = avg_loss + loss.item()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                print('Epoch {:d} | Batch {:d}/{:d} | Loss {:f}'.format(epoch, i, len(train_loader),\n",
    "                                                                        avg_loss / float(i + 1)))\n",
    "        return avg_loss/len(train_loader)\n",
    "    \n",
    "    def test_loop(self, epoch, test_loader, record=None, return_std=False):\n",
    "        acc_all = []\n",
    "\n",
    "        iter_num = len(test_loader)\n",
    "\n",
    "        self.eval()\n",
    "        for i, (x, _) in enumerate(test_loader):\n",
    "            self.n_query = x.size(1) - self.n_support\n",
    "            correct_this, count_this = self.correct(x)\n",
    "            acc_all.append(correct_this / count_this * 100)\n",
    "\n",
    "        acc_all = np.asarray(acc_all)\n",
    "        acc_mean = np.mean(acc_all)\n",
    "        acc_std = np.std(acc_all)\n",
    "        print(f'Epoch {epoch} | Test Acc = {acc_mean:4.2f}% +- {1.96 * acc_std / np.sqrt(iter_num):4.2f}%')\n",
    "\n",
    "        if return_std:\n",
    "            return acc_mean, acc_std\n",
    "        else:\n",
    "            return acc_mean\n",
    "        \n",
    "    def test(self, ftrain, ftest):\n",
    "        #print(\"using this function\")\n",
    "\n",
    "        #ftest = ftest.mean(2)\n",
    "        ftest = F.normalize(ftest, p=2, dim=2, eps=1e-12)\n",
    "        ftrain = F.normalize(ftrain, p=2, dim=2, eps=1e-12)\n",
    "        #print(\"ftrain: \", ftrain.shape)\n",
    "        #print(\"ftest: \", ftest.shape)\n",
    "        #ftrain:  torch.Size([5, 75])\n",
    "        #ftest:  torch.Size([5, 75])\n",
    "        \n",
    "\n",
    "        #scores = self.scale_cls * torch.sum(ftest * ftrain, dim=0)\n",
    "        #print(\"scores: \", scores.shape)\n",
    "        scores = ftest * ftrain\n",
    "        scores = 7 * torch.sum(scores, dim=2)\n",
    "\n",
    "        scores = scores.view(self.n_way * self.n_query, self.n_way)\n",
    "        \n",
    "        return scores\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Special class for few-shot dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.check_init()\n",
    "\n",
    "    def check_init(self):\n",
    "        \"\"\"\n",
    "        Convenience function to check that the FewShotDataset is properly configured.\n",
    "        \"\"\"\n",
    "        required_attrs = ['_dataset_name', '_data_dir']\n",
    "        for attr in required_attrs:\n",
    "            if not hasattr(self, attr):\n",
    "                raise ValueError(f'FewShotDataset must have attribute {attr}.')\n",
    "\n",
    "        if not os.path.exists(self._data_dir):\n",
    "            raise ValueError(\n",
    "                f'{self._data_dir} does not exist yet. Please generate/download the dataset first.')\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def __getitem__(self, i):\n",
    "        return NotImplemented\n",
    "\n",
    "    @abstractmethod\n",
    "    def __len__(self):\n",
    "        return NotImplemented\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def dim(self):\n",
    "        return NotImplemented\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_data_loader(self, mode='train') -> DataLoader:\n",
    "        return NotImplemented\n",
    "\n",
    "    @property\n",
    "    def dataset_name(self):\n",
    "        \"\"\"\n",
    "        A string that identifies the dataset, e.g., 'swissprot'\n",
    "        \"\"\"\n",
    "        return self._dataset_name\n",
    "\n",
    "    @property\n",
    "    def data_dir(self):\n",
    "        return self._data_dir\n",
    "\n",
    "    def initialize_data_dir(self, root_dir):\n",
    "        os.makedirs(root_dir, exist_ok=True)\n",
    "        #self._data_dir = os.path.join(root_dir, self._dataset_name)\n",
    "        self._data_dir = \"data/swissprot\"\n",
    "\n",
    "class SPDataset(FewShotDataset, ABC):\n",
    "    _dataset_name = 'swissprot'\n",
    "\n",
    "    def load_swissprot(self, level = 5, mode='train', min_samples = 20):\n",
    "        samples = get_samples_using_ic(root = self.data_dir)\n",
    "        samples = check_min_samples(samples, min_samples)\n",
    "        unique_ids = set(get_mode_ids(samples)[mode])\n",
    "        return [sample for sample in samples if sample.annot in unique_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROTDIM = 1280\n",
    "\n",
    "class SubDataset(Dataset):\n",
    "    def __init__(self, samples, data_dir):\n",
    "        self.samples = samples\n",
    "        self.encoder = encodings(data_dir)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        sample = self.samples[i]\n",
    "        return sample.input_seq, self.encoder[sample.annot]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return PROTDIM\n",
    "\n",
    "class EpisodicBatchSampler(object):\n",
    "    def __init__(self, n_classes, n_way, n_episodes):\n",
    "        self.n_classes = n_classes\n",
    "        self.n_way = n_way\n",
    "        self.n_episodes = n_episodes\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_episodes\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(self.n_episodes):\n",
    "            yield torch.randperm(self.n_classes)[:self.n_way]\n",
    "\n",
    "class SPSetDataset(SPDataset):\n",
    "    def __init__(self, n_way, n_support, n_query, n_episode=100, root='./data', mode='train'):\n",
    "        self.initialize_data_dir(root)\n",
    "\n",
    "        self.n_way = n_way\n",
    "        self.n_episode = n_episode\n",
    "        min_samples = n_support + n_query\n",
    "        self.encoder = encodings(self.data_dir)\n",
    "\n",
    "        # check if samples_all.pkl exists\n",
    "        if os.path.exists('samples_all.pkl'):\n",
    "            # load samples_all using pickle\n",
    "            with open('samples_all.pkl', 'rb') as f:\n",
    "                samples_all = pickle.load(f)\n",
    "        else:\n",
    "            samples_all = self.load_swissprot(mode = mode, min_samples = min_samples)\n",
    "\n",
    "            # save samples_all using pickle\n",
    "            with open('samples_all.pkl', 'wb') as f:\n",
    "                pickle.dump(samples_all, f)\n",
    "            \n",
    "\n",
    "\n",
    "        self.categories = get_ids(samples_all) # Unique annotations\n",
    "        self.x_dim = PROTDIM\n",
    "\n",
    "        self.sub_dataloader = []\n",
    "\n",
    "        sub_data_loader_params = dict(batch_size=min_samples,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=0,  # use main thread only or may receive multiple batches\n",
    "                                      pin_memory=False)\n",
    "\n",
    "        # Create the sub datasets for each annotation of the categories and collect all the dataloaders in `self.sub_dataloader`.\n",
    "        for annotation in self.categories:\n",
    "            samples = [sample for sample in samples_all if sample.annot == annotation]\n",
    "            sub_dataset = SubDataset(samples, self.data_dir)\n",
    "            self.sub_dataloader.append(torch.utils.data.DataLoader(sub_dataset, **sub_data_loader_params))\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return next(iter(self.sub_dataloader[i]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.categories)\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self.x_dim\n",
    "\n",
    "    def get_data_loader(self) -> DataLoader:\n",
    "        sampler = EpisodicBatchSampler(len(self), self.n_way, self.n_episode)\n",
    "        data_loader_params = dict(batch_sampler=sampler, num_workers=0, pin_memory=True)\n",
    "        \n",
    "        # check if data_loader.pkl exists\n",
    "        if os.path.exists('data_loader.pkl'):\n",
    "            # load data_loader using pickle\n",
    "            with open('data_loader.pkl', 'rb') as f:\n",
    "                data_loader = pickle.load(f)\n",
    "        else:\n",
    "            data_loader = torch.utils.data.DataLoader(self, **data_loader_params)\n",
    "        \n",
    "            # save data_loader using pickle\n",
    "            with open('data_loader.pkl', 'wb') as f:\n",
    "                pickle.dump(data_loader, f)\n",
    "        return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_way, n_support, n_query, n_train_episode):\n",
    "    # Load train dataset. Remember to use make use of functions defined in the `SPSetDataset`.\n",
    "    \n",
    "    if os.path.exists('train_dataset.pkl'):\n",
    "        # load train_dataset using pickle\n",
    "        with open('train_dataset.pkl', 'rb') as f:\n",
    "            train_dataset = pickle.load(f)\n",
    "    else:\n",
    "        train_dataset = SPSetDataset(n_way, n_support, n_query, n_episode=n_train_episode, root='./data', mode='train')\n",
    "        # save as pickle\n",
    "        with open('train_dataset.pkl', 'wb') as f:\n",
    "            pickle.dump(train_dataset, f)\n",
    "    train_loader = train_dataset.get_data_loader()\n",
    "\n",
    "    # Load test dataset. Remember to use make use of functions defined in the `SPSetDataset`.\n",
    "    if os.path.exists('test_dataset.pkl'):\n",
    "        # load test_dataset using pickle\n",
    "        with open('test_dataset.pkl', 'rb') as f:\n",
    "            test_dataset = pickle.load(f)\n",
    "    else:\n",
    "        test_dataset = SPSetDataset(n_way, n_support, n_query, n_episode=100, root='./data', mode='test')\n",
    "        # save as pickle\n",
    "        with open('test_dataset.pkl', 'wb') as f:\n",
    "            pickle.dump(test_dataset, f)\n",
    "    test_loader =  test_dataset.get_data_loader()\n",
    "\n",
    "    # Initialize a fully connected network `FCNet` in `fcnet.py` with two hidden layers of 512 units each as feature extractor.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    backbone = FCNet(train_dataset.dim).to(device)\n",
    "\n",
    "\n",
    "    # Initialize model using the backbone and the optimizer.\n",
    "    model = CanNet(backbone, n_way, n_support).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    test_accs = []; train_losses = []\n",
    "    for epoch in range(1000):\n",
    "        model.train()\n",
    "\n",
    "        # Implement training of the model. Remember to make use of functions defined in the `ProtoNet` and `MetaTemplate` class.\n",
    "        epoch_loss = model.train_loop(epoch, train_loader, optimizer)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # Evaluate test performance for epoch. Remember to make use of functions defined in the `ProtoNet` and `MetaTemplate` class.\n",
    "        test_acc = model.test_loop(epoch, test_loader)\n",
    "        test_accs.append(test_acc)\n",
    "        print(f'Epoch {epoch} | Train Loss {epoch_loss} | Test Acc {test_acc}')\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 2.5))\n",
    "    ax1.plot(range(len(train_losses)), train_losses)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Train Loss')\n",
    "    ax1.grid()\n",
    "\n",
    "    ax2.plot(range(len(test_accs)), test_accs)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Test Accuracy')\n",
    "    ax2.grid()\n",
    "    fig.suptitle(f\"n_way={n_way}, n_support={n_support}, n_query={n_query}, n_train_episode={n_train_episode}\")\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_way': 5, 'n_support': 5, 'n_query': 15, 'n_train_episode': 5}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Batch 0/5 | Loss 9.148237\n",
      "Epoch 0 | Test Acc = 24.27% +- 4.52%\n",
      "Epoch 0 | Train Loss 9.121541786193848 | Test Acc 24.26666666666667\n",
      "Epoch 1 | Batch 0/5 | Loss 9.239084\n",
      "Epoch 1 | Test Acc = 20.80% +- 3.19%\n",
      "Epoch 1 | Train Loss 9.317740440368652 | Test Acc 20.800000000000004\n",
      "Epoch 2 | Batch 0/5 | Loss 8.491815\n",
      "Epoch 2 | Test Acc = 21.60% +- 2.71%\n",
      "Epoch 2 | Train Loss 9.173041534423827 | Test Acc 21.6\n",
      "Epoch 3 | Batch 0/5 | Loss 8.804955\n",
      "Epoch 3 | Test Acc = 19.73% +- 2.71%\n",
      "Epoch 3 | Train Loss 9.325857543945313 | Test Acc 19.733333333333334\n",
      "Epoch 4 | Batch 0/5 | Loss 8.721394\n",
      "Epoch 4 | Test Acc = 17.87% +- 2.29%\n",
      "Epoch 4 | Train Loss 9.301127433776855 | Test Acc 17.866666666666667\n",
      "Epoch 5 | Batch 0/5 | Loss 9.187111\n",
      "Epoch 5 | Test Acc = 18.40% +- 2.90%\n",
      "Epoch 5 | Train Loss 9.257326698303222 | Test Acc 18.400000000000002\n",
      "Epoch 6 | Batch 0/5 | Loss 9.023845\n",
      "Epoch 6 | Test Acc = 24.00% +- 2.09%\n",
      "Epoch 6 | Train Loss 9.091049575805664 | Test Acc 24.0\n",
      "Epoch 7 | Batch 0/5 | Loss 9.224272\n",
      "Epoch 7 | Test Acc = 19.73% +- 1.36%\n",
      "Epoch 7 | Train Loss 9.200143432617187 | Test Acc 19.733333333333338\n",
      "Epoch 8 | Batch 0/5 | Loss 8.738708\n",
      "Epoch 8 | Test Acc = 18.13% +- 2.17%\n",
      "Epoch 8 | Train Loss 9.162560081481933 | Test Acc 18.133333333333333\n",
      "Epoch 9 | Batch 0/5 | Loss 8.652802\n",
      "Epoch 9 | Test Acc = 18.67% +- 1.65%\n",
      "Epoch 9 | Train Loss 8.894764709472657 | Test Acc 18.666666666666668\n",
      "Epoch 10 | Batch 0/5 | Loss 9.180414\n",
      "Epoch 10 | Test Acc = 19.20% +- 4.41%\n",
      "Epoch 10 | Train Loss 9.198147964477538 | Test Acc 19.2\n",
      "Epoch 11 | Batch 0/5 | Loss 8.539557\n",
      "Epoch 11 | Test Acc = 16.53% +- 3.95%\n",
      "Epoch 11 | Train Loss 9.010533714294434 | Test Acc 16.53333333333333\n",
      "Epoch 12 | Batch 0/5 | Loss 9.016376\n",
      "Epoch 12 | Test Acc = 18.67% +- 5.07%\n",
      "Epoch 12 | Train Loss 9.050380325317382 | Test Acc 18.666666666666664\n",
      "Epoch 13 | Batch 0/5 | Loss 8.937337\n",
      "Epoch 13 | Test Acc = 17.07% +- 5.50%\n",
      "Epoch 13 | Train Loss 9.134254455566406 | Test Acc 17.06666666666667\n",
      "Epoch 14 | Batch 0/5 | Loss 9.098509\n",
      "Epoch 14 | Test Acc = 17.60% +- 2.01%\n",
      "Epoch 14 | Train Loss 8.989145278930664 | Test Acc 17.6\n",
      "Epoch 15 | Batch 0/5 | Loss 9.419856\n",
      "Epoch 15 | Test Acc = 18.40% +- 1.55%\n",
      "Epoch 15 | Train Loss 9.07093849182129 | Test Acc 18.4\n",
      "Epoch 16 | Batch 0/5 | Loss 8.891825\n",
      "Epoch 16 | Test Acc = 20.53% +- 3.81%\n",
      "Epoch 16 | Train Loss 9.168486785888671 | Test Acc 20.53333333333333\n",
      "Epoch 17 | Batch 0/5 | Loss 8.931010\n",
      "Epoch 17 | Test Acc = 19.47% +- 3.01%\n",
      "Epoch 17 | Train Loss 8.794356155395509 | Test Acc 19.46666666666667\n",
      "Epoch 18 | Batch 0/5 | Loss 8.783064\n",
      "Epoch 18 | Test Acc = 20.53% +- 0.93%\n",
      "Epoch 18 | Train Loss 8.929183006286621 | Test Acc 20.533333333333335\n",
      "Epoch 19 | Batch 0/5 | Loss 9.103844\n",
      "Epoch 19 | Test Acc = 23.73% +- 2.27%\n",
      "Epoch 19 | Train Loss 9.16402587890625 | Test Acc 23.733333333333338\n",
      "Epoch 20 | Batch 0/5 | Loss 8.701622\n",
      "Epoch 20 | Test Acc = 20.27% +- 3.50%\n",
      "Epoch 20 | Train Loss 8.911148643493652 | Test Acc 20.26666666666667\n",
      "Epoch 21 | Batch 0/5 | Loss 9.016007\n",
      "Epoch 21 | Test Acc = 18.67% +- 4.25%\n",
      "Epoch 21 | Train Loss 8.90890884399414 | Test Acc 18.666666666666668\n",
      "Epoch 22 | Batch 0/5 | Loss 9.156821\n",
      "Epoch 22 | Test Acc = 18.93% +- 1.72%\n",
      "Epoch 22 | Train Loss 9.125558853149414 | Test Acc 18.933333333333334\n",
      "Epoch 23 | Batch 0/5 | Loss 9.043388\n",
      "Epoch 23 | Test Acc = 20.27% +- 5.40%\n",
      "Epoch 23 | Train Loss 9.025338745117187 | Test Acc 20.26666666666667\n",
      "Epoch 24 | Batch 0/5 | Loss 8.833393\n",
      "Epoch 24 | Test Acc = 20.80% +- 3.19%\n",
      "Epoch 24 | Train Loss 8.947961616516114 | Test Acc 20.8\n",
      "Epoch 25 | Batch 0/5 | Loss 9.318407\n",
      "Epoch 25 | Test Acc = 17.87% +- 2.17%\n",
      "Epoch 25 | Train Loss 9.009510040283203 | Test Acc 17.866666666666667\n",
      "Epoch 26 | Batch 0/5 | Loss 9.019460\n",
      "Epoch 26 | Test Acc = 20.53% +- 2.29%\n",
      "Epoch 26 | Train Loss 9.023369789123535 | Test Acc 20.53333333333334\n",
      "Epoch 27 | Batch 0/5 | Loss 8.966352\n",
      "Epoch 27 | Test Acc = 19.20% +- 2.04%\n",
      "Epoch 27 | Train Loss 9.072637748718261 | Test Acc 19.2\n",
      "Epoch 28 | Batch 0/5 | Loss 8.799995\n",
      "Epoch 28 | Test Acc = 22.13% +- 2.17%\n",
      "Epoch 28 | Train Loss 8.892197418212891 | Test Acc 22.133333333333336\n",
      "Epoch 29 | Batch 0/5 | Loss 9.024053\n",
      "Epoch 29 | Test Acc = 18.40% +- 3.73%\n",
      "Epoch 29 | Train Loss 8.835076904296875 | Test Acc 18.4\n",
      "Epoch 30 | Batch 0/5 | Loss 8.647177\n",
      "Epoch 30 | Test Acc = 20.27% +- 2.38%\n",
      "Epoch 30 | Train Loss 8.792104148864746 | Test Acc 20.26666666666667\n",
      "Epoch 31 | Batch 0/5 | Loss 8.512044\n",
      "Epoch 31 | Test Acc = 21.07% +- 1.36%\n",
      "Epoch 31 | Train Loss 8.903163528442382 | Test Acc 21.06666666666667\n",
      "Epoch 32 | Batch 0/5 | Loss 8.939527\n",
      "Epoch 32 | Test Acc = 24.00% +- 6.10%\n",
      "Epoch 32 | Train Loss 8.95911521911621 | Test Acc 24.0\n",
      "Epoch 33 | Batch 0/5 | Loss 8.623397\n",
      "Epoch 33 | Test Acc = 20.80% +- 3.35%\n",
      "Epoch 33 | Train Loss 8.914602661132813 | Test Acc 20.8\n",
      "Epoch 34 | Batch 0/5 | Loss 8.584741\n",
      "Epoch 34 | Test Acc = 19.73% +- 1.87%\n",
      "Epoch 34 | Train Loss 8.758941268920898 | Test Acc 19.73333333333333\n",
      "Epoch 35 | Batch 0/5 | Loss 9.101137\n",
      "Epoch 35 | Test Acc = 22.40% +- 2.01%\n",
      "Epoch 35 | Train Loss 8.824638175964356 | Test Acc 22.4\n",
      "Epoch 36 | Batch 0/5 | Loss 8.749766\n",
      "Epoch 36 | Test Acc = 20.53% +- 1.40%\n",
      "Epoch 36 | Train Loss 8.76811580657959 | Test Acc 20.53333333333333\n",
      "Epoch 37 | Batch 0/5 | Loss 8.470856\n",
      "Epoch 37 | Test Acc = 22.40% +- 4.34%\n",
      "Epoch 37 | Train Loss 8.78532600402832 | Test Acc 22.400000000000002\n",
      "Epoch 38 | Batch 0/5 | Loss 8.959453\n",
      "Epoch 38 | Test Acc = 22.40% +- 1.72%\n",
      "Epoch 38 | Train Loss 8.615887641906738 | Test Acc 22.4\n",
      "Epoch 39 | Batch 0/5 | Loss 9.260094\n",
      "Epoch 39 | Test Acc = 19.47% +- 0.93%\n",
      "Epoch 39 | Train Loss 8.987457656860352 | Test Acc 19.46666666666667\n",
      "Epoch 40 | Batch 0/5 | Loss 8.931101\n",
      "Epoch 40 | Test Acc = 20.00% +- 4.96%\n",
      "Epoch 40 | Train Loss 8.846919059753418 | Test Acc 20.0\n",
      "Epoch 41 | Batch 0/5 | Loss 8.368792\n",
      "Epoch 41 | Test Acc = 21.60% +- 5.60%\n",
      "Epoch 41 | Train Loss 8.617907905578614 | Test Acc 21.6\n",
      "Epoch 42 | Batch 0/5 | Loss 8.860015\n",
      "Epoch 42 | Test Acc = 14.93% +- 3.42%\n",
      "Epoch 42 | Train Loss 8.896406745910644 | Test Acc 14.933333333333337\n",
      "Epoch 43 | Batch 0/5 | Loss 8.447126\n",
      "Epoch 43 | Test Acc = 23.47% +- 6.55%\n",
      "Epoch 43 | Train Loss 8.446830558776856 | Test Acc 23.46666666666667\n",
      "Epoch 44 | Batch 0/5 | Loss 8.591054\n",
      "Epoch 44 | Test Acc = 18.67% +- 3.22%\n",
      "Epoch 44 | Train Loss 8.719449234008788 | Test Acc 18.666666666666664\n",
      "Epoch 45 | Batch 0/5 | Loss 9.172041\n",
      "Epoch 45 | Test Acc = 17.33% +- 2.09%\n",
      "Epoch 45 | Train Loss 8.924084281921386 | Test Acc 17.333333333333336\n",
      "Epoch 46 | Batch 0/5 | Loss 8.645390\n",
      "Epoch 46 | Test Acc = 19.73% +- 2.71%\n",
      "Epoch 46 | Train Loss 8.795176124572754 | Test Acc 19.73333333333333\n",
      "Epoch 47 | Batch 0/5 | Loss 8.577347\n",
      "Epoch 47 | Test Acc = 21.07% +- 2.27%\n",
      "Epoch 47 | Train Loss 8.610572624206544 | Test Acc 21.066666666666663\n",
      "Epoch 48 | Batch 0/5 | Loss 8.129229\n",
      "Epoch 48 | Test Acc = 17.87% +- 0.57%\n",
      "Epoch 48 | Train Loss 8.551285552978516 | Test Acc 17.866666666666667\n",
      "Epoch 49 | Batch 0/5 | Loss 9.052530\n",
      "Epoch 49 | Test Acc = 17.60% +- 6.50%\n",
      "Epoch 49 | Train Loss 8.703257942199707 | Test Acc 17.6\n",
      "Epoch 50 | Batch 0/5 | Loss 8.908343\n",
      "Epoch 50 | Test Acc = 17.33% +- 4.18%\n",
      "Epoch 50 | Train Loss 8.772321319580078 | Test Acc 17.333333333333336\n",
      "Epoch 51 | Batch 0/5 | Loss 8.261432\n",
      "Epoch 51 | Test Acc = 22.13% +- 4.16%\n",
      "Epoch 51 | Train Loss 8.470454025268555 | Test Acc 22.133333333333336\n",
      "Epoch 52 | Batch 0/5 | Loss 8.562111\n",
      "Epoch 52 | Test Acc = 20.00% +- 2.22%\n",
      "Epoch 52 | Train Loss 8.853972434997559 | Test Acc 20.0\n",
      "Epoch 53 | Batch 0/5 | Loss 8.608313\n",
      "Epoch 53 | Test Acc = 21.07% +- 3.26%\n",
      "Epoch 53 | Train Loss 8.94500274658203 | Test Acc 21.06666666666667\n",
      "Epoch 54 | Batch 0/5 | Loss 8.557395\n",
      "Epoch 54 | Test Acc = 17.33% +- 3.22%\n",
      "Epoch 54 | Train Loss 8.491485214233398 | Test Acc 17.333333333333336\n",
      "Epoch 55 | Batch 0/5 | Loss 8.577520\n",
      "Epoch 55 | Test Acc = 15.73% +- 2.27%\n",
      "Epoch 55 | Train Loss 8.741539764404298 | Test Acc 15.733333333333334\n",
      "Epoch 56 | Batch 0/5 | Loss 8.449564\n",
      "Epoch 56 | Test Acc = 22.13% +- 4.88%\n",
      "Epoch 56 | Train Loss 8.4477388381958 | Test Acc 22.133333333333333\n",
      "Epoch 57 | Batch 0/5 | Loss 8.775049\n",
      "Epoch 57 | Test Acc = 20.53% +- 2.41%\n",
      "Epoch 57 | Train Loss 8.661177062988282 | Test Acc 20.53333333333334\n",
      "Epoch 58 | Batch 0/5 | Loss 8.387258\n",
      "Epoch 58 | Test Acc = 18.67% +- 3.54%\n",
      "Epoch 58 | Train Loss 8.487030410766602 | Test Acc 18.666666666666668\n",
      "Epoch 59 | Batch 0/5 | Loss 8.898181\n",
      "Epoch 59 | Test Acc = 22.67% +- 4.85%\n",
      "Epoch 59 | Train Loss 8.69081974029541 | Test Acc 22.666666666666664\n",
      "Epoch 60 | Batch 0/5 | Loss 8.714808\n",
      "Epoch 60 | Test Acc = 17.33% +- 4.50%\n",
      "Epoch 60 | Train Loss 8.689974784851074 | Test Acc 17.333333333333336\n",
      "Epoch 61 | Batch 0/5 | Loss 9.108945\n",
      "Epoch 61 | Test Acc = 20.00% +- 3.05%\n",
      "Epoch 61 | Train Loss 8.775276947021485 | Test Acc 20.0\n",
      "Epoch 62 | Batch 0/5 | Loss 8.590718\n",
      "Epoch 62 | Test Acc = 19.73% +- 2.90%\n",
      "Epoch 62 | Train Loss 8.729545974731446 | Test Acc 19.733333333333334\n",
      "Epoch 63 | Batch 0/5 | Loss 8.201427\n",
      "Epoch 63 | Test Acc = 20.00% +- 2.77%\n",
      "Epoch 63 | Train Loss 8.364008522033691 | Test Acc 20.000000000000004\n",
      "Epoch 64 | Batch 0/5 | Loss 8.235298\n",
      "Epoch 64 | Test Acc = 17.87% +- 1.75%\n",
      "Epoch 64 | Train Loss 8.520653915405273 | Test Acc 17.866666666666667\n",
      "Epoch 65 | Batch 0/5 | Loss 8.531328\n",
      "Epoch 65 | Test Acc = 16.80% +- 3.51%\n",
      "Epoch 65 | Train Loss 8.597883796691894 | Test Acc 16.800000000000004\n",
      "Epoch 66 | Batch 0/5 | Loss 8.362304\n",
      "Epoch 66 | Test Acc = 19.73% +- 3.94%\n",
      "Epoch 66 | Train Loss 8.475043487548827 | Test Acc 19.73333333333333\n",
      "Epoch 67 | Batch 0/5 | Loss 8.875665\n",
      "Epoch 67 | Test Acc = 20.00% +- 5.82%\n",
      "Epoch 67 | Train Loss 8.632640647888184 | Test Acc 20.0\n",
      "Epoch 68 | Batch 0/5 | Loss 8.869438\n",
      "Epoch 68 | Test Acc = 24.27% +- 5.20%\n",
      "Epoch 68 | Train Loss 8.36676778793335 | Test Acc 24.26666666666667\n",
      "Epoch 69 | Batch 0/5 | Loss 8.457819\n",
      "Epoch 69 | Test Acc = 26.40% +- 4.76%\n",
      "Epoch 69 | Train Loss 8.45314426422119 | Test Acc 26.4\n",
      "Epoch 70 | Batch 0/5 | Loss 8.860186\n",
      "Epoch 70 | Test Acc = 19.47% +- 2.82%\n",
      "Epoch 70 | Train Loss 8.575024604797363 | Test Acc 19.46666666666667\n",
      "Epoch 71 | Batch 0/5 | Loss 9.086940\n",
      "Epoch 71 | Test Acc = 22.40% +- 5.50%\n",
      "Epoch 71 | Train Loss 8.570738792419434 | Test Acc 22.400000000000002\n",
      "Epoch 72 | Batch 0/5 | Loss 8.165747\n",
      "Epoch 72 | Test Acc = 17.87% +- 1.19%\n",
      "Epoch 72 | Train Loss 8.663025665283204 | Test Acc 17.866666666666667\n",
      "Epoch 73 | Batch 0/5 | Loss 8.504444\n",
      "Epoch 73 | Test Acc = 23.47% +- 7.56%\n",
      "Epoch 73 | Train Loss 8.493300437927246 | Test Acc 23.466666666666665\n",
      "Epoch 74 | Batch 0/5 | Loss 8.626910\n",
      "Epoch 74 | Test Acc = 21.60% +- 6.59%\n",
      "Epoch 74 | Train Loss 8.594750595092773 | Test Acc 21.6\n",
      "Epoch 75 | Batch 0/5 | Loss 8.151382\n",
      "Epoch 75 | Test Acc = 20.27% +- 6.42%\n",
      "Epoch 75 | Train Loss 8.435040473937988 | Test Acc 20.266666666666666\n",
      "Epoch 76 | Batch 0/5 | Loss 8.852089\n",
      "Epoch 76 | Test Acc = 21.87% +- 4.77%\n",
      "Epoch 76 | Train Loss 8.635862922668457 | Test Acc 21.866666666666667\n",
      "Epoch 77 | Batch 0/5 | Loss 8.364715\n",
      "Epoch 77 | Test Acc = 21.33% +- 4.90%\n",
      "Epoch 77 | Train Loss 8.447927093505859 | Test Acc 21.333333333333332\n",
      "Epoch 78 | Batch 0/5 | Loss 8.248748\n",
      "Epoch 78 | Test Acc = 18.67% +- 5.87%\n",
      "Epoch 78 | Train Loss 8.258599090576173 | Test Acc 18.666666666666668\n",
      "Epoch 79 | Batch 0/5 | Loss 8.063590\n",
      "Epoch 79 | Test Acc = 18.67% +- 2.22%\n",
      "Epoch 79 | Train Loss 8.315005111694337 | Test Acc 18.666666666666668\n",
      "Epoch 80 | Batch 0/5 | Loss 9.101492\n",
      "Epoch 80 | Test Acc = 21.07% +- 2.71%\n",
      "Epoch 80 | Train Loss 8.333026504516601 | Test Acc 21.06666666666667\n",
      "Epoch 81 | Batch 0/5 | Loss 8.489597\n",
      "Epoch 81 | Test Acc = 24.53% +- 3.35%\n",
      "Epoch 81 | Train Loss 8.263522529602051 | Test Acc 24.53333333333333\n",
      "Epoch 82 | Batch 0/5 | Loss 8.039187\n",
      "Epoch 82 | Test Acc = 23.73% +- 4.27%\n",
      "Epoch 82 | Train Loss 7.946926021575928 | Test Acc 23.733333333333338\n",
      "Epoch 83 | Batch 0/5 | Loss 8.495433\n",
      "Epoch 83 | Test Acc = 23.47% +- 6.96%\n",
      "Epoch 83 | Train Loss 8.396128463745118 | Test Acc 23.46666666666667\n",
      "Epoch 84 | Batch 0/5 | Loss 8.009268\n",
      "Epoch 84 | Test Acc = 20.53% +- 4.22%\n",
      "Epoch 84 | Train Loss 8.004644966125488 | Test Acc 20.533333333333335\n",
      "Epoch 85 | Batch 0/5 | Loss 8.177632\n",
      "Epoch 85 | Test Acc = 22.40% +- 3.87%\n",
      "Epoch 85 | Train Loss 8.234294605255126 | Test Acc 22.4\n",
      "Epoch 86 | Batch 0/5 | Loss 7.934278\n",
      "Epoch 86 | Test Acc = 21.33% +- 5.38%\n",
      "Epoch 86 | Train Loss 8.128088760375977 | Test Acc 21.333333333333336\n",
      "Epoch 87 | Batch 0/5 | Loss 8.364809\n",
      "Epoch 87 | Test Acc = 17.07% +- 2.90%\n",
      "Epoch 87 | Train Loss 8.334466171264648 | Test Acc 17.066666666666666\n",
      "Epoch 88 | Batch 0/5 | Loss 7.986573\n",
      "Epoch 88 | Test Acc = 17.60% +- 3.42%\n",
      "Epoch 88 | Train Loss 8.153436660766602 | Test Acc 17.6\n",
      "Epoch 89 | Batch 0/5 | Loss 8.262533\n",
      "Epoch 89 | Test Acc = 21.07% +- 7.07%\n",
      "Epoch 89 | Train Loss 8.46562385559082 | Test Acc 21.06666666666667\n",
      "Epoch 90 | Batch 0/5 | Loss 8.038157\n",
      "Epoch 90 | Test Acc = 22.67% +- 5.33%\n",
      "Epoch 90 | Train Loss 8.244901371002197 | Test Acc 22.666666666666668\n",
      "Epoch 91 | Batch 0/5 | Loss 8.632659\n",
      "Epoch 91 | Test Acc = 17.60% +- 3.94%\n",
      "Epoch 91 | Train Loss 8.13476629257202 | Test Acc 17.599999999999998\n",
      "Epoch 92 | Batch 0/5 | Loss 8.263632\n",
      "Epoch 92 | Test Acc = 19.47% +- 2.82%\n",
      "Epoch 92 | Train Loss 7.992522239685059 | Test Acc 19.46666666666667\n",
      "Epoch 93 | Batch 0/5 | Loss 8.002204\n",
      "Epoch 93 | Test Acc = 14.67% +- 1.05%\n",
      "Epoch 93 | Train Loss 8.317766094207764 | Test Acc 14.666666666666666\n",
      "Epoch 94 | Batch 0/5 | Loss 8.430130\n",
      "Epoch 94 | Test Acc = 22.93% +- 2.01%\n",
      "Epoch 94 | Train Loss 8.2354248046875 | Test Acc 22.93333333333333\n",
      "Epoch 95 | Batch 0/5 | Loss 8.181946\n",
      "Epoch 95 | Test Acc = 18.93% +- 3.08%\n",
      "Epoch 95 | Train Loss 8.040196228027344 | Test Acc 18.933333333333334\n",
      "Epoch 96 | Batch 0/5 | Loss 7.860268\n",
      "Epoch 96 | Test Acc = 21.33% +- 4.50%\n",
      "Epoch 96 | Train Loss 7.762681484222412 | Test Acc 21.333333333333336\n",
      "Epoch 97 | Batch 0/5 | Loss 8.124866\n",
      "Epoch 97 | Test Acc = 20.53% +- 6.30%\n",
      "Epoch 97 | Train Loss 8.134440040588379 | Test Acc 20.53333333333333\n",
      "Epoch 98 | Batch 0/5 | Loss 8.301330\n",
      "Epoch 98 | Test Acc = 19.73% +- 7.44%\n",
      "Epoch 98 | Train Loss 8.242411613464355 | Test Acc 19.733333333333334\n",
      "Epoch 99 | Batch 0/5 | Loss 8.690230\n",
      "Epoch 99 | Test Acc = 18.67% +- 3.70%\n",
      "Epoch 99 | Train Loss 8.006782531738281 | Test Acc 18.666666666666668\n",
      "Epoch 100 | Batch 0/5 | Loss 8.518641\n",
      "Epoch 100 | Test Acc = 19.73% +- 2.71%\n",
      "Epoch 100 | Train Loss 8.201581573486328 | Test Acc 19.733333333333338\n",
      "Epoch 101 | Batch 0/5 | Loss 7.614554\n",
      "Epoch 101 | Test Acc = 22.40% +- 4.76%\n",
      "Epoch 101 | Train Loss 7.89635124206543 | Test Acc 22.4\n",
      "Epoch 102 | Batch 0/5 | Loss 7.792678\n",
      "Epoch 102 | Test Acc = 18.67% +- 2.77%\n",
      "Epoch 102 | Train Loss 7.9225770950317385 | Test Acc 18.666666666666664\n",
      "Epoch 103 | Batch 0/5 | Loss 8.032022\n",
      "Epoch 103 | Test Acc = 21.07% +- 4.27%\n",
      "Epoch 103 | Train Loss 8.154827499389649 | Test Acc 21.066666666666666\n",
      "Epoch 104 | Batch 0/5 | Loss 8.158072\n",
      "Epoch 104 | Test Acc = 22.67% +- 2.56%\n",
      "Epoch 104 | Train Loss 8.18603582382202 | Test Acc 22.666666666666668\n",
      "Epoch 105 | Batch 0/5 | Loss 7.794559\n",
      "Epoch 105 | Test Acc = 20.80% +- 3.35%\n",
      "Epoch 105 | Train Loss 8.034711647033692 | Test Acc 20.8\n",
      "Epoch 106 | Batch 0/5 | Loss 8.025275\n",
      "Epoch 106 | Test Acc = 19.73% +- 2.27%\n",
      "Epoch 106 | Train Loss 8.155448150634765 | Test Acc 19.733333333333334\n",
      "Epoch 107 | Batch 0/5 | Loss 8.549856\n",
      "Epoch 107 | Test Acc = 18.40% +- 3.17%\n",
      "Epoch 107 | Train Loss 8.137921714782715 | Test Acc 18.4\n",
      "Epoch 108 | Batch 0/5 | Loss 7.583226\n",
      "Epoch 108 | Test Acc = 21.87% +- 1.40%\n",
      "Epoch 108 | Train Loss 7.846829509735107 | Test Acc 21.866666666666667\n",
      "Epoch 109 | Batch 0/5 | Loss 7.906063\n",
      "Epoch 109 | Test Acc = 22.13% +- 5.05%\n",
      "Epoch 109 | Train Loss 7.925940704345703 | Test Acc 22.133333333333333\n",
      "Epoch 110 | Batch 0/5 | Loss 8.419849\n",
      "Epoch 110 | Test Acc = 20.00% +- 5.28%\n",
      "Epoch 110 | Train Loss 8.090166664123535 | Test Acc 20.0\n",
      "Epoch 111 | Batch 0/5 | Loss 7.621943\n",
      "Epoch 111 | Test Acc = 17.33% +- 3.31%\n",
      "Epoch 111 | Train Loss 7.69242525100708 | Test Acc 17.333333333333332\n",
      "Epoch 112 | Batch 0/5 | Loss 7.695671\n",
      "Epoch 112 | Test Acc = 19.73% +- 3.73%\n",
      "Epoch 112 | Train Loss 7.675996398925781 | Test Acc 19.733333333333334\n",
      "Epoch 113 | Batch 0/5 | Loss 7.650971\n",
      "Epoch 113 | Test Acc = 18.67% +- 7.82%\n",
      "Epoch 113 | Train Loss 8.041301155090332 | Test Acc 18.666666666666668\n",
      "Epoch 114 | Batch 0/5 | Loss 7.493196\n",
      "Epoch 114 | Test Acc = 17.60% +- 4.52%\n",
      "Epoch 114 | Train Loss 8.043658447265624 | Test Acc 17.6\n",
      "Epoch 115 | Batch 0/5 | Loss 8.129445\n",
      "Epoch 115 | Test Acc = 17.60% +- 2.90%\n",
      "Epoch 115 | Train Loss 7.48969144821167 | Test Acc 17.6\n",
      "Epoch 116 | Batch 0/5 | Loss 8.119403\n",
      "Epoch 116 | Test Acc = 21.07% +- 6.67%\n",
      "Epoch 116 | Train Loss 7.762612628936767 | Test Acc 21.06666666666667\n",
      "Epoch 117 | Batch 0/5 | Loss 7.761206\n",
      "Epoch 117 | Test Acc = 23.73% +- 2.27%\n",
      "Epoch 117 | Train Loss 7.842432498931885 | Test Acc 23.733333333333338\n",
      "Epoch 118 | Batch 0/5 | Loss 8.117287\n",
      "Epoch 118 | Test Acc = 21.07% +- 2.27%\n",
      "Epoch 118 | Train Loss 7.874600791931153 | Test Acc 21.06666666666667\n",
      "Epoch 119 | Batch 0/5 | Loss 7.801756\n",
      "Epoch 119 | Test Acc = 18.13% +- 2.62%\n",
      "Epoch 119 | Train Loss 7.8816162109375 | Test Acc 18.133333333333333\n",
      "Epoch 120 | Batch 0/5 | Loss 7.756425\n",
      "Epoch 120 | Test Acc = 20.53% +- 3.19%\n",
      "Epoch 120 | Train Loss 7.700552082061767 | Test Acc 20.53333333333334\n",
      "Epoch 121 | Batch 0/5 | Loss 8.363804\n",
      "Epoch 121 | Test Acc = 18.40% +- 1.15%\n",
      "Epoch 121 | Train Loss 8.124256134033203 | Test Acc 18.400000000000002\n",
      "Epoch 122 | Batch 0/5 | Loss 7.834743\n",
      "Epoch 122 | Test Acc = 24.80% +- 5.61%\n",
      "Epoch 122 | Train Loss 7.785301589965821 | Test Acc 24.800000000000004\n",
      "Epoch 123 | Batch 0/5 | Loss 7.918748\n",
      "Epoch 123 | Test Acc = 20.27% +- 5.50%\n",
      "Epoch 123 | Train Loss 7.657833385467529 | Test Acc 20.26666666666667\n",
      "Epoch 124 | Batch 0/5 | Loss 7.363351\n",
      "Epoch 124 | Test Acc = 19.20% +- 1.59%\n",
      "Epoch 124 | Train Loss 7.553600883483886 | Test Acc 19.200000000000006\n",
      "Epoch 125 | Batch 0/5 | Loss 7.605309\n",
      "Epoch 125 | Test Acc = 21.33% +- 3.62%\n",
      "Epoch 125 | Train Loss 7.417775058746338 | Test Acc 21.333333333333336\n",
      "Epoch 126 | Batch 0/5 | Loss 7.374787\n",
      "Epoch 126 | Test Acc = 16.53% +- 5.94%\n",
      "Epoch 126 | Train Loss 7.845306301116944 | Test Acc 16.533333333333335\n",
      "Epoch 127 | Batch 0/5 | Loss 7.855587\n",
      "Epoch 127 | Test Acc = 17.07% +- 1.87%\n",
      "Epoch 127 | Train Loss 7.7773772239685055 | Test Acc 17.06666666666667\n",
      "Epoch 128 | Batch 0/5 | Loss 7.432944\n",
      "Epoch 128 | Test Acc = 22.40% +- 7.70%\n",
      "Epoch 128 | Train Loss 7.75124225616455 | Test Acc 22.4\n",
      "Epoch 129 | Batch 0/5 | Loss 7.536711\n",
      "Epoch 129 | Test Acc = 20.80% +- 6.25%\n",
      "Epoch 129 | Train Loss 7.612392711639404 | Test Acc 20.8\n",
      "Epoch 130 | Batch 0/5 | Loss 7.137026\n",
      "Epoch 130 | Test Acc = 19.20% +- 3.35%\n",
      "Epoch 130 | Train Loss 7.379284000396728 | Test Acc 19.200000000000003\n",
      "Epoch 131 | Batch 0/5 | Loss 7.451427\n",
      "Epoch 131 | Test Acc = 21.33% +- 7.35%\n",
      "Epoch 131 | Train Loss 7.610513305664062 | Test Acc 21.333333333333336\n",
      "Epoch 132 | Batch 0/5 | Loss 7.725327\n",
      "Epoch 132 | Test Acc = 18.67% +- 2.22%\n",
      "Epoch 132 | Train Loss 7.699049377441407 | Test Acc 18.666666666666668\n",
      "Epoch 133 | Batch 0/5 | Loss 7.546045\n",
      "Epoch 133 | Test Acc = 21.07% +- 4.40%\n",
      "Epoch 133 | Train Loss 7.322841358184815 | Test Acc 21.06666666666667\n",
      "Epoch 134 | Batch 0/5 | Loss 7.739024\n",
      "Epoch 134 | Test Acc = 16.53% +- 4.09%\n",
      "Epoch 134 | Train Loss 7.487097072601318 | Test Acc 16.533333333333335\n",
      "Epoch 135 | Batch 0/5 | Loss 8.093311\n",
      "Epoch 135 | Test Acc = 22.13% +- 2.73%\n",
      "Epoch 135 | Train Loss 7.465876483917237 | Test Acc 22.133333333333333\n",
      "Epoch 136 | Batch 0/5 | Loss 7.407638\n",
      "Epoch 136 | Test Acc = 25.87% +- 9.65%\n",
      "Epoch 136 | Train Loss 7.553439807891846 | Test Acc 25.866666666666667\n",
      "Epoch 137 | Batch 0/5 | Loss 7.314447\n",
      "Epoch 137 | Test Acc = 16.80% +- 1.90%\n",
      "Epoch 137 | Train Loss 7.518739700317383 | Test Acc 16.8\n",
      "Epoch 138 | Batch 0/5 | Loss 7.264698\n",
      "Epoch 138 | Test Acc = 21.33% +- 4.25%\n",
      "Epoch 138 | Train Loss 7.748764419555664 | Test Acc 21.333333333333336\n",
      "Epoch 139 | Batch 0/5 | Loss 7.838549\n",
      "Epoch 139 | Test Acc = 16.80% +- 2.73%\n",
      "Epoch 139 | Train Loss 7.253615760803223 | Test Acc 16.8\n",
      "Epoch 140 | Batch 0/5 | Loss 7.136106\n",
      "Epoch 140 | Test Acc = 20.53% +- 5.80%\n",
      "Epoch 140 | Train Loss 7.447679615020752 | Test Acc 20.533333333333335\n",
      "Epoch 141 | Batch 0/5 | Loss 7.245049\n",
      "Epoch 141 | Test Acc = 21.07% +- 1.72%\n",
      "Epoch 141 | Train Loss 7.791281700134277 | Test Acc 21.066666666666666\n",
      "Epoch 142 | Batch 0/5 | Loss 6.876185\n",
      "Epoch 142 | Test Acc = 25.60% +- 5.35%\n",
      "Epoch 142 | Train Loss 7.403888797760009 | Test Acc 25.6\n",
      "Epoch 143 | Batch 0/5 | Loss 8.019275\n",
      "Epoch 143 | Test Acc = 20.53% +- 5.85%\n",
      "Epoch 143 | Train Loss 7.52628002166748 | Test Acc 20.53333333333333\n",
      "Epoch 144 | Batch 0/5 | Loss 7.888680\n",
      "Epoch 144 | Test Acc = 19.73% +- 6.16%\n",
      "Epoch 144 | Train Loss 7.562657642364502 | Test Acc 19.73333333333333\n",
      "Epoch 145 | Batch 0/5 | Loss 7.530334\n",
      "Epoch 145 | Test Acc = 22.93% +- 3.50%\n",
      "Epoch 145 | Train Loss 7.3450664520263675 | Test Acc 22.933333333333334\n",
      "Epoch 146 | Batch 0/5 | Loss 7.266135\n",
      "Epoch 146 | Test Acc = 21.60% +- 5.93%\n",
      "Epoch 146 | Train Loss 7.19927978515625 | Test Acc 21.6\n",
      "Epoch 147 | Batch 0/5 | Loss 7.977919\n",
      "Epoch 147 | Test Acc = 17.60% +- 2.71%\n",
      "Epoch 147 | Train Loss 7.288427734375 | Test Acc 17.6\n",
      "Epoch 148 | Batch 0/5 | Loss 7.361852\n",
      "Epoch 148 | Test Acc = 20.27% +- 1.87%\n",
      "Epoch 148 | Train Loss 7.381497573852539 | Test Acc 20.26666666666667\n",
      "Epoch 149 | Batch 0/5 | Loss 6.729025\n",
      "Epoch 149 | Test Acc = 19.73% +- 2.80%\n",
      "Epoch 149 | Train Loss 7.347907161712646 | Test Acc 19.733333333333334\n",
      "Epoch 150 | Batch 0/5 | Loss 7.717585\n",
      "Epoch 150 | Test Acc = 21.33% +- 4.90%\n",
      "Epoch 150 | Train Loss 7.455947685241699 | Test Acc 21.333333333333332\n",
      "Epoch 151 | Batch 0/5 | Loss 8.230514\n",
      "Epoch 151 | Test Acc = 20.00% +- 3.70%\n",
      "Epoch 151 | Train Loss 7.498828506469726 | Test Acc 20.000000000000004\n",
      "Epoch 152 | Batch 0/5 | Loss 7.710316\n",
      "Epoch 152 | Test Acc = 25.07% +- 7.91%\n",
      "Epoch 152 | Train Loss 7.334973430633545 | Test Acc 25.06666666666667\n",
      "Epoch 153 | Batch 0/5 | Loss 7.017602\n",
      "Epoch 153 | Test Acc = 18.40% +- 4.52%\n",
      "Epoch 153 | Train Loss 7.274249267578125 | Test Acc 18.400000000000002\n",
      "Epoch 154 | Batch 0/5 | Loss 6.990798\n",
      "Epoch 154 | Test Acc = 21.87% +- 5.56%\n",
      "Epoch 154 | Train Loss 7.290690612792969 | Test Acc 21.866666666666667\n",
      "Epoch 155 | Batch 0/5 | Loss 7.599001\n",
      "Epoch 155 | Test Acc = 22.40% +- 5.09%\n",
      "Epoch 155 | Train Loss 7.40138692855835 | Test Acc 22.4\n",
      "Epoch 156 | Batch 0/5 | Loss 7.425365\n",
      "Epoch 156 | Test Acc = 25.33% +- 7.79%\n",
      "Epoch 156 | Train Loss 7.103561782836914 | Test Acc 25.333333333333332\n",
      "Epoch 157 | Batch 0/5 | Loss 7.412447\n",
      "Epoch 157 | Test Acc = 18.67% +- 3.47%\n",
      "Epoch 157 | Train Loss 7.084893703460693 | Test Acc 18.666666666666668\n",
      "Epoch 158 | Batch 0/5 | Loss 7.128674\n",
      "Epoch 158 | Test Acc = 18.93% +- 5.20%\n",
      "Epoch 158 | Train Loss 7.26075496673584 | Test Acc 18.933333333333337\n",
      "Epoch 159 | Batch 0/5 | Loss 7.081094\n",
      "Epoch 159 | Test Acc = 17.87% +- 3.95%\n",
      "Epoch 159 | Train Loss 7.069693279266358 | Test Acc 17.866666666666667\n",
      "Epoch 160 | Batch 0/5 | Loss 7.408902\n",
      "Epoch 160 | Test Acc = 18.67% +- 4.79%\n",
      "Epoch 160 | Train Loss 6.972132205963135 | Test Acc 18.666666666666668\n",
      "Epoch 161 | Batch 0/5 | Loss 6.810550\n",
      "Epoch 161 | Test Acc = 21.07% +- 8.41%\n",
      "Epoch 161 | Train Loss 7.365441226959229 | Test Acc 21.06666666666667\n",
      "Epoch 162 | Batch 0/5 | Loss 6.837052\n",
      "Epoch 162 | Test Acc = 17.33% +- 3.91%\n",
      "Epoch 162 | Train Loss 7.302991580963135 | Test Acc 17.333333333333336\n",
      "Epoch 163 | Batch 0/5 | Loss 7.185878\n",
      "Epoch 163 | Test Acc = 17.07% +- 2.38%\n",
      "Epoch 163 | Train Loss 7.215362644195556 | Test Acc 17.06666666666667\n",
      "Epoch 164 | Batch 0/5 | Loss 7.233768\n",
      "Epoch 164 | Test Acc = 16.27% +- 2.50%\n",
      "Epoch 164 | Train Loss 7.284148120880127 | Test Acc 16.266666666666666\n",
      "Epoch 165 | Batch 0/5 | Loss 6.614461\n",
      "Epoch 165 | Test Acc = 16.80% +- 1.19%\n",
      "Epoch 165 | Train Loss 6.911268043518066 | Test Acc 16.8\n",
      "Epoch 166 | Batch 0/5 | Loss 6.750560\n",
      "Epoch 166 | Test Acc = 20.53% +- 6.84%\n",
      "Epoch 166 | Train Loss 6.74399471282959 | Test Acc 20.53333333333333\n",
      "Epoch 167 | Batch 0/5 | Loss 7.121117\n",
      "Epoch 167 | Test Acc = 22.67% +- 5.87%\n",
      "Epoch 167 | Train Loss 6.666859531402588 | Test Acc 22.666666666666668\n",
      "Epoch 168 | Batch 0/5 | Loss 7.499770\n",
      "Epoch 168 | Test Acc = 23.20% +- 6.88%\n",
      "Epoch 168 | Train Loss 7.1633069038391115 | Test Acc 23.2\n",
      "Epoch 169 | Batch 0/5 | Loss 8.179422\n",
      "Epoch 169 | Test Acc = 20.00% +- 3.54%\n",
      "Epoch 169 | Train Loss 7.312880611419677 | Test Acc 20.000000000000004\n",
      "Epoch 170 | Batch 0/5 | Loss 7.314569\n",
      "Epoch 170 | Test Acc = 17.33% +- 1.05%\n",
      "Epoch 170 | Train Loss 7.420080375671387 | Test Acc 17.333333333333336\n",
      "Epoch 171 | Batch 0/5 | Loss 6.815368\n",
      "Epoch 171 | Test Acc = 24.53% +- 7.45%\n",
      "Epoch 171 | Train Loss 7.090235233306885 | Test Acc 24.533333333333335\n",
      "Epoch 172 | Batch 0/5 | Loss 7.080032\n",
      "Epoch 172 | Test Acc = 19.20% +- 2.04%\n",
      "Epoch 172 | Train Loss 7.056525230407715 | Test Acc 19.2\n",
      "Epoch 173 | Batch 0/5 | Loss 6.629932\n",
      "Epoch 173 | Test Acc = 21.07% +- 3.94%\n",
      "Epoch 173 | Train Loss 6.838826847076416 | Test Acc 21.066666666666666\n",
      "Epoch 174 | Batch 0/5 | Loss 6.290838\n",
      "Epoch 174 | Test Acc = 16.53% +- 2.41%\n",
      "Epoch 174 | Train Loss 6.698517990112305 | Test Acc 16.533333333333335\n",
      "Epoch 175 | Batch 0/5 | Loss 7.151206\n",
      "Epoch 175 | Test Acc = 19.20% +- 3.35%\n",
      "Epoch 175 | Train Loss 6.913606262207031 | Test Acc 19.200000000000003\n",
      "Epoch 176 | Batch 0/5 | Loss 6.401968\n",
      "Epoch 176 | Test Acc = 20.53% +- 3.59%\n",
      "Epoch 176 | Train Loss 6.586816215515137 | Test Acc 20.533333333333335\n",
      "Epoch 177 | Batch 0/5 | Loss 6.663159\n",
      "Epoch 177 | Test Acc = 21.60% +- 6.33%\n",
      "Epoch 177 | Train Loss 6.975280380249023 | Test Acc 21.6\n",
      "Epoch 178 | Batch 0/5 | Loss 6.305642\n",
      "Epoch 178 | Test Acc = 20.80% +- 1.59%\n",
      "Epoch 178 | Train Loss 6.97901029586792 | Test Acc 20.8\n",
      "Epoch 179 | Batch 0/5 | Loss 6.369077\n",
      "Epoch 179 | Test Acc = 18.67% +- 2.56%\n",
      "Epoch 179 | Train Loss 7.175535106658936 | Test Acc 18.666666666666664\n",
      "Epoch 180 | Batch 0/5 | Loss 7.179631\n",
      "Epoch 180 | Test Acc = 18.93% +- 1.87%\n",
      "Epoch 180 | Train Loss 7.024452018737793 | Test Acc 18.933333333333334\n",
      "Epoch 181 | Batch 0/5 | Loss 6.237904\n",
      "Epoch 181 | Test Acc = 18.93% +- 5.93%\n",
      "Epoch 181 | Train Loss 6.676014137268067 | Test Acc 18.933333333333334\n",
      "Epoch 182 | Batch 0/5 | Loss 7.800461\n",
      "Epoch 182 | Test Acc = 21.60% +- 7.80%\n",
      "Epoch 182 | Train Loss 7.0504021644592285 | Test Acc 21.6\n",
      "Epoch 183 | Batch 0/5 | Loss 7.366819\n",
      "Epoch 183 | Test Acc = 19.20% +- 2.29%\n",
      "Epoch 183 | Train Loss 6.891833019256592 | Test Acc 19.2\n",
      "Epoch 184 | Batch 0/5 | Loss 6.921610\n",
      "Epoch 184 | Test Acc = 19.20% +- 6.64%\n",
      "Epoch 184 | Train Loss 6.599635124206543 | Test Acc 19.2\n",
      "Epoch 185 | Batch 0/5 | Loss 6.218530\n",
      "Epoch 185 | Test Acc = 23.73% +- 1.36%\n",
      "Epoch 185 | Train Loss 6.762149810791016 | Test Acc 23.733333333333334\n",
      "Epoch 186 | Batch 0/5 | Loss 6.360412\n",
      "Epoch 186 | Test Acc = 20.00% +- 2.22%\n",
      "Epoch 186 | Train Loss 6.915866947174072 | Test Acc 20.0\n",
      "Epoch 187 | Batch 0/5 | Loss 7.093773\n",
      "Epoch 187 | Test Acc = 21.33% +- 6.27%\n",
      "Epoch 187 | Train Loss 6.739021301269531 | Test Acc 21.333333333333332\n",
      "Epoch 188 | Batch 0/5 | Loss 7.215723\n",
      "Epoch 188 | Test Acc = 22.93% +- 6.75%\n",
      "Epoch 188 | Train Loss 6.865939521789551 | Test Acc 22.933333333333334\n",
      "Epoch 189 | Batch 0/5 | Loss 6.283698\n",
      "Epoch 189 | Test Acc = 18.67% +- 1.28%\n",
      "Epoch 189 | Train Loss 6.553851985931397 | Test Acc 18.666666666666668\n",
      "Epoch 190 | Batch 0/5 | Loss 6.236356\n",
      "Epoch 190 | Test Acc = 17.07% +- 2.01%\n",
      "Epoch 190 | Train Loss 6.697312736511231 | Test Acc 17.06666666666667\n",
      "Epoch 191 | Batch 0/5 | Loss 6.667732\n",
      "Epoch 191 | Test Acc = 23.73% +- 5.74%\n",
      "Epoch 191 | Train Loss 6.550612449645996 | Test Acc 23.733333333333334\n",
      "Epoch 192 | Batch 0/5 | Loss 6.558588\n",
      "Epoch 192 | Test Acc = 17.33% +- 2.09%\n",
      "Epoch 192 | Train Loss 6.803907585144043 | Test Acc 17.333333333333336\n",
      "Epoch 193 | Batch 0/5 | Loss 7.544914\n",
      "Epoch 193 | Test Acc = 18.13% +- 3.35%\n",
      "Epoch 193 | Train Loss 6.907220268249512 | Test Acc 18.133333333333333\n",
      "Epoch 194 | Batch 0/5 | Loss 6.370389\n",
      "Epoch 194 | Test Acc = 22.40% +- 3.26%\n",
      "Epoch 194 | Train Loss 6.607717704772949 | Test Acc 22.4\n",
      "Epoch 195 | Batch 0/5 | Loss 6.787357\n",
      "Epoch 195 | Test Acc = 18.67% +- 2.22%\n",
      "Epoch 195 | Train Loss 6.9715752601623535 | Test Acc 18.666666666666668\n",
      "Epoch 196 | Batch 0/5 | Loss 6.182865\n",
      "Epoch 196 | Test Acc = 20.00% +- 3.22%\n",
      "Epoch 196 | Train Loss 6.598474216461182 | Test Acc 20.0\n",
      "Epoch 197 | Batch 0/5 | Loss 5.950538\n",
      "Epoch 197 | Test Acc = 17.60% +- 1.72%\n",
      "Epoch 197 | Train Loss 6.627964210510254 | Test Acc 17.6\n",
      "Epoch 198 | Batch 0/5 | Loss 7.349242\n",
      "Epoch 198 | Test Acc = 18.13% +- 2.17%\n",
      "Epoch 198 | Train Loss 6.688130474090576 | Test Acc 18.133333333333333\n",
      "Epoch 199 | Batch 0/5 | Loss 6.214378\n",
      "Epoch 199 | Test Acc = 16.80% +- 2.73%\n",
      "Epoch 199 | Train Loss 6.491888236999512 | Test Acc 16.8\n",
      "Epoch 200 | Batch 0/5 | Loss 6.283351\n",
      "Epoch 200 | Test Acc = 20.27% +- 6.07%\n",
      "Epoch 200 | Train Loss 6.680509185791015 | Test Acc 20.266666666666666\n",
      "Epoch 201 | Batch 0/5 | Loss 6.404357\n",
      "Epoch 201 | Test Acc = 19.20% +- 2.92%\n",
      "Epoch 201 | Train Loss 6.746501064300537 | Test Acc 19.2\n",
      "Epoch 202 | Batch 0/5 | Loss 6.226124\n",
      "Epoch 202 | Test Acc = 22.40% +- 4.98%\n",
      "Epoch 202 | Train Loss 6.465737342834473 | Test Acc 22.4\n",
      "Epoch 203 | Batch 0/5 | Loss 6.327197\n",
      "Epoch 203 | Test Acc = 20.80% +- 3.19%\n",
      "Epoch 203 | Train Loss 6.230444240570068 | Test Acc 20.8\n",
      "Epoch 204 | Batch 0/5 | Loss 6.300282\n",
      "Epoch 204 | Test Acc = 19.47% +- 3.74%\n",
      "Epoch 204 | Train Loss 6.66391658782959 | Test Acc 19.46666666666667\n",
      "Epoch 205 | Batch 0/5 | Loss 6.642385\n",
      "Epoch 205 | Test Acc = 20.53% +- 4.02%\n",
      "Epoch 205 | Train Loss 6.757275390625 | Test Acc 20.533333333333335\n",
      "Epoch 206 | Batch 0/5 | Loss 6.353754\n",
      "Epoch 206 | Test Acc = 16.53% +- 4.16%\n",
      "Epoch 206 | Train Loss 6.244482517242432 | Test Acc 16.53333333333333\n",
      "Epoch 207 | Batch 0/5 | Loss 6.786211\n",
      "Epoch 207 | Test Acc = 20.80% +- 3.01%\n",
      "Epoch 207 | Train Loss 6.359059619903564 | Test Acc 20.8\n",
      "Epoch 208 | Batch 0/5 | Loss 6.556488\n",
      "Epoch 208 | Test Acc = 19.73% +- 2.80%\n",
      "Epoch 208 | Train Loss 6.378439426422119 | Test Acc 19.733333333333338\n",
      "Epoch 209 | Batch 0/5 | Loss 7.237006\n",
      "Epoch 209 | Test Acc = 19.20% +- 2.41%\n",
      "Epoch 209 | Train Loss 6.748820400238037 | Test Acc 19.2\n",
      "Epoch 210 | Batch 0/5 | Loss 5.541599\n",
      "Epoch 210 | Test Acc = 20.00% +- 2.56%\n",
      "Epoch 210 | Train Loss 6.007379245758057 | Test Acc 20.0\n",
      "Epoch 211 | Batch 0/5 | Loss 6.006949\n",
      "Epoch 211 | Test Acc = 17.07% +- 2.01%\n",
      "Epoch 211 | Train Loss 6.263088512420654 | Test Acc 17.06666666666667\n",
      "Epoch 212 | Batch 0/5 | Loss 7.063307\n",
      "Epoch 212 | Test Acc = 21.07% +- 1.36%\n",
      "Epoch 212 | Train Loss 6.433494758605957 | Test Acc 21.06666666666667\n",
      "Epoch 213 | Batch 0/5 | Loss 6.793074\n",
      "Epoch 213 | Test Acc = 18.67% +- 2.09%\n",
      "Epoch 213 | Train Loss 6.535360527038574 | Test Acc 18.666666666666668\n",
      "Epoch 214 | Batch 0/5 | Loss 6.870541\n",
      "Epoch 214 | Test Acc = 19.47% +- 2.41%\n",
      "Epoch 214 | Train Loss 6.7269329071044925 | Test Acc 19.46666666666667\n",
      "Epoch 215 | Batch 0/5 | Loss 6.241139\n",
      "Epoch 215 | Test Acc = 20.27% +- 2.90%\n",
      "Epoch 215 | Train Loss 6.374029064178467 | Test Acc 20.26666666666667\n",
      "Epoch 216 | Batch 0/5 | Loss 6.773856\n",
      "Epoch 216 | Test Acc = 22.13% +- 1.75%\n",
      "Epoch 216 | Train Loss 6.457218933105469 | Test Acc 22.133333333333333\n",
      "Epoch 217 | Batch 0/5 | Loss 6.374331\n",
      "Epoch 217 | Test Acc = 18.93% +- 2.71%\n",
      "Epoch 217 | Train Loss 6.278531455993653 | Test Acc 18.933333333333334\n",
      "Epoch 218 | Batch 0/5 | Loss 5.703426\n",
      "Epoch 218 | Test Acc = 24.27% +- 6.99%\n",
      "Epoch 218 | Train Loss 6.125638675689697 | Test Acc 24.266666666666662\n",
      "Epoch 219 | Batch 0/5 | Loss 6.185709\n",
      "Epoch 219 | Test Acc = 25.33% +- 8.26%\n",
      "Epoch 219 | Train Loss 6.47911491394043 | Test Acc 25.333333333333336\n",
      "Epoch 220 | Batch 0/5 | Loss 6.188496\n",
      "Epoch 220 | Test Acc = 20.80% +- 2.73%\n",
      "Epoch 220 | Train Loss 6.242379093170166 | Test Acc 20.8\n",
      "Epoch 221 | Batch 0/5 | Loss 6.686236\n",
      "Epoch 221 | Test Acc = 16.27% +- 4.58%\n",
      "Epoch 221 | Train Loss 6.588646316528321 | Test Acc 16.26666666666667\n",
      "Epoch 222 | Batch 0/5 | Loss 6.934257\n",
      "Epoch 222 | Test Acc = 18.40% +- 3.08%\n",
      "Epoch 222 | Train Loss 6.43684949874878 | Test Acc 18.400000000000002\n",
      "Epoch 223 | Batch 0/5 | Loss 6.338614\n",
      "Epoch 223 | Test Acc = 22.93% +- 6.16%\n",
      "Epoch 223 | Train Loss 6.390844821929932 | Test Acc 22.933333333333334\n",
      "Epoch 224 | Batch 0/5 | Loss 7.282331\n",
      "Epoch 224 | Test Acc = 18.13% +- 2.52%\n",
      "Epoch 224 | Train Loss 6.74724645614624 | Test Acc 18.133333333333333\n",
      "Epoch 225 | Batch 0/5 | Loss 6.527312\n",
      "Epoch 225 | Test Acc = 21.07% +- 4.27%\n",
      "Epoch 225 | Train Loss 6.531308650970459 | Test Acc 21.066666666666666\n",
      "Epoch 226 | Batch 0/5 | Loss 6.187956\n",
      "Epoch 226 | Test Acc = 16.00% +- 3.14%\n",
      "Epoch 226 | Train Loss 6.364545726776123 | Test Acc 16.0\n",
      "Epoch 227 | Batch 0/5 | Loss 6.992887\n",
      "Epoch 227 | Test Acc = 21.07% +- 1.72%\n",
      "Epoch 227 | Train Loss 6.452422428131103 | Test Acc 21.06666666666667\n",
      "Epoch 228 | Batch 0/5 | Loss 6.265046\n",
      "Epoch 228 | Test Acc = 17.07% +- 4.52%\n",
      "Epoch 228 | Train Loss 6.165519142150879 | Test Acc 17.06666666666667\n",
      "Epoch 229 | Batch 0/5 | Loss 6.855140\n",
      "Epoch 229 | Test Acc = 19.73% +- 0.47%\n",
      "Epoch 229 | Train Loss 6.212088203430175 | Test Acc 19.733333333333334\n",
      "Epoch 230 | Batch 0/5 | Loss 5.271810\n",
      "Epoch 230 | Test Acc = 18.13% +- 2.52%\n",
      "Epoch 230 | Train Loss 6.116520977020263 | Test Acc 18.133333333333333\n",
      "Epoch 231 | Batch 0/5 | Loss 6.190314\n",
      "Epoch 231 | Test Acc = 19.47% +- 3.67%\n",
      "Epoch 231 | Train Loss 6.166303825378418 | Test Acc 19.46666666666667\n",
      "Epoch 232 | Batch 0/5 | Loss 5.782841\n",
      "Epoch 232 | Test Acc = 20.53% +- 2.92%\n",
      "Epoch 232 | Train Loss 5.951143455505371 | Test Acc 20.53333333333334\n",
      "Epoch 233 | Batch 0/5 | Loss 6.204401\n",
      "Epoch 233 | Test Acc = 21.07% +- 0.87%\n",
      "Epoch 233 | Train Loss 6.536518955230713 | Test Acc 21.06666666666667\n",
      "Epoch 234 | Batch 0/5 | Loss 7.151334\n",
      "Epoch 234 | Test Acc = 21.60% +- 3.50%\n",
      "Epoch 234 | Train Loss 6.550359153747559 | Test Acc 21.6\n",
      "Epoch 235 | Batch 0/5 | Loss 6.554562\n",
      "Epoch 235 | Test Acc = 21.33% +- 4.31%\n",
      "Epoch 235 | Train Loss 6.473731422424317 | Test Acc 21.333333333333332\n",
      "Epoch 236 | Batch 0/5 | Loss 6.416182\n",
      "Epoch 236 | Test Acc = 17.07% +- 3.80%\n",
      "Epoch 236 | Train Loss 6.007487487792969 | Test Acc 17.06666666666667\n",
      "Epoch 237 | Batch 0/5 | Loss 5.929548\n",
      "Epoch 237 | Test Acc = 22.93% +- 5.20%\n",
      "Epoch 237 | Train Loss 6.4918207168579105 | Test Acc 22.933333333333337\n",
      "Epoch 238 | Batch 0/5 | Loss 5.842291\n",
      "Epoch 238 | Test Acc = 18.40% +- 3.87%\n",
      "Epoch 238 | Train Loss 5.803243160247803 | Test Acc 18.400000000000002\n",
      "Epoch 239 | Batch 0/5 | Loss 5.379693\n",
      "Epoch 239 | Test Acc = 21.87% +- 1.40%\n",
      "Epoch 239 | Train Loss 6.187147045135498 | Test Acc 21.866666666666664\n",
      "Epoch 240 | Batch 0/5 | Loss 5.923019\n",
      "Epoch 240 | Test Acc = 23.20% +- 6.25%\n",
      "Epoch 240 | Train Loss 6.284926509857177 | Test Acc 23.200000000000003\n",
      "Epoch 241 | Batch 0/5 | Loss 6.889300\n",
      "Epoch 241 | Test Acc = 20.80% +- 1.40%\n",
      "Epoch 241 | Train Loss 6.3763354301452635 | Test Acc 20.8\n",
      "Epoch 242 | Batch 0/5 | Loss 5.628843\n",
      "Epoch 242 | Test Acc = 20.53% +- 3.51%\n",
      "Epoch 242 | Train Loss 6.057551002502441 | Test Acc 20.53333333333334\n",
      "Epoch 243 | Batch 0/5 | Loss 6.476717\n",
      "Epoch 243 | Test Acc = 21.33% +- 4.73%\n",
      "Epoch 243 | Train Loss 5.92274694442749 | Test Acc 21.333333333333332\n",
      "Epoch 244 | Batch 0/5 | Loss 6.298627\n",
      "Epoch 244 | Test Acc = 20.27% +- 4.52%\n",
      "Epoch 244 | Train Loss 6.2327838897705075 | Test Acc 20.26666666666667\n",
      "Epoch 245 | Batch 0/5 | Loss 5.164994\n",
      "Epoch 245 | Test Acc = 20.80% +- 5.56%\n",
      "Epoch 245 | Train Loss 6.2343605041503904 | Test Acc 20.8\n",
      "Epoch 246 | Batch 0/5 | Loss 6.355521\n",
      "Epoch 246 | Test Acc = 19.20% +- 2.04%\n",
      "Epoch 246 | Train Loss 6.285712432861328 | Test Acc 19.2\n",
      "Epoch 247 | Batch 0/5 | Loss 5.768557\n",
      "Epoch 247 | Test Acc = 20.53% +- 2.17%\n",
      "Epoch 247 | Train Loss 6.302466201782226 | Test Acc 20.53333333333333\n",
      "Epoch 248 | Batch 0/5 | Loss 6.027584\n",
      "Epoch 248 | Test Acc = 18.67% +- 2.34%\n",
      "Epoch 248 | Train Loss 6.170895099639893 | Test Acc 18.666666666666668\n",
      "Epoch 249 | Batch 0/5 | Loss 5.631361\n",
      "Epoch 249 | Test Acc = 17.33% +- 2.09%\n",
      "Epoch 249 | Train Loss 5.744128704071045 | Test Acc 17.333333333333336\n",
      "Epoch 250 | Batch 0/5 | Loss 5.889503\n",
      "Epoch 250 | Test Acc = 24.80% +- 7.07%\n",
      "Epoch 250 | Train Loss 6.0371020317077635 | Test Acc 24.8\n",
      "Epoch 251 | Batch 0/5 | Loss 7.031983\n",
      "Epoch 251 | Test Acc = 20.80% +- 2.41%\n",
      "Epoch 251 | Train Loss 6.293386650085449 | Test Acc 20.8\n",
      "Epoch 252 | Batch 0/5 | Loss 6.219580\n",
      "Epoch 252 | Test Acc = 18.13% +- 2.62%\n",
      "Epoch 252 | Train Loss 6.088185977935791 | Test Acc 18.133333333333336\n",
      "Epoch 253 | Batch 0/5 | Loss 5.846442\n",
      "Epoch 253 | Test Acc = 19.47% +- 1.59%\n",
      "Epoch 253 | Train Loss 6.068723201751709 | Test Acc 19.46666666666667\n",
      "Epoch 254 | Batch 0/5 | Loss 6.337662\n",
      "Epoch 254 | Test Acc = 19.73% +- 2.80%\n",
      "Epoch 254 | Train Loss 6.305423831939697 | Test Acc 19.733333333333338\n",
      "Epoch 255 | Batch 0/5 | Loss 6.657084\n",
      "Epoch 255 | Test Acc = 16.53% +- 3.81%\n",
      "Epoch 255 | Train Loss 5.8849084854125975 | Test Acc 16.533333333333335\n",
      "Epoch 256 | Batch 0/5 | Loss 6.443906\n",
      "Epoch 256 | Test Acc = 17.33% +- 3.22%\n",
      "Epoch 256 | Train Loss 6.119146728515625 | Test Acc 17.333333333333336\n",
      "Epoch 257 | Batch 0/5 | Loss 6.001141\n",
      "Epoch 257 | Test Acc = 20.27% +- 3.50%\n",
      "Epoch 257 | Train Loss 5.959441757202148 | Test Acc 20.266666666666666\n",
      "Epoch 258 | Batch 0/5 | Loss 6.023296\n",
      "Epoch 258 | Test Acc = 19.73% +- 2.60%\n",
      "Epoch 258 | Train Loss 5.851033020019531 | Test Acc 19.733333333333334\n",
      "Epoch 259 | Batch 0/5 | Loss 6.342866\n",
      "Epoch 259 | Test Acc = 18.67% +- 3.14%\n",
      "Epoch 259 | Train Loss 6.019493961334229 | Test Acc 18.666666666666668\n",
      "Epoch 260 | Batch 0/5 | Loss 5.767844\n",
      "Epoch 260 | Test Acc = 18.40% +- 1.72%\n",
      "Epoch 260 | Train Loss 5.918052768707275 | Test Acc 18.4\n",
      "Epoch 261 | Batch 0/5 | Loss 5.906088\n",
      "Epoch 261 | Test Acc = 22.93% +- 3.42%\n",
      "Epoch 261 | Train Loss 6.101907444000244 | Test Acc 22.93333333333333\n",
      "Epoch 262 | Batch 0/5 | Loss 6.293995\n",
      "Epoch 262 | Test Acc = 17.60% +- 2.01%\n",
      "Epoch 262 | Train Loss 5.940615558624268 | Test Acc 17.6\n",
      "Epoch 263 | Batch 0/5 | Loss 6.398392\n",
      "Epoch 263 | Test Acc = 21.33% +- 6.57%\n",
      "Epoch 263 | Train Loss 5.9269977569580075 | Test Acc 21.333333333333336\n",
      "Epoch 264 | Batch 0/5 | Loss 5.608422\n",
      "Epoch 264 | Test Acc = 20.00% +- 4.12%\n",
      "Epoch 264 | Train Loss 5.548452663421631 | Test Acc 20.000000000000004\n",
      "Epoch 265 | Batch 0/5 | Loss 5.453005\n",
      "Epoch 265 | Test Acc = 20.00% +- 1.96%\n",
      "Epoch 265 | Train Loss 5.6447649002075195 | Test Acc 20.0\n",
      "Epoch 266 | Batch 0/5 | Loss 5.562755\n",
      "Epoch 266 | Test Acc = 19.20% +- 3.01%\n",
      "Epoch 266 | Train Loss 5.909214305877685 | Test Acc 19.200000000000003\n",
      "Epoch 267 | Batch 0/5 | Loss 5.716983\n",
      "Epoch 267 | Test Acc = 17.87% +- 1.40%\n",
      "Epoch 267 | Train Loss 5.613285255432129 | Test Acc 17.866666666666667\n",
      "Epoch 268 | Batch 0/5 | Loss 6.221592\n",
      "Epoch 268 | Test Acc = 22.67% +- 1.81%\n",
      "Epoch 268 | Train Loss 5.970516872406006 | Test Acc 22.666666666666668\n",
      "Epoch 269 | Batch 0/5 | Loss 5.764830\n",
      "Epoch 269 | Test Acc = 19.73% +- 3.94%\n",
      "Epoch 269 | Train Loss 5.728630638122558 | Test Acc 19.733333333333334\n",
      "Epoch 270 | Batch 0/5 | Loss 6.290238\n",
      "Epoch 270 | Test Acc = 22.93% +- 2.27%\n",
      "Epoch 270 | Train Loss 6.049621295928955 | Test Acc 22.933333333333334\n",
      "Epoch 271 | Batch 0/5 | Loss 5.881439\n",
      "Epoch 271 | Test Acc = 17.60% +- 2.50%\n",
      "Epoch 271 | Train Loss 5.8720855712890625 | Test Acc 17.6\n",
      "Epoch 272 | Batch 0/5 | Loss 5.630728\n",
      "Epoch 272 | Test Acc = 17.33% +- 3.05%\n",
      "Epoch 272 | Train Loss 5.913575553894043 | Test Acc 17.333333333333332\n",
      "Epoch 273 | Batch 0/5 | Loss 5.822178\n",
      "Epoch 273 | Test Acc = 19.73% +- 2.38%\n",
      "Epoch 273 | Train Loss 5.9990438461303714 | Test Acc 19.733333333333334\n",
      "Epoch 274 | Batch 0/5 | Loss 5.716873\n",
      "Epoch 274 | Test Acc = 22.40% +- 2.50%\n",
      "Epoch 274 | Train Loss 5.461262798309326 | Test Acc 22.4\n",
      "Epoch 275 | Batch 0/5 | Loss 6.442471\n",
      "Epoch 275 | Test Acc = 20.00% +- 4.67%\n",
      "Epoch 275 | Train Loss 5.909387493133545 | Test Acc 20.000000000000004\n",
      "Epoch 276 | Batch 0/5 | Loss 5.765842\n",
      "Epoch 276 | Test Acc = 22.67% +- 4.18%\n",
      "Epoch 276 | Train Loss 5.8956114768981935 | Test Acc 22.666666666666668\n",
      "Epoch 277 | Batch 0/5 | Loss 5.611842\n",
      "Epoch 277 | Test Acc = 20.53% +- 2.52%\n",
      "Epoch 277 | Train Loss 5.510113143920899 | Test Acc 20.53333333333333\n",
      "Epoch 278 | Batch 0/5 | Loss 5.563585\n",
      "Epoch 278 | Test Acc = 17.87% +- 3.01%\n",
      "Epoch 278 | Train Loss 5.70142240524292 | Test Acc 17.866666666666667\n",
      "Epoch 279 | Batch 0/5 | Loss 6.185345\n",
      "Epoch 279 | Test Acc = 17.87% +- 3.27%\n",
      "Epoch 279 | Train Loss 5.63931303024292 | Test Acc 17.866666666666667\n",
      "Epoch 280 | Batch 0/5 | Loss 6.531736\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[105], line 41\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(n_way, n_support, n_query, n_train_episode)\u001b[0m\n\u001b[1;32m     38\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Implement training of the model. Remember to make use of functions defined in the `ProtoNet` and `MetaTemplate` class.\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(epoch_loss)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Evaluate test performance for epoch. Remember to make use of functions defined in the `ProtoNet` and `MetaTemplate` class.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[99], line 243\u001b[0m, in \u001b[0;36mCanNet.train_loop\u001b[0;34m(self, epoch, train_loader, optimizer)\u001b[0m\n\u001b[1;32m    241\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_forward_loss(x, y_true_query)\n\u001b[1;32m    242\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 243\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m avg_loss \u001b[38;5;241m+\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m print_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/fewshotbench/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fewshotbench/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/anaconda3/envs/fewshotbench/lib/python3.10/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/fewshotbench/lib/python3.10/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/fewshotbench/lib/python3.10/site-packages/torch/optim/adam.py:434\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# Handle complex parameters\u001b[39;00m\n\u001b[1;32m    433\u001b[0m device_grads \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_grads]\n\u001b[0;32m--> 434\u001b[0m device_exp_avgs \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_exp_avgs]\n\u001b[1;32m    435\u001b[0m device_exp_avg_sqs \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_exp_avg_sqs]\n\u001b[1;32m    436\u001b[0m params_ \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_params]\n",
      "File \u001b[0;32m~/anaconda3/envs/fewshotbench/lib/python3.10/site-packages/torch/optim/adam.py:434\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# Handle complex parameters\u001b[39;00m\n\u001b[1;32m    433\u001b[0m device_grads \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_grads]\n\u001b[0;32m--> 434\u001b[0m device_exp_avgs \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_exp_avgs]\n\u001b[1;32m    435\u001b[0m device_exp_avg_sqs \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_exp_avg_sqs]\n\u001b[1;32m    436\u001b[0m params_ \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_params]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_model(**parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fewshotbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
