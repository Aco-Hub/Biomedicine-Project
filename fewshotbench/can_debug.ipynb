{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by exercises 8\n",
    "from abc import abstractmethod, ABC\n",
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from IPython.display import clear_output\n",
    "#from backbones.fcnet import FCNet\n",
    "from fcnet import FCNet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  EXISTS: go-basic.obo\n",
      "go-basic.obo: fmt(1.2) rel(2023-06-11) 46,420 Terms; optional_attrs(relationship)\n"
     ]
    }
   ],
   "source": [
    "from datasets.prot.utils import get_samples_using_ic, check_min_samples, get_mode_ids, encodings, get_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaTemplate(nn.Module):\n",
    "    def __init__(self, backbone, n_way, n_support):\n",
    "        super(MetaTemplate, self).__init__()\n",
    "        self.n_way = n_way\n",
    "        self.n_support = n_support\n",
    "        self.n_query = -1  # (change depends on input)\n",
    "        self.feature = backbone\n",
    "        self.feat_dim = self.feature.final_feat_dim\n",
    "\n",
    "        # n_way = nb_classes per episode\n",
    "        # n_support = nb_samples per class for support set\n",
    "        # n_query = nb_samples per class for query set\n",
    "        # backbone = feature extractor (embedding network) ie. function f\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_forward(self, x, is_feature=False):\n",
    "        \"\"\"\n",
    "        forward pass, returns score (probabilities for query set)\n",
    "        output is logits for all query set (prob of each class)\n",
    "        first dim of output is n_way * n_query, nb of samples in query set\n",
    "        second dim is prob to belong to each class\n",
    "        x: [n_way, n_support + n_query, **embedding_dim]\n",
    "        out: [n_way * n_query, n_way]\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_forward_loss(self, x):\n",
    "        \"\"\"\n",
    "        takes the episode and compute loss of episode\n",
    "        x: [n_way, n_support + n_query, **embedding_dim]\n",
    "        out: loss (scalar)\n",
    "        \"\"\"\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.feature.forward(x)\n",
    "        return out\n",
    "\n",
    "    def parse_feature(self, x):\n",
    "        '''\n",
    "        create embeddings for support and query set\n",
    "        :param x: [n_way, n_support + n_query, **embedding_dim]\n",
    "        out: z_supp, z_queryÂ¨\n",
    "            z_supp: [n_way, n_support, feat_dim]\n",
    "            z_query: [n_way, n_query, feat_dim]\n",
    "        '''\n",
    "        x = Variable(x.to(self.device))\n",
    "        # reshape x to create one batch of size n_way * (n_support + n_query) and of dim whatever is dim of x\n",
    "        # x is of shape [n_way, n_support + n_query, **embedding_dim] originally, we have to reshape it to pass it to the NN ie. shape (batch_size, dim_size)\n",
    "        # note: x.contigous is used to make sure that is in the same place in mem (more efficient)\n",
    "        x = x.contiguous().view(self.n_way * (self.n_support + self.n_query), * x.size()[2:])\n",
    "        # Compute support and query feature.\n",
    "        z_all = self.forward(x)\n",
    "\n",
    "        # Reverse the transformation to distribute the samples based on the dimensions of their individual categories and flatten the embeddings.\n",
    "        # transformation is the transformation just above to one batch, ie. transform to original shape [n_way, n_support + n_query, **embedding_dim]\n",
    "        z_all = z_all.view(self.n_way, self.n_support + self.n_query, -1)\n",
    "\n",
    "        # Extract the support and query features.\n",
    "        z_support = z_all[:, :self.n_support, :]\n",
    "        z_query = z_all[:, self.n_support:, :]\n",
    "\n",
    "        return z_support, z_query\n",
    "\n",
    "    def correct(self, x):\n",
    "        # Compute the predictions scores.\n",
    "        scores = self.set_forward(x)\n",
    "\n",
    "        # Compute the top1 elements.\n",
    "        topk_scores, topk_labels = scores.data.topk(k=1, dim=1, largest=True, sorted=True)\n",
    "\n",
    "        # Detach the variables (transforming to numpy also detach the tensor)\n",
    "        topk_ind = topk_labels.cpu().numpy()\n",
    "\n",
    "        # Create the category labels for the queries, this is unique for the few shot learning setup\n",
    "        y_query = np.repeat(range(self.n_way), self.n_query)\n",
    "\n",
    "        #>>> np.repeat(range(10), 2)\n",
    "        #array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9])\n",
    "\n",
    "        # Compute number of elements that are correctly classified.\n",
    "        top1_correct = np.sum(topk_ind[:, 0] == y_query)\n",
    "        return float(top1_correct), len(y_query)\n",
    "\n",
    "    def train_loop(self, epoch, train_loader, optimizer):\n",
    "        print_freq = 10\n",
    "\n",
    "        avg_loss = 0\n",
    "        for i, (x, _) in enumerate(train_loader):\n",
    "            self.n_query = x.size(1) - self.n_support\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.set_forward_loss(x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss = avg_loss + loss.item()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                print('Epoch {:d} | Batch {:d}/{:d} | Loss {:f}'.format(epoch, i, len(train_loader),\n",
    "                                                                        avg_loss / float(i + 1)))\n",
    "        return avg_loss/len(train_loader)\n",
    "    \n",
    "    def test_loop(self, epoch, test_loader, record=None, return_std=False):\n",
    "        acc_all = []\n",
    "\n",
    "        iter_num = len(test_loader)\n",
    "        for i, (x, _) in enumerate(test_loader):\n",
    "            self.n_query = x.size(1) - self.n_support\n",
    "            correct_this, count_this = self.correct(x)\n",
    "            acc_all.append(correct_this / count_this * 100)\n",
    "\n",
    "        acc_all = np.asarray(acc_all)\n",
    "        acc_mean = np.mean(acc_all)\n",
    "        acc_std = np.std(acc_all)\n",
    "        print(f'Epoch {epoch} | Test Acc = {acc_mean:4.2f}% +- {1.96 * acc_std / np.sqrt(iter_num):4.2f}%')\n",
    "\n",
    "        if return_std:\n",
    "            return acc_mean, acc_std\n",
    "        else:\n",
    "            return acc_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is modified from https://github.com/blue-blue272/fewshot-CAN\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from methods.meta_template import MetaTemplate\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Basic convolutional block:\n",
    "    convolution + batch normalization.\n",
    "\n",
    "    Args (following http://pytorch.org/docs/master/nn.html#torch.nn.Conv2d):\n",
    "    - in_c (int): number of input channels.\n",
    "    - out_c (int): number of output channels.\n",
    "    - k (int or tuple): kernel size.\n",
    "    - s (int or tuple): stride.\n",
    "    - p (int or tuple): padding.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_c, out_c, k, s=1, p=0):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_c, out_c, k, stride=s, padding=p)\n",
    "        self.bn = nn.BatchNorm2d(out_c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bn(self.conv(x))\n",
    "\n",
    "\n",
    "class CanNet(MetaTemplate):\n",
    "    def __init__(self, backbone, n_way, n_support, reduction_ratio=6, temperature=0.025, scale_cls=7):\n",
    "        super(CanNet, self).__init__(backbone, n_way, n_support)\n",
    "        self.m = self.feat_dim\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.linear = nn.Linear(self.m, n_way)\n",
    "        self.fusion_conv = nn.Conv1d(self.feat_dim, 1, kernel_size=1)\n",
    "        self.w1 = nn.Linear(self.m, int(self.m / reduction_ratio))\n",
    "        self.activation = nn.ReLU()\n",
    "        self.w2 = nn.Linear(int(self.m / reduction_ratio), self.m)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.temperature = temperature\n",
    "        self.scale_cls = scale_cls\n",
    "\n",
    "    def set_forward(self, x, is_feature=False):\n",
    "        z_support, z_query = self.parse_feature(x, is_feature)\n",
    "\n",
    "        z_support = z_support.contiguous()\n",
    "        z_proto = z_support.view(self.n_way, self.n_support, -1).mean(1)  # the shape of z is [n_data, n_dim]\n",
    "        \n",
    "        #z_query = z_query.contiguous().view(self.n_way * self.n_query, -1)\n",
    "\n",
    "\n",
    "        z_proto_attention = torch.zeros_like(z_query).cuda() # shape: [n_way, n_dim]\n",
    "        z_query_attention = torch.zeros_like(z_query).cuda() # shape: [n_way * n_query, n_dim]\n",
    "        for i in range(z_proto.shape[0]):\n",
    "            for j in range(z_query.shape[1]):\n",
    "                attention_map_i = self.cross_attention_module(z_proto[i], z_query[i, j])\n",
    "                z_proto_attention[i, j], z_query_attention[i, j] = attention_map_i\n",
    "            \n",
    "        ftrain = z_proto_attention\n",
    "        ftest = z_query_attention\n",
    "\n",
    "        # Normalize ftest and ftrain along the feature dimension\n",
    "        ftest_norm = F.normalize(ftest, p=2, dim=1, eps=1e-12)\n",
    "        ftrain_norm = F.normalize(ftrain, p=2, dim=1, eps=1e-12)\n",
    "\n",
    "        print(\"ftest_norm: \", ftest_norm.shape)\n",
    "        print(\"ftrain: \", ftrain_norm.shape)\n",
    "\n",
    "        # Calculate cls_scores by taking the matrix product of ftest_norm and ftrain_norm (transposed)\n",
    "        cls_scores = self.scale_cls * torch.matmul(ftest_norm, ftrain_norm.T)\n",
    "\n",
    "        # y_test cannot be defined here because we don't have the info for it, it should be in another method\n",
    "        #ftest = torch.matmul(ftest, ytest) \n",
    "        #ytest = self.linear(ftest)\n",
    "\n",
    "        dists = euclidean_dist(z_query_attention, z_proto_attention)\n",
    "        cls_scores = -dists\n",
    "        return cls_scores\n",
    "    \n",
    "    def set_forward_loss(self, x):\n",
    "        y_query = torch.from_numpy(np.repeat(range( self.n_way ), self.n_query ))\n",
    "        y_query = Variable(y_query.cuda())\n",
    "\n",
    "        #scores, cls_scores = self.set_forward(x)\n",
    "        cls_scores = self.set_forward(x)\n",
    "\n",
    "        #l1 = self.loss_fn(scores, y_query )\n",
    "        l2 = self.loss_fn(cls_scores, y_query )\n",
    "        #loss = (l1 + l2) / 2\n",
    "        loss = l2\n",
    "        \n",
    "\n",
    "        return loss\n",
    "\n",
    "    def fusion_layer(self, z):\n",
    "        \"\"\"\n",
    "        Generates cross attention map A\n",
    "        :param R: [n_dim,n_dim]\n",
    "        :return: A  [n_dim]\n",
    "        \"\"\"\n",
    "        conv_query = self.fusion_conv(z)\n",
    "\n",
    "        GAP = torch.mean(z, dim=-1)\n",
    "\n",
    "        w = self.w2(self.activation(self.w1(GAP)))\n",
    "\n",
    "        # combine w and conv_query and apply temperature\n",
    "        print(\"w_shape: \" , w.shape)\n",
    "        print(\"conv_query_shape: \" , conv_query.shape)\n",
    "        fused = w.T * conv_query / self.temperature\n",
    "\n",
    "        A = self.softmax(fused)\n",
    "\n",
    "        return A\n",
    "\n",
    "    def cross_attention_module(self, z_support, z_query):\n",
    "        \"\"\"\n",
    "        TODO: do this operation for all pairs at once instead of looping, look at base code\n",
    "        Takes 1 support embedding and 1 query embedding and returns cross-attentioned embeddings\n",
    "        :param z_support: [n_dim]\n",
    "        :param z_query: [n_dim]\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        def correlation_layer(z_support, z_query): \n",
    "            \"\"\"\n",
    "            Takes 1 support embedding and 1 query embedding and returns correlation map\n",
    "            ie. P and Q in the paper. P is [P1, P2, ..., Pn] where n is the dimension of the embeddings, same for Q.\n",
    "            :param z_support: [n_dim] ie. P\n",
    "            :param z_query: [n_dim] ie. Q\n",
    "            :return: correlation_map: [n_dim, n_dim]. Note: we use R^q = correlation_map and R^p = correlation_map.T \n",
    "            \"\"\"\n",
    "\n",
    "            # compute cosine similarity between support and query embeddings\n",
    "            #print(\"z_support_shape\", z_support.shape)\n",
    "            #print(\"z_query_shape\", z_query.shape)\n",
    "\n",
    "            #P = z_support / torch.linalg.norm(z_support, ord=2)\n",
    "            #Q = z_query / torch.linalg.norm(z_query, ord=2)\n",
    "            P = F.normalize(z_support, p=2, dim=0, eps=1e-12)\n",
    "            Q = F.normalize(z_query, p=2, dim=0, eps=1e-12)\n",
    "\n",
    "            #print(\"P shape after norm\", P.shape)\n",
    "            #print(\"Q shape after norm\", Q.shape)\n",
    "            # P is of dim (n_dim) and Q is of dim (ndim)\n",
    "            # we need to change P to (n_dim, 1) and Q to (ndim, 1)\n",
    "            #P = P.reshape(P.shape, 1)\n",
    "            #Q = Q.reshape(Q.shape, 1)\n",
    "            #print(\"P shape after reshape\", P.shape)\n",
    "            #print(\"Q shape after reshape\", Q.shape)\n",
    "            correlation_map = P.unsqueeze(1) @ Q.unsqueeze(1).T # dim: [n_dim, n_dim]\n",
    "            #print(\"correlation shape\", correlation_map.shape)\n",
    "\n",
    "            return correlation_map\n",
    "\n",
    "        P_k = z_support\n",
    "        Q_b = z_query\n",
    "\n",
    "        # compute correlation map\n",
    "        print(\"P_shape before correlation\", P_k.shape)\n",
    "        print(\"Q_shape before correlation\", Q_b.shape)\n",
    "        R_p = correlation_layer(P_k, Q_b)\n",
    "        R_q = R_p.T\n",
    "        print(\"R_p_shape after correlation\", R_p.shape)\n",
    "        print(\"R_q_shape after correlation\", R_q.shape)\n",
    "\n",
    "        # compute fusion layer\n",
    "        A_p = self.fusion_layer(R_p)\n",
    "        A_q = self.fusion_layer(R_q)\n",
    "\n",
    "        \"\"\"\n",
    "        A_p = A_p * P_k\n",
    "        P_bk = A_p + P_k\n",
    "\n",
    "        A_q = A_q * Q_b\n",
    "        Q_bk = A_q + Q_b\n",
    "        \"\"\"\n",
    "        P_bk = P_k * (1 + A_p)\n",
    "        Q_bk = Q_b * (1 + A_q)\n",
    "\n",
    "        return P_bk, Q_bk\n",
    "    \n",
    "    def correct(self, x):\n",
    "        #scores, _ = self.set_forward(x)\n",
    "        scores = self.set_forward(x)\n",
    "        y_query = np.repeat(range(self.n_way), self.n_query)\n",
    "\n",
    "        topk_scores, topk_labels = scores.data.topk(1, 1, True, True)\n",
    "        topk_ind = topk_labels.cpu().numpy()\n",
    "        top1_correct = np.sum(topk_ind[:, 0] == y_query)\n",
    "        return float(top1_correct), len(y_query)\n",
    "\n",
    " \n",
    "        \n",
    "def euclidean_dist( x, y):\n",
    "    # x: N x D\n",
    "    # y: M x D\n",
    "    n = x.size(0)\n",
    "    m = y.size(0)\n",
    "    d = x.size(1)\n",
    "    assert d == y.size(1)\n",
    "\n",
    "    x = x.unsqueeze(1).expand(n, m, d)\n",
    "    y = y.unsqueeze(0).expand(n, m, d)\n",
    "\n",
    "    return torch.pow(x - y, 2).sum(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Special class for few-shot dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.check_init()\n",
    "\n",
    "    def check_init(self):\n",
    "        \"\"\"\n",
    "        Convenience function to check that the FewShotDataset is properly configured.\n",
    "        \"\"\"\n",
    "        required_attrs = ['_dataset_name', '_data_dir']\n",
    "        for attr in required_attrs:\n",
    "            if not hasattr(self, attr):\n",
    "                raise ValueError(f'FewShotDataset must have attribute {attr}.')\n",
    "\n",
    "        if not os.path.exists(self._data_dir):\n",
    "            raise ValueError(\n",
    "                f'{self._data_dir} does not exist yet. Please generate/download the dataset first.')\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def __getitem__(self, i):\n",
    "        return NotImplemented\n",
    "\n",
    "    @abstractmethod\n",
    "    def __len__(self):\n",
    "        return NotImplemented\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def dim(self):\n",
    "        return NotImplemented\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_data_loader(self, mode='train') -> DataLoader:\n",
    "        return NotImplemented\n",
    "\n",
    "    @property\n",
    "    def dataset_name(self):\n",
    "        \"\"\"\n",
    "        A string that identifies the dataset, e.g., 'swissprot'\n",
    "        \"\"\"\n",
    "        return self._dataset_name\n",
    "\n",
    "    @property\n",
    "    def data_dir(self):\n",
    "        return self._data_dir\n",
    "\n",
    "    def initialize_data_dir(self, root_dir):\n",
    "        os.makedirs(root_dir, exist_ok=True)\n",
    "        #self._data_dir = os.path.join(root_dir, self._dataset_name)\n",
    "        self._data_dir = \"data/swissprot\"\n",
    "\n",
    "class SPDataset(FewShotDataset, ABC):\n",
    "    _dataset_name = 'swissprot'\n",
    "\n",
    "    def load_swissprot(self, level = 5, mode='train', min_samples = 20):\n",
    "        samples = get_samples_using_ic(root = self.data_dir)\n",
    "        samples = check_min_samples(samples, min_samples)\n",
    "        unique_ids = set(get_mode_ids(samples)[mode])\n",
    "        return [sample for sample in samples if sample.annot in unique_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROTDIM = 1280\n",
    "\n",
    "class SubDataset(Dataset):\n",
    "    def __init__(self, samples, data_dir):\n",
    "        self.samples = samples\n",
    "        self.encoder = encodings(data_dir)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        sample = self.samples[i]\n",
    "        return sample.input_seq, self.encoder[sample.annot]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return PROTDIM\n",
    "\n",
    "class EpisodicBatchSampler(object):\n",
    "    def __init__(self, n_classes, n_way, n_episodes):\n",
    "        self.n_classes = n_classes\n",
    "        self.n_way = n_way\n",
    "        self.n_episodes = n_episodes\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_episodes\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(self.n_episodes):\n",
    "            yield torch.randperm(self.n_classes)[:self.n_way]\n",
    "\n",
    "class SPSetDataset(SPDataset):\n",
    "    def __init__(self, n_way, n_support, n_query, n_episode=100, root='./data', mode='train'):\n",
    "        self.initialize_data_dir(root)\n",
    "\n",
    "        self.n_way = n_way\n",
    "        self.n_episode = n_episode\n",
    "        min_samples = n_support + n_query\n",
    "        self.encoder = encodings(self.data_dir)\n",
    "\n",
    "        # check if samples_all.pkl exists\n",
    "        if os.path.exists('samples_all.pkl'):\n",
    "            # load samples_all using pickle\n",
    "            with open('samples_all.pkl', 'rb') as f:\n",
    "                samples_all = pickle.load(f)\n",
    "        else:\n",
    "            samples_all = self.load_swissprot(mode = mode, min_samples = min_samples)\n",
    "\n",
    "            # save samples_all using pickle\n",
    "            with open('samples_all.pkl', 'wb') as f:\n",
    "                pickle.dump(samples_all, f)\n",
    "            \n",
    "\n",
    "\n",
    "        self.categories = get_ids(samples_all) # Unique annotations\n",
    "        self.x_dim = PROTDIM\n",
    "\n",
    "        self.sub_dataloader = []\n",
    "\n",
    "        sub_data_loader_params = dict(batch_size=min_samples,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=0,  # use main thread only or may receive multiple batches\n",
    "                                      pin_memory=False)\n",
    "\n",
    "        # Create the sub datasets for each annotation of the categories and collect all the dataloaders in `self.sub_dataloader`.\n",
    "        for annotation in self.categories:\n",
    "            samples = [sample for sample in samples_all if sample.annot == annotation]\n",
    "            sub_dataset = SubDataset(samples, self.data_dir)\n",
    "            self.sub_dataloader.append(torch.utils.data.DataLoader(sub_dataset, **sub_data_loader_params))\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return next(iter(self.sub_dataloader[i]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.categories)\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self.x_dim\n",
    "\n",
    "    def get_data_loader(self) -> DataLoader:\n",
    "        sampler = EpisodicBatchSampler(len(self), self.n_way, self.n_episode)\n",
    "        data_loader_params = dict(batch_sampler=sampler, num_workers=0, pin_memory=True)\n",
    "        \n",
    "        # check if data_loader.pkl exists\n",
    "        if os.path.exists('data_loader.pkl'):\n",
    "            # load data_loader using pickle\n",
    "            with open('data_loader.pkl', 'rb') as f:\n",
    "                data_loader = pickle.load(f)\n",
    "        else:\n",
    "            data_loader = torch.utils.data.DataLoader(self, **data_loader_params)\n",
    "        \n",
    "            # save data_loader using pickle\n",
    "            with open('data_loader.pkl', 'wb') as f:\n",
    "                pickle.dump(data_loader, f)\n",
    "        return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_way, n_support, n_query, n_train_episode):\n",
    "    # Load train dataset. Remember to use make use of functions defined in the `SPSetDataset`.\n",
    "    \n",
    "    if os.path.exists('train_dataset.pkl'):\n",
    "        # load train_dataset using pickle\n",
    "        with open('train_dataset.pkl', 'rb') as f:\n",
    "            train_dataset = pickle.load(f)\n",
    "    else:\n",
    "        train_dataset = SPSetDataset(n_way, n_support, n_query, n_episode=n_train_episode, root='./data', mode='train')\n",
    "        # save as pickle\n",
    "        with open('train_dataset.pkl', 'wb') as f:\n",
    "            pickle.dump(train_dataset, f)\n",
    "    train_loader = train_dataset.get_data_loader()\n",
    "\n",
    "    # Load test dataset. Remember to use make use of functions defined in the `SPSetDataset`.\n",
    "    if os.path.exists('test_dataset.pkl'):\n",
    "        # load test_dataset using pickle\n",
    "        with open('test_dataset.pkl', 'rb') as f:\n",
    "            test_dataset = pickle.load(f)\n",
    "    else:\n",
    "        test_dataset = SPSetDataset(n_way, n_support, n_query, n_episode=100, root='./data', mode='test')\n",
    "        # save as pickle\n",
    "        with open('test_dataset.pkl', 'wb') as f:\n",
    "            pickle.dump(test_dataset, f)\n",
    "    test_loader =  test_dataset.get_data_loader()\n",
    "\n",
    "    # Initialize a fully connected network `FCNet` in `fcnet.py` with two hidden layers of 512 units each as feature extractor.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    backbone = FCNet(train_dataset.dim).to(device)\n",
    "\n",
    "\n",
    "    # Initialize model using the backbone and the optimizer.\n",
    "    model = CanNet(backbone, n_way, n_support).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    test_accs = []; train_losses = []\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "\n",
    "        # Implement training of the model. Remember to make use of functions defined in the `ProtoNet` and `MetaTemplate` class.\n",
    "        epoch_loss = model.train_loop(epoch, train_loader, optimizer)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # Evaluate test performance for epoch. Remember to make use of functions defined in the `ProtoNet` and `MetaTemplate` class.\n",
    "        test_acc = model.test_loop(test_loader)\n",
    "        test_accs.append(test_acc)\n",
    "        print(f'Epoch {epoch} | Train Loss {epoch_loss} | Test Acc {test_acc}')\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 2.5))\n",
    "    ax1.plot(range(len(train_losses)), train_losses)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Train Loss')\n",
    "    ax1.grid()\n",
    "\n",
    "    ax2.plot(range(len(test_accs)), test_accs)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Test Accuracy')\n",
    "    ax2.grid()\n",
    "    fig.suptitle(f\"n_way={n_way}, n_support={n_support}, n_query={n_query}, n_train_episode={n_train_episode}\")\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_way': 5, 'n_support': 5, 'n_query': 15, 'n_train_episode': 5}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17982/778028402.py:113: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3571.)\n",
      "  fused = w.T * conv_query / self.temperature\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "P_shape before correlation torch.Size([64])\n",
      "Q_shape before correlation torch.Size([64])\n",
      "R_p_shape after correlation torch.Size([64, 64])\n",
      "R_q_shape after correlation torch.Size([64, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "w_shape:  torch.Size([64])\n",
      "conv_query_shape:  torch.Size([1, 64])\n",
      "ftest_norm:  torch.Size([5, 15, 64])\n",
      "ftrain:  torch.Size([5, 15, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (64) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/Henrique/Desktop/EPFL_MA3/DL_biomedic/Biomedicine-Project/fewshotbench/can_debug.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Henrique/Desktop/EPFL_MA3/DL_biomedic/Biomedicine-Project/fewshotbench/can_debug.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparameters)\n",
      "\u001b[1;32m/mnt/c/Users/Henrique/Desktop/EPFL_MA3/DL_biomedic/Biomedicine-Project/fewshotbench/can_debug.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Henrique/Desktop/EPFL_MA3/DL_biomedic/Biomedicine-Project/fewshotbench/can_debug.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Henrique/Desktop/EPFL_MA3/DL_biomedic/Biomedicine-Project/fewshotbench/can_debug.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# Implement training of the model. Remember to make use of functions defined in the `ProtoNet` and `MetaTemplate` class.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Henrique/Desktop/EPFL_MA3/DL_biomedic/Biomedicine-Project/fewshotbench/can_debug.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m epoch_loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_loop(epoch, train_loader, optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Henrique/Desktop/EPFL_MA3/DL_biomedic/Biomedicine-Project/fewshotbench/can_debug.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m train_losses\u001b[39m.\u001b[39mappend(epoch_loss)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Henrique/Desktop/EPFL_MA3/DL_biomedic/Biomedicine-Project/fewshotbench/can_debug.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# Evaluate test performance for epoch. Remember to make use of functions defined in the `ProtoNet` and `MetaTemplate` class.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/Henrique/Desktop/EPFL_MA3/DL_biomedic/Biomedicine-Project/fewshotbench/methods/meta_template.py:92\u001b[0m, in \u001b[0;36mMetaTemplate.train_loop\u001b[0;34m(self, epoch, train_loader, optimizer)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_way \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     91\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 92\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_forward_loss(x)\n\u001b[1;32m     93\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     94\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;32m/mnt/c/Users/Henrique/Desktop/EPFL_MA3/DL_biomedic/Biomedicine-Project/fewshotbench/can_debug.ipynb Cell 12\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Henrique/Desktop/EPFL_MA3/DL_biomedic/Biomedicine-Project/fewshotbench/can_debug.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m y_query \u001b[39m=\u001b[39m Variable(y_query\u001b[39m.\u001b[39mcuda())\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Henrique/Desktop/EPFL_MA3/DL_biomedic/Biomedicine-Project/fewshotbench/can_debug.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39m#scores, cls_scores = self.set_forward(x)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Henrique/Desktop/EPFL_MA3/DL_biomedic/Biomedicine-Project/fewshotbench/can_debug.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m cls_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_forward(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Henrique/Desktop/EPFL_MA3/DL_biomedic/Biomedicine-Project/fewshotbench/can_debug.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39m#l1 = self.loss_fn(scores, y_query )\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Henrique/Desktop/EPFL_MA3/DL_biomedic/Biomedicine-Project/fewshotbench/can_debug.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=90'>91</a>\u001b[0m l2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn(cls_scores, y_query )\n",
      "\u001b[1;32m/mnt/c/Users/Henrique/Desktop/EPFL_MA3/DL_biomedic/Biomedicine-Project/fewshotbench/can_debug.ipynb Cell 12\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Henrique/Desktop/EPFL_MA3/DL_biomedic/Biomedicine-Project/fewshotbench/can_debug.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mftrain: \u001b[39m\u001b[39m\"\u001b[39m, ftrain_norm\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Henrique/Desktop/EPFL_MA3/DL_biomedic/Biomedicine-Project/fewshotbench/can_debug.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m# Calculate cls_scores by taking the matrix product of ftest_norm and ftrain_norm (transposed)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Henrique/Desktop/EPFL_MA3/DL_biomedic/Biomedicine-Project/fewshotbench/can_debug.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m cls_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_cls \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(ftest_norm, ftrain_norm\u001b[39m.\u001b[39;49mT)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Henrique/Desktop/EPFL_MA3/DL_biomedic/Biomedicine-Project/fewshotbench/can_debug.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# y_test cannot be defined here because we don't have the info for it, it should be in another method\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Henrique/Desktop/EPFL_MA3/DL_biomedic/Biomedicine-Project/fewshotbench/can_debug.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m#ftest = torch.matmul(ftest, ytest) \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Henrique/Desktop/EPFL_MA3/DL_biomedic/Biomedicine-Project/fewshotbench/can_debug.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39m#ytest = self.linear(ftest)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Henrique/Desktop/EPFL_MA3/DL_biomedic/Biomedicine-Project/fewshotbench/can_debug.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m dists \u001b[39m=\u001b[39m euclidean_dist(z_query_attention, z_proto_attention)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (64) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "train_model(**parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
