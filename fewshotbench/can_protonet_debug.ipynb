{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by exercises 8\n",
    "from abc import abstractmethod, ABC\n",
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from IPython.display import clear_output\n",
    "#from backbones.fcnet import FCNet\n",
    "from fcnet import FCNet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  EXISTS: go-basic.obo\n",
      "go-basic.obo: fmt(1.2) rel(2023-06-11) 46,420 Terms; optional_attrs(relationship)\n"
     ]
    }
   ],
   "source": [
    "from datasets.prot.utils import get_samples_using_ic, check_min_samples, get_mode_ids, encodings, get_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaTemplate(nn.Module):\n",
    "    def __init__(self, backbone, n_way, n_support):\n",
    "        super(MetaTemplate, self).__init__()\n",
    "        self.n_way = n_way\n",
    "        self.n_support = n_support\n",
    "        self.n_query = -1  # (change depends on input)\n",
    "        self.feature = backbone\n",
    "        self.feat_dim = self.feature.final_feat_dim\n",
    "\n",
    "        # n_way = nb_classes per episode\n",
    "        # n_support = nb_samples per class for support set\n",
    "        # n_query = nb_samples per class for query set\n",
    "        # backbone = feature extractor (embedding network) ie. function f\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_forward(self, x, is_feature=False):\n",
    "        \"\"\"\n",
    "        forward pass, returns score (probabilities for query set)\n",
    "        output is logits for all query set (prob of each class)\n",
    "        first dim of output is n_way * n_query, nb of samples in query set\n",
    "        second dim is prob to belong to each class\n",
    "        x: [n_way, n_support + n_query, **embedding_dim]\n",
    "        out: [n_way * n_query, n_way]\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_forward_loss(self, x):\n",
    "        \"\"\"\n",
    "        takes the episode and compute loss of episode\n",
    "        x: [n_way, n_support + n_query, **embedding_dim]\n",
    "        out: loss (scalar)\n",
    "        \"\"\"\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.feature.forward(x)\n",
    "        return out\n",
    "\n",
    "    def parse_feature(self, x):\n",
    "        '''\n",
    "        create embeddings for support and query set\n",
    "        :param x: [n_way, n_support + n_query, **embedding_dim]\n",
    "        out: z_supp, z_queryÂ¨\n",
    "            z_supp: [n_way, n_support, feat_dim]\n",
    "            z_query: [n_way, n_query, feat_dim]\n",
    "        '''\n",
    "        x = Variable(x.to(self.device))\n",
    "        # reshape x to create one batch of size n_way * (n_support + n_query) and of dim whatever is dim of x\n",
    "        # x is of shape [n_way, n_support + n_query, **embedding_dim] originally, we have to reshape it to pass it to the NN ie. shape (batch_size, dim_size)\n",
    "        # note: x.contigous is used to make sure that is in the same place in mem (more efficient)\n",
    "        x = x.contiguous().view(self.n_way * (self.n_support + self.n_query), * x.size()[2:])\n",
    "        # Compute support and query feature.\n",
    "        z_all = self.forward(x)\n",
    "\n",
    "        # Reverse the transformation to distribute the samples based on the dimensions of their individual categories and flatten the embeddings.\n",
    "        # transformation is the transformation just above to one batch, ie. transform to original shape [n_way, n_support + n_query, **embedding_dim]\n",
    "        z_all = z_all.view(self.n_way, self.n_support + self.n_query, -1)\n",
    "\n",
    "        # Extract the support and query features.\n",
    "        z_support = z_all[:, :self.n_support, :]\n",
    "        z_query = z_all[:, self.n_support:, :]\n",
    "\n",
    "        return z_support, z_query\n",
    "\n",
    "    def correct(self, x):\n",
    "        # Compute the predictions scores.\n",
    "        scores = self.set_forward(x)\n",
    "\n",
    "        # Compute the top1 elements.\n",
    "        topk_scores, topk_labels = scores.data.topk(k=1, dim=1, largest=True, sorted=True)\n",
    "\n",
    "        # Detach the variables (transforming to numpy also detach the tensor)\n",
    "        topk_ind = topk_labels.cpu().numpy()\n",
    "\n",
    "        # Create the category labels for the queries, this is unique for the few shot learning setup\n",
    "        y_query = np.repeat(range(self.n_way), self.n_query)\n",
    "\n",
    "        #>>> np.repeat(range(10), 2)\n",
    "        #array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9])\n",
    "\n",
    "        # Compute number of elements that are correctly classified.\n",
    "        top1_correct = np.sum(topk_ind[:, 0] == y_query)\n",
    "        return float(top1_correct), len(y_query)\n",
    "\n",
    "    def train_loop(self, epoch, train_loader, optimizer):\n",
    "        print_freq = 10\n",
    "\n",
    "        avg_loss = 0\n",
    "        for i, (x, _) in enumerate(train_loader):\n",
    "            self.n_query = x.size(1) - self.n_support\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.set_forward_loss(x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss = avg_loss + loss.item()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                print('Epoch {:d} | Batch {:d}/{:d} | Loss {:f}'.format(epoch, i, len(train_loader),\n",
    "                                                                        avg_loss / float(i + 1)))\n",
    "        return avg_loss/len(train_loader)\n",
    "    \n",
    "    def test_loop(self, epoch, test_loader, record=None, return_std=False):\n",
    "        acc_all = []\n",
    "\n",
    "        iter_num = len(test_loader)\n",
    "        for i, (x, _) in enumerate(test_loader):\n",
    "            self.n_query = x.size(1) - self.n_support\n",
    "            correct_this, count_this = self.correct(x)\n",
    "            acc_all.append(correct_this / count_this * 100)\n",
    "\n",
    "        acc_all = np.asarray(acc_all)\n",
    "        acc_mean = np.mean(acc_all)\n",
    "        acc_std = np.std(acc_all)\n",
    "        print(f'Epoch {epoch} | Test Acc = {acc_mean:4.2f}% +- {1.96 * acc_std / np.sqrt(iter_num):4.2f}%')\n",
    "\n",
    "        if return_std:\n",
    "            return acc_mean, acc_std\n",
    "        else:\n",
    "            return acc_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProtoNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoNet(MetaTemplate):\n",
    "    def __init__(self, backbone, n_way, n_support):\n",
    "        super(ProtoNet, self).__init__(backbone, n_way, n_support)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def set_forward(self, x):\n",
    "        # Compute the prototypes (support) and queries (embeddings) for each datapoint.\n",
    "        # Remember that you implemented a function to compute this before.\n",
    "        z_support, z_query = self.parse_feature(x)\n",
    "            \n",
    "        # Compute the prototype.\n",
    "        z_support = z_support.contiguous().view(self.n_way, self.n_support, -1)\n",
    "        z_proto = z_support.mean(dim=1)\n",
    "        \n",
    "        # Format the queries for the similarity computation.\n",
    "        z_query = z_query.contiguous().view(self.n_way * self.n_query, -1)\n",
    "\n",
    "        # Compute similarity score based on the euclidean distance between prototypes and queries.\n",
    "        scores = -euclidean_dist(z_query, z_proto)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def set_forward_loss(self, x):\n",
    "        # Compute the similarity scores between the prototypes and the queries.\n",
    "        scores = self.set_forward(x)\n",
    "        \n",
    "        # Create the category labels for the queries.\n",
    "        y_query = torch.from_numpy(np.repeat(range(self.n_way), self.n_query))\n",
    "        y_query = Variable(y_query).to(self.device)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = self.loss_fn(scores, y_query)\n",
    "        return loss\n",
    "\n",
    "def euclidean_dist( x, y):\n",
    "    # x: N x D\n",
    "    # y: M x D\n",
    "    n = x.size(0)\n",
    "    m = y.size(0)\n",
    "    d = x.size(1)\n",
    "    assert d == y.size(1)\n",
    "\n",
    "    x = x.unsqueeze(1).expand(n, m, d)\n",
    "    y = y.unsqueeze(0).expand(n, m, d)\n",
    "\n",
    "    return torch.pow(x - y, 2).sum(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropyLoss, self).__init__()\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs, targets): # inputs: torch.Size([75, 59, 64]), targets: torch.Size([75])\n",
    "        inputs = inputs.view(inputs.size(0), inputs.size(1), -1)\n",
    "        # inputs: torch.Size([75, 59, 64])\n",
    "        log_probs = self.logsoftmax(inputs)\n",
    "\n",
    "        # below = problematic line\n",
    "        # torch zeros (75, 59)\n",
    "        targets = torch.zeros(inputs.size(0), inputs.size(1)).scatter_(1, targets.unsqueeze(1).data.cpu(), 1)\n",
    "        targets = targets.unsqueeze(-1)\n",
    "        targets = targets.cuda()\n",
    "        loss = (- targets * log_probs).mean(0).sum() \n",
    "        return loss / inputs.size(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CanNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CanNet(MetaTemplate):\n",
    "    def __init__(self, backbone, n_way, n_support, reduction_ratio=6, temperature=0.025, scale_cls=7, num_classes=7195):\n",
    "        super(CanNet, self).__init__(backbone, n_way, n_support)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.m = self.feat_dim\n",
    "        self.num_classes = num_classes\n",
    "        #self.linear = nn.Linear(self.m, n_way)\n",
    "        #self.linear = nn.Linear(self.m, self.num_classes)\n",
    "        self.linear = nn.Linear(1, self.num_classes)\n",
    "        self.fusion_conv = nn.Conv1d(self.feat_dim, 1, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm1d(int(self.feat_dim / reduction_ratio))\n",
    "        self.w1 = nn.Linear(self.m, int(self.m / reduction_ratio))\n",
    "        self.activation = nn.ReLU()\n",
    "        self.w2 = nn.Linear(int(self.m / reduction_ratio), self.m)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.temperature = temperature\n",
    "        self.scale_cls = scale_cls\n",
    "        self.weight_factor = 0.5\n",
    "        self.cosine_distance = nn.CosineSimilarity(dim=2, eps=1e-6)\n",
    "\n",
    "    def set_forward(self, x):\n",
    "        # Compute the prototypes (support) and queries (embeddings) for each datapoint.\n",
    "        z_support, z_query = self.parse_feature(x)\n",
    "            \n",
    "        # Compute the prototype.\n",
    "        z_support = z_support.contiguous().view(self.n_way, self.n_support, -1)\n",
    "        z_proto = z_support.mean(dim=1)\n",
    "        \n",
    "        # Format the queries for the similarity computation.\n",
    "        z_query = z_query.contiguous().view(self.n_way * self.n_query, -1)\n",
    "        z_proto_attention, z_query_attention = self.cross_attention_module(z_proto, z_query)\n",
    "        \n",
    "        # ftest is used for the global classification loss, the second loss\n",
    "        ftest = z_query_attention\n",
    "        \n",
    "        # z_proto_attention: torch.Size([5, 75, 64]) ie. [n_way, n_query, feat_dim]\n",
    "        # z_query_attention: torch.Size([5, 75, 64]) ie. [n_way, n_query, feat_dim]\n",
    "        z_proto_attention_mean = z_proto_attention.mean(dim=1) # torch.Size([5, 64])\n",
    "        z_query_attention_mean = z_query_attention.mean(dim=0) # torch.Size([75, 64])\n",
    "\n",
    "        # Compute similarity score based on the euclidean distance between prototypes and queries.\n",
    "        #scores = -euclidean_dist(z_query, z_proto)\n",
    "        scores = -euclidean_dist(z_query_attention_mean, z_proto_attention_mean)\n",
    "\n",
    "        # use cosine similarity instead of euclidean distance for the scores\n",
    "        #score_cosine = self.cosine_distance(z_query_attention_mean.unsqueeze(1), z_proto_attention_mean.unsqueeze(0))\n",
    "        #scores = score_cosine\n",
    "\n",
    "        return scores, ftest\n",
    "\n",
    "    def set_forward_loss(self, x, y_true_query):\n",
    "        # Compute the similarity scores between the prototypes and the queries.\n",
    "        scores, ftest = self.set_forward(x)\n",
    "        \n",
    "        # Create the category labels for the queries.\n",
    "        y_query = torch.from_numpy(np.repeat(range(self.n_way), self.n_query))\n",
    "        y_query = Variable(y_query).to(self.device)\n",
    "\n",
    "        # Compute the knn loss (base protonet loss)\n",
    "        l1 = self.loss_fn(scores, y_query)\n",
    "\n",
    "        # Compute the global classification loss\n",
    "\n",
    "        def one_hot(labels_train):\n",
    "            \"\"\"\n",
    "            Turn the labels_train to one-hot encoding.\n",
    "            Args:\n",
    "                labels_train: [batch_size, num_train_examples]\n",
    "            Return:\n",
    "                labels_train_1hot: [batch_size, num_train_examples, K]\n",
    "            \"\"\"\n",
    "            labels_train = labels_train.cpu()\n",
    "            nKnovel = 1 + labels_train.max()\n",
    "            labels_train_1hot_size = list(labels_train.size()) + [nKnovel,]\n",
    "            labels_train_unsqueeze = labels_train.unsqueeze(dim=labels_train.dim())\n",
    "            labels_train_1hot = torch.zeros(labels_train_1hot_size).scatter_(len(labels_train_1hot_size) - 1, labels_train_unsqueeze, 1)\n",
    "            return labels_train_1hot\n",
    "\n",
    "        y_query_one_hot = one_hot(y_query).cuda()\n",
    "        # ftest is of shape (5, 75, 64), change it to (1, 75, 64, 5) to be able to do matmul\n",
    "        ftest = ftest.unsqueeze(0) # torch.Size([1, 5, 75, 64])\n",
    "        ftest = ftest.transpose(2, 3) # torch.Size([1, 5, 64, 75])\n",
    "        ftest = ftest.transpose(1, 3) # torch.Size([1, 75, 64, 5])\n",
    "        \n",
    "        # this matmul is incorrect should be ftest: (1, 75, 64, 5) and y_query_one_hot: (1, 75, 5, 1)\n",
    "        y_query_one_hot = y_query_one_hot.unsqueeze(0) # torch.Size([1, 5, 75, 5])\n",
    "        y_query_one_hot = y_query_one_hot.unsqueeze(3) # torch.Size([1, 5, 75, 5, 1])\n",
    "        ftest = torch.matmul(ftest, y_query_one_hot) # torch.Size([1, 75, 64, 1])\n",
    "        ftest = ftest.view(-1, self.m) # torch.Size([75, 64])\n",
    "\n",
    "        ftest = ftest.unsqueeze(2) # torch.Size([75, 64, 1])\n",
    "\n",
    "        ytest = self.linear(ftest) # torch.Size([75, 64, 59])\n",
    "        ytest = ytest.transpose(2, 1) # torch.Size([75, 59, 64])\n",
    "        y_true_query = y_true_query.reshape(-1) #torch.Size([75])\n",
    "\n",
    "        # special loss used in the paper\n",
    "        criterion = CrossEntropyLoss()\n",
    "\n",
    "        # compute the global classification loss\n",
    "        l2 = criterion(ytest, y_true_query)\n",
    "        loss = self.weight_factor * l1 + l2\n",
    "\n",
    "        return loss\n",
    "    def fusion_layer(self, z):\n",
    "        \"\"\"\n",
    "        Generates cross attention map A\n",
    "        :param R: [n_dim,n_dim]\n",
    "        :return: A  [n_dim]\n",
    "        \"\"\"\n",
    "\n",
    "        GAP = torch.mean(z, dim=-2)\n",
    "\n",
    "        w = self.w2(self.activation(self.w1(GAP)))\n",
    "\n",
    "\n",
    "        fusion = z * w.unsqueeze(2)\n",
    "\n",
    "        conv = torch.mean(fusion,dim=-1)\n",
    "\n",
    "        A = self.softmax(conv/self.temperature)\n",
    "\n",
    "        return A\n",
    "\n",
    "    def cross_attention_module(self, z_support, z_query):\n",
    "\n",
    "        def correlation_layer(z_support, z_query): \n",
    "            \"\"\"\n",
    "            Takes 1 support embedding and 1 query embedding and returns correlation map\n",
    "            ie. P and Q in the paper. P is [P1, P2, ..., Pn] where n is the dimension of the embeddings, same for Q.\n",
    "            :param z_support: [n_dim] ie. P\n",
    "            :param z_query: [n_dim] ie. Q\n",
    "            :return: correlation_map: [n_dim, n_dim]. Note: we use R^q = correlation_map and R^p = correlation_map.T \n",
    "            \"\"\"\n",
    "\n",
    "            # compute cosine similarity between support and query embeddings\n",
    "            P = F.normalize(z_support, p=2, dim=-1, eps=1e-12)\n",
    "            Q = F.normalize(z_query, p=2, dim=-1, eps=1e-12)\n",
    "            P = z_support\n",
    "            Q = z_query\n",
    "\n",
    "            correlation_map = torch.einsum(\"ij,kl->ikjl\",P,Q)  # dim: [n_dim, n_dim]\n",
    "\n",
    "            return correlation_map\n",
    "\n",
    "        P_k = z_support\n",
    "        Q_b = z_query\n",
    "\n",
    "        # compute correlation map\n",
    "        R_p = correlation_layer(P_k, Q_b)\n",
    "        R_q = R_p.transpose(2, 3)\n",
    "\n",
    "        # compute fusion layer\n",
    "        A_p = self.fusion_layer(R_p)\n",
    "        A_q = self.fusion_layer(R_q)\n",
    "        P_bk = P_k.unsqueeze(1) * (1 + A_p)\n",
    "        Q_bk = Q_b.unsqueeze(0) * (1 + A_q)\n",
    "\n",
    "        return P_bk, Q_bk\n",
    "\n",
    "    def train_loop(self, epoch, train_loader, optimizer):\n",
    "        \"\"\"\n",
    "        Same training loop as base, but added labels for the global classification loss\n",
    "        \"\"\"\n",
    "        print_freq = 10\n",
    "\n",
    "        avg_loss = 0\n",
    "        self.train()\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            y_all = y.cuda()\n",
    "\n",
    "            # y true query is the global labels for the query set\n",
    "            y_true_query = y_all[:, self.n_support:]\n",
    "\n",
    "            self.n_query = x.size(1) - self.n_support\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.set_forward_loss(x, y_true_query)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss = avg_loss + loss.item()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                print('Epoch {:d} | Batch {:d}/{:d} | Loss {:f}'.format(epoch, i, len(train_loader),\n",
    "                                                                        avg_loss / float(i + 1)))\n",
    "        return avg_loss/len(train_loader)\n",
    "\n",
    "    def correct(self, x):\n",
    "        # Compute the predictions scores.\n",
    "        scores, _ = self.set_forward(x)\n",
    "\n",
    "        # Compute the top1 elements.\n",
    "        topk_scores, topk_labels = scores.data.topk(k=1, dim=1, largest=True, sorted=True)\n",
    "\n",
    "        # Detach the variables (transforming to numpy also detach the tensor)\n",
    "        topk_ind = topk_labels.cpu().numpy()\n",
    "\n",
    "        # Create the category labels for the queries, this is unique for the few shot learning setup\n",
    "        y_query = np.repeat(range(self.n_way), self.n_query)\n",
    "\n",
    "        #>>> np.repeat(range(10), 2)\n",
    "        #array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9])\n",
    "\n",
    "        # Compute number of elements that are correctly classified.\n",
    "        top1_correct = np.sum(topk_ind[:, 0] == y_query)\n",
    "        return float(top1_correct), len(y_query)\n",
    "def euclidean_dist( x, y):\n",
    "    # x: N x D\n",
    "    # y: M x D\n",
    "    n = x.size(0)\n",
    "    m = y.size(0)\n",
    "    d = x.size(1)\n",
    "    assert d == y.size(1)\n",
    "\n",
    "    x = x.unsqueeze(1).expand(n, m, d)\n",
    "    y = y.unsqueeze(0).expand(n, m, d)\n",
    "\n",
    "    return torch.pow(x - y, 2).sum(2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Special class for few-shot dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.check_init()\n",
    "\n",
    "    def check_init(self):\n",
    "        \"\"\"\n",
    "        Convenience function to check that the FewShotDataset is properly configured.\n",
    "        \"\"\"\n",
    "        required_attrs = ['_dataset_name', '_data_dir']\n",
    "        for attr in required_attrs:\n",
    "            if not hasattr(self, attr):\n",
    "                raise ValueError(f'FewShotDataset must have attribute {attr}.')\n",
    "\n",
    "        if not os.path.exists(self._data_dir):\n",
    "            raise ValueError(\n",
    "                f'{self._data_dir} does not exist yet. Please generate/download the dataset first.')\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def __getitem__(self, i):\n",
    "        return NotImplemented\n",
    "\n",
    "    @abstractmethod\n",
    "    def __len__(self):\n",
    "        return NotImplemented\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def dim(self):\n",
    "        return NotImplemented\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_data_loader(self, mode='train') -> DataLoader:\n",
    "        return NotImplemented\n",
    "\n",
    "    @property\n",
    "    def dataset_name(self):\n",
    "        \"\"\"\n",
    "        A string that identifies the dataset, e.g., 'swissprot'\n",
    "        \"\"\"\n",
    "        return self._dataset_name\n",
    "\n",
    "    @property\n",
    "    def data_dir(self):\n",
    "        return self._data_dir\n",
    "\n",
    "    def initialize_data_dir(self, root_dir):\n",
    "        os.makedirs(root_dir, exist_ok=True)\n",
    "        #self._data_dir = os.path.join(root_dir, self._dataset_name)\n",
    "        self._data_dir = \"data/swissprot\"\n",
    "\n",
    "class SPDataset(FewShotDataset, ABC):\n",
    "    _dataset_name = 'swissprot'\n",
    "\n",
    "    def load_swissprot(self, level = 5, mode='train', min_samples = 20):\n",
    "        samples = get_samples_using_ic(root = self.data_dir)\n",
    "        samples = check_min_samples(samples, min_samples)\n",
    "        unique_ids = set(get_mode_ids(samples)[mode])\n",
    "        return [sample for sample in samples if sample.annot in unique_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROTDIM = 1280\n",
    "\n",
    "class SubDataset(Dataset):\n",
    "    def __init__(self, samples, data_dir):\n",
    "        self.samples = samples\n",
    "        self.encoder = encodings(data_dir)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        sample = self.samples[i]\n",
    "        return sample.input_seq, self.encoder[sample.annot]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return PROTDIM\n",
    "\n",
    "class EpisodicBatchSampler(object):\n",
    "    def __init__(self, n_classes, n_way, n_episodes):\n",
    "        self.n_classes = n_classes\n",
    "        self.n_way = n_way\n",
    "        self.n_episodes = n_episodes\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_episodes\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(self.n_episodes):\n",
    "            yield torch.randperm(self.n_classes)[:self.n_way]\n",
    "\n",
    "class SPSetDataset(SPDataset):\n",
    "    def __init__(self, n_way, n_support, n_query, n_episode=100, root='./data', mode='train'):\n",
    "        self.initialize_data_dir(root)\n",
    "\n",
    "        self.n_way = n_way\n",
    "        self.n_episode = n_episode\n",
    "        min_samples = n_support + n_query\n",
    "        self.encoder = encodings(self.data_dir)\n",
    "\n",
    "        # check if samples_all.pkl exists\n",
    "        if os.path.exists('samples_all.pkl'):\n",
    "            # load samples_all using pickle\n",
    "            with open('samples_all.pkl', 'rb') as f:\n",
    "                samples_all = pickle.load(f)\n",
    "        else:\n",
    "            samples_all = self.load_swissprot(mode = mode, min_samples = min_samples)\n",
    "\n",
    "            # save samples_all using pickle\n",
    "            with open('samples_all.pkl', 'wb') as f:\n",
    "                pickle.dump(samples_all, f)\n",
    "            \n",
    "\n",
    "\n",
    "        self.categories = get_ids(samples_all) # Unique annotations\n",
    "        self.x_dim = PROTDIM\n",
    "\n",
    "        self.sub_dataloader = []\n",
    "\n",
    "        sub_data_loader_params = dict(batch_size=min_samples,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=0,  # use main thread only or may receive multiple batches\n",
    "                                      pin_memory=False)\n",
    "\n",
    "        # Create the sub datasets for each annotation of the categories and collect all the dataloaders in `self.sub_dataloader`.\n",
    "        for annotation in self.categories:\n",
    "            samples = [sample for sample in samples_all if sample.annot == annotation]\n",
    "            sub_dataset = SubDataset(samples, self.data_dir)\n",
    "            self.sub_dataloader.append(torch.utils.data.DataLoader(sub_dataset, **sub_data_loader_params))\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return next(iter(self.sub_dataloader[i]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.categories)\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self.x_dim\n",
    "\n",
    "    def get_data_loader(self) -> DataLoader:\n",
    "        sampler = EpisodicBatchSampler(len(self), self.n_way, self.n_episode)\n",
    "        data_loader_params = dict(batch_sampler=sampler, num_workers=0, pin_memory=True)\n",
    "        \n",
    "        # check if data_loader.pkl exists\n",
    "        if os.path.exists('data_loader.pkl'):\n",
    "            # load data_loader using pickle\n",
    "            with open('data_loader.pkl', 'rb') as f:\n",
    "                data_loader = pickle.load(f)\n",
    "        else:\n",
    "            data_loader = torch.utils.data.DataLoader(self, **data_loader_params)\n",
    "        \n",
    "            # save data_loader using pickle\n",
    "            with open('data_loader.pkl', 'wb') as f:\n",
    "                pickle.dump(data_loader, f)\n",
    "        return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_way, n_support, n_query, n_train_episode):\n",
    "    # Load train dataset. Remember to use make use of functions defined in the `SPSetDataset`.\n",
    "    \n",
    "    if os.path.exists('train_dataset.pkl'):\n",
    "        # load train_dataset using pickle\n",
    "        with open('train_dataset.pkl', 'rb') as f:\n",
    "            train_dataset = pickle.load(f)\n",
    "    else:\n",
    "        train_dataset = SPSetDataset(n_way, n_support, n_query, n_episode=n_train_episode, root='./data', mode='train')\n",
    "        # save as pickle\n",
    "        with open('train_dataset.pkl', 'wb') as f:\n",
    "            pickle.dump(train_dataset, f)\n",
    "    train_loader = train_dataset.get_data_loader()\n",
    "\n",
    "    # Load test dataset. Remember to use make use of functions defined in the `SPSetDataset`.\n",
    "    if os.path.exists('test_dataset.pkl'):\n",
    "        # load test_dataset using pickle\n",
    "        with open('test_dataset.pkl', 'rb') as f:\n",
    "            test_dataset = pickle.load(f)\n",
    "    else:\n",
    "        test_dataset = SPSetDataset(n_way, n_support, n_query, n_episode=100, root='./data', mode='test')\n",
    "        # save as pickle\n",
    "        with open('test_dataset.pkl', 'wb') as f:\n",
    "            pickle.dump(test_dataset, f)\n",
    "    test_loader =  test_dataset.get_data_loader()\n",
    "\n",
    "    # Initialize a fully connected network `FCNet` in `fcnet.py` with two hidden layers of 512 units each as feature extractor.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    backbone = FCNet(train_dataset.dim).to(device)\n",
    "    print(\"traindataset_dim: \", train_dataset.dim)\n",
    "\n",
    "\n",
    "    # Initialize model using the backbone and the optimizer.\n",
    "    model = CanNet(backbone, n_way, n_support).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    test_accs = []; train_losses = []\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "\n",
    "        # Implement training of the model. Remember to make use of functions defined in the `ProtoNet` and `MetaTemplate` class.\n",
    "        epoch_loss = model.train_loop(epoch, train_loader, optimizer)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # Evaluate test performance for epoch. Remember to make use of functions defined in the `ProtoNet` and `MetaTemplate` class.\n",
    "        test_acc = model.test_loop(epoch, test_loader)\n",
    "        test_accs.append(test_acc)\n",
    "        print(f'Epoch {epoch} | Train Loss {epoch_loss} | Test Acc {test_acc}')\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 2.5))\n",
    "    ax1.plot(range(len(train_losses)), train_losses)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Train Loss')\n",
    "    ax1.grid()\n",
    "\n",
    "    ax2.plot(range(len(test_accs)), test_accs)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Test Accuracy')\n",
    "    ax2.grid()\n",
    "    fig.suptitle(f\"n_way={n_way}, n_support={n_support}, n_query={n_query}, n_train_episode={n_train_episode}\")\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_way': 5, 'n_support': 5, 'n_query': 15, 'n_train_episode': 5}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traindataset_dim:  1280\n",
      "Epoch 0 | Batch 0/5 | Loss 10.643782\n",
      "Epoch 0 | Test Acc = 55.20% +- 10.73%\n",
      "Epoch 0 | Train Loss 10.862957191467284 | Test Acc 55.2\n",
      "Epoch 1 | Batch 0/5 | Loss 11.428755\n",
      "Epoch 1 | Test Acc = 42.67% +- 8.84%\n",
      "Epoch 1 | Train Loss 11.501664733886718 | Test Acc 42.666666666666664\n",
      "Epoch 2 | Batch 0/5 | Loss 10.210355\n",
      "Epoch 2 | Test Acc = 63.47% +- 12.60%\n",
      "Epoch 2 | Train Loss 10.370027542114258 | Test Acc 63.46666666666666\n",
      "Epoch 3 | Batch 0/5 | Loss 10.639069\n",
      "Epoch 3 | Test Acc = 54.93% +- 7.14%\n",
      "Epoch 3 | Train Loss 10.792311859130859 | Test Acc 54.93333333333332\n",
      "Epoch 4 | Batch 0/5 | Loss 10.012110\n",
      "Epoch 4 | Test Acc = 60.53% +- 6.80%\n",
      "Epoch 4 | Train Loss 10.286305999755859 | Test Acc 60.53333333333334\n",
      "Epoch 5 | Batch 0/5 | Loss 10.752545\n",
      "Epoch 5 | Test Acc = 64.00% +- 8.96%\n",
      "Epoch 5 | Train Loss 10.2491943359375 | Test Acc 64.0\n",
      "Epoch 6 | Batch 0/5 | Loss 9.901505\n",
      "Epoch 6 | Test Acc = 52.00% +- 5.38%\n",
      "Epoch 6 | Train Loss 10.231465148925782 | Test Acc 52.0\n",
      "Epoch 7 | Batch 0/5 | Loss 9.985037\n",
      "Epoch 7 | Test Acc = 62.13% +- 6.21%\n",
      "Epoch 7 | Train Loss 10.397839164733886 | Test Acc 62.13333333333334\n",
      "Epoch 8 | Batch 0/5 | Loss 10.621439\n",
      "Epoch 8 | Test Acc = 58.13% +- 6.59%\n",
      "Epoch 8 | Train Loss 10.468852424621582 | Test Acc 58.13333333333334\n",
      "Epoch 9 | Batch 0/5 | Loss 10.668059\n",
      "Epoch 9 | Test Acc = 57.60% +- 7.91%\n",
      "Epoch 9 | Train Loss 10.03263053894043 | Test Acc 57.6\n",
      "Epoch 10 | Batch 0/5 | Loss 9.862883\n",
      "Epoch 10 | Test Acc = 66.13% +- 2.82%\n",
      "Epoch 10 | Train Loss 10.087672805786132 | Test Acc 66.13333333333333\n",
      "Epoch 11 | Batch 0/5 | Loss 9.791697\n",
      "Epoch 11 | Test Acc = 58.13% +- 11.22%\n",
      "Epoch 11 | Train Loss 10.306552314758301 | Test Acc 58.13333333333334\n",
      "Epoch 12 | Batch 0/5 | Loss 10.561711\n",
      "Epoch 12 | Test Acc = 64.00% +- 3.05%\n",
      "Epoch 12 | Train Loss 10.67226734161377 | Test Acc 64.0\n",
      "Epoch 13 | Batch 0/5 | Loss 9.565694\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 42\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(n_way, n_support, n_query, n_train_episode)\u001b[0m\n\u001b[1;32m     39\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Implement training of the model. Remember to make use of functions defined in the `ProtoNet` and `MetaTemplate` class.\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(epoch_loss)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Evaluate test performance for epoch. Remember to make use of functions defined in the `ProtoNet` and `MetaTemplate` class.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 177\u001b[0m, in \u001b[0;36mCanNet.train_loop\u001b[0;34m(self, epoch, train_loader, optimizer)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_query \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_support\n\u001b[1;32m    176\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 177\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_forward_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    179\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[7], line 101\u001b[0m, in \u001b[0;36mCanNet.set_forward_loss\u001b[0;34m(self, x, y_true_query)\u001b[0m\n\u001b[1;32m     98\u001b[0m criterion \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# compute the global classification loss\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m l2 \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mytest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_factor \u001b[38;5;241m*\u001b[39m l1 \u001b[38;5;241m+\u001b[39m l2\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/fewshotbench/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m      9\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogsoftmax(inputs)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# below = problematic line\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# torch zeros (75, 59)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mscatter_(\u001b[38;5;241m1\u001b[39m, \u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mcuda()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(**parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fewshotbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
