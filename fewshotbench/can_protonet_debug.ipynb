{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by exercises 8\n",
    "from abc import abstractmethod, ABC\n",
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from IPython.display import clear_output\n",
    "#from backbones.fcnet import FCNet\n",
    "from fcnet import FCNet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.prot.utils import get_samples_using_ic, check_min_samples, get_mode_ids, encodings, get_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaTemplate(nn.Module):\n",
    "    def __init__(self, backbone, n_way, n_support):\n",
    "        super(MetaTemplate, self).__init__()\n",
    "        self.n_way = n_way\n",
    "        self.n_support = n_support\n",
    "        self.n_query = -1  # (change depends on input)\n",
    "        self.feature = backbone\n",
    "        self.feat_dim = self.feature.final_feat_dim\n",
    "\n",
    "        # n_way = nb_classes per episode\n",
    "        # n_support = nb_samples per class for support set\n",
    "        # n_query = nb_samples per class for query set\n",
    "        # backbone = feature extractor (embedding network) ie. function f\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_forward(self, x, is_feature=False):\n",
    "        \"\"\"\n",
    "        forward pass, returns score (probabilities for query set)\n",
    "        output is logits for all query set (prob of each class)\n",
    "        first dim of output is n_way * n_query, nb of samples in query set\n",
    "        second dim is prob to belong to each class\n",
    "        x: [n_way, n_support + n_query, **embedding_dim]\n",
    "        out: [n_way * n_query, n_way]\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_forward_loss(self, x):\n",
    "        \"\"\"\n",
    "        takes the episode and compute loss of episode\n",
    "        x: [n_way, n_support + n_query, **embedding_dim]\n",
    "        out: loss (scalar)\n",
    "        \"\"\"\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.feature.forward(x)\n",
    "        return out\n",
    "\n",
    "    def parse_feature(self, x):\n",
    "        '''\n",
    "        create embeddings for support and query set\n",
    "        :param x: [n_way, n_support + n_query, **embedding_dim]\n",
    "        out: z_supp, z_queryÂ¨\n",
    "            z_supp: [n_way, n_support, feat_dim]\n",
    "            z_query: [n_way, n_query, feat_dim]\n",
    "        '''\n",
    "        x = Variable(x.to(self.device))\n",
    "        # reshape x to create one batch of size n_way * (n_support + n_query) and of dim whatever is dim of x\n",
    "        # x is of shape [n_way, n_support + n_query, **embedding_dim] originally, we have to reshape it to pass it to the NN ie. shape (batch_size, dim_size)\n",
    "        # note: x.contigous is used to make sure that is in the same place in mem (more efficient)\n",
    "        x = x.contiguous().view(self.n_way * (self.n_support + self.n_query), * x.size()[2:])\n",
    "        # Compute support and query feature.\n",
    "        z_all = self.forward(x)\n",
    "\n",
    "        # Reverse the transformation to distribute the samples based on the dimensions of their individual categories and flatten the embeddings.\n",
    "        # transformation is the transformation just above to one batch, ie. transform to original shape [n_way, n_support + n_query, **embedding_dim]\n",
    "        z_all = z_all.view(self.n_way, self.n_support + self.n_query, -1)\n",
    "\n",
    "        # Extract the support and query features.\n",
    "        z_support = z_all[:, :self.n_support, :]\n",
    "        z_query = z_all[:, self.n_support:, :]\n",
    "\n",
    "        return z_support, z_query\n",
    "\n",
    "    def correct(self, x):\n",
    "        # Compute the predictions scores.\n",
    "        scores, _ = self.set_forward(x)\n",
    "\n",
    "        # Compute the top1 elements.\n",
    "        topk_scores, topk_labels = scores.data.topk(k=1, dim=1, largest=True, sorted=True)\n",
    "\n",
    "        # Detach the variables (transforming to numpy also detach the tensor)\n",
    "        topk_ind = topk_labels.cpu().numpy()\n",
    "\n",
    "        # Create the category labels for the queries, this is unique for the few shot learning setup\n",
    "        y_query = np.repeat(range(self.n_way), self.n_query)\n",
    "\n",
    "        #>>> np.repeat(range(10), 2)\n",
    "        #array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9])\n",
    "\n",
    "        # Compute number of elements that are correctly classified.\n",
    "        top1_correct = np.sum(topk_ind[:, 0] == y_query)\n",
    "        return float(top1_correct), len(y_query)\n",
    "\n",
    "    def train_loop(self, epoch, train_loader, optimizer):\n",
    "        print_freq = 10\n",
    "\n",
    "        avg_loss = 0\n",
    "        for i, (x, _) in enumerate(train_loader):\n",
    "            self.n_query = x.size(1) - self.n_support\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.set_forward_loss(x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss = avg_loss + loss.item()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                print('Epoch {:d} | Batch {:d}/{:d} | Loss {:f}'.format(epoch, i, len(train_loader),\n",
    "                                                                        avg_loss / float(i + 1)))\n",
    "        return avg_loss/len(train_loader)\n",
    "    \n",
    "    def test_loop(self, epoch, test_loader, record=None, return_std=False):\n",
    "        acc_all = []\n",
    "\n",
    "        iter_num = len(test_loader)\n",
    "        for i, (x, _) in enumerate(test_loader):\n",
    "            self.n_query = x.size(1) - self.n_support\n",
    "            correct_this, count_this = self.correct(x)\n",
    "            acc_all.append(correct_this / count_this * 100)\n",
    "\n",
    "        acc_all = np.asarray(acc_all)\n",
    "        acc_mean = np.mean(acc_all)\n",
    "        acc_std = np.std(acc_all)\n",
    "        print(f'Epoch {epoch} | Test Acc = {acc_mean:4.2f}% +- {1.96 * acc_std / np.sqrt(iter_num):4.2f}%')\n",
    "\n",
    "        if return_std:\n",
    "            return acc_mean, acc_std\n",
    "        else:\n",
    "            return acc_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can network 1 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoNet(MetaTemplate):\n",
    "    def __init__(self, backbone, n_way, n_support):\n",
    "        super(ProtoNet, self).__init__(backbone, n_way, n_support)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def set_forward(self, x):\n",
    "        # Compute the prototypes (support) and queries (embeddings) for each datapoint.\n",
    "        # Remember that you implemented a function to compute this before.\n",
    "        z_support, z_query = self.parse_feature(x)\n",
    "            \n",
    "        # Compute the prototype.\n",
    "        z_support = z_support.contiguous().view(self.n_way, self.n_support, -1)\n",
    "        z_proto = z_support.mean(dim=1)\n",
    "        \n",
    "        # Format the queries for the similarity computation.\n",
    "        z_query = z_query.contiguous().view(self.n_way * self.n_query, -1)\n",
    "\n",
    "        # Compute similarity score based on the euclidean distance between prototypes and queries.\n",
    "        scores = -euclidean_dist(z_query, z_proto)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def set_forward_loss(self, x):\n",
    "        # Compute the similarity scores between the prototypes and the queries.\n",
    "        scores = self.set_forward(x)\n",
    "        \n",
    "        # Create the category labels for the queries.\n",
    "        y_query = torch.from_numpy(np.repeat(range(self.n_way), self.n_query))\n",
    "        y_query = Variable(y_query).to(self.device)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = self.loss_fn(scores, y_query)\n",
    "        return loss\n",
    "\n",
    "def euclidean_dist( x, y):\n",
    "    # x: N x D\n",
    "    # y: M x D\n",
    "    n = x.size(0)\n",
    "    m = y.size(0)\n",
    "    d = x.size(1)\n",
    "    assert d == y.size(1)\n",
    "\n",
    "    x = x.unsqueeze(1).expand(n, m, d)\n",
    "    y = y.unsqueeze(0).expand(n, m, d)\n",
    "\n",
    "    return torch.pow(x - y, 2).sum(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropyLoss, self).__init__()\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs, targets): # inputs: torch.Size([75, 59, 64]), targets: torch.Size([75])\n",
    "        inputs = inputs.view(inputs.size(0), inputs.size(1), -1)\n",
    "        # inputs: torch.Size([75, 59, 64])\n",
    "        log_probs = self.logsoftmax(inputs)\n",
    "\n",
    "        # below = problematic line\n",
    "        # torch zeros (75, 59)\n",
    "        targets = torch.zeros(inputs.size(0), inputs.size(1)).scatter_(1, targets.unsqueeze(1).data.cpu(), 1)\n",
    "        targets = targets.unsqueeze(-1)\n",
    "        targets = targets.cuda()\n",
    "        loss = (- targets * log_probs).mean(0).sum() \n",
    "        return loss / inputs.size(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CanNet(MetaTemplate):\n",
    "    def __init__(self, backbone, n_way, n_support, reduction_ratio=6, temperature=0.025, scale_cls=7, num_classes=7195):\n",
    "        super(CanNet, self).__init__(backbone, n_way, n_support)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.m = self.feat_dim\n",
    "        self.num_classes = num_classes\n",
    "        #self.linear = nn.Linear(self.m, n_way)\n",
    "        #self.linear = nn.Linear(self.m, self.num_classes)\n",
    "        self.linear = nn.Linear(1, self.num_classes)\n",
    "        self.fusion_conv = nn.Conv1d(self.feat_dim, 1, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm1d(int(self.feat_dim / reduction_ratio))\n",
    "        self.w1 = nn.Linear(self.m, int(self.m / reduction_ratio))\n",
    "        self.activation = nn.ReLU()\n",
    "        self.w2 = nn.Linear(int(self.m / reduction_ratio), self.m)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.temperature = temperature\n",
    "        self.scale_cls = scale_cls\n",
    "\n",
    "    def set_forward(self, x):\n",
    "        # Compute the prototypes (support) and queries (embeddings) for each datapoint.\n",
    "        # Remember that you implemented a function to compute this before.\n",
    "        z_support, z_query = self.parse_feature(x)\n",
    "            \n",
    "        # Compute the prototype.\n",
    "        z_support = z_support.contiguous().view(self.n_way, self.n_support, -1)\n",
    "        z_proto = z_support.mean(dim=1)\n",
    "        \n",
    "        # Format the queries for the similarity computation.\n",
    "        z_query = z_query.contiguous().view(self.n_way * self.n_query, -1)\n",
    "        z_proto_attention, z_query_attention = self.cross_attention_module(z_proto, z_query)\n",
    "        \n",
    "        ftest = z_query_attention\n",
    "        \n",
    "        # z_proto_attention: torch.Size([5, 75, 64]) ie. [n_way, n_query, feat_dim]\n",
    "        # z_query_attention: torch.Size([5, 75, 64]) ie. [n_way, n_query, feat_dim]\n",
    "        z_proto_attention = z_proto_attention.mean(dim=1)\n",
    "        z_query_attention = z_query_attention.mean(dim=0)\n",
    "\n",
    "        # z_query: torch.Size([75, 64]) ie. [n_query, feat_dim] \n",
    "        # Compute similarity score based on the euclidean distance between prototypes and queries.\n",
    "        #scores = -euclidean_dist(z_query, z_proto)\n",
    "        scores = -euclidean_dist(z_query_attention, z_proto_attention)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        return scores, ftest\n",
    "\n",
    "    def set_forward_loss(self, x, y_true_query):\n",
    "        # Compute the similarity scores between the prototypes and the queries.\n",
    "        scores, ftest = self.set_forward(x)\n",
    "        \n",
    "        # Create the category labels for the queries.\n",
    "        y_query = torch.from_numpy(np.repeat(range(self.n_way), self.n_query))\n",
    "        y_query = Variable(y_query).to(self.device)\n",
    "\n",
    "        # Compute the loss\n",
    "        l1 = self.loss_fn(scores, y_query)\n",
    "\n",
    "\n",
    "        def one_hot(labels_train):\n",
    "            \"\"\"\n",
    "            Turn the labels_train to one-hot encoding.\n",
    "            Args:\n",
    "                labels_train: [batch_size, num_train_examples]\n",
    "            Return:\n",
    "                labels_train_1hot: [batch_size, num_train_examples, K]\n",
    "            \"\"\"\n",
    "            labels_train = labels_train.cpu()\n",
    "            nKnovel = 1 + labels_train.max()\n",
    "            labels_train_1hot_size = list(labels_train.size()) + [nKnovel,]\n",
    "            labels_train_unsqueeze = labels_train.unsqueeze(dim=labels_train.dim())\n",
    "            labels_train_1hot = torch.zeros(labels_train_1hot_size).scatter_(len(labels_train_1hot_size) - 1, labels_train_unsqueeze, 1)\n",
    "            return labels_train_1hot\n",
    "\n",
    "        y_query_one_hot = one_hot(y_query).cuda()\n",
    "        # ftest is of shape (5, 75, 64), change it to (1, 75, 64, 5) to be able to do matmul\n",
    "        ftest = ftest.unsqueeze(0) # torch.Size([1, 5, 75, 64])\n",
    "        ftest = ftest.transpose(2, 3) # torch.Size([1, 5, 64, 75])\n",
    "        ftest = ftest.transpose(1, 3) # torch.Size([1, 75, 64, 5])\n",
    "        \n",
    "\n",
    "        # this matmul is incorrect should be ftest: (1, 75, 64, 5) and y_query_one_hot: (1, 75, 5, 1)\n",
    "        y_query_one_hot = y_query_one_hot.unsqueeze(0) # torch.Size([1, 5, 75, 5])\n",
    "        y_query_one_hot = y_query_one_hot.unsqueeze(3) # torch.Size([1, 5, 75, 5, 1])\n",
    "        ftest = torch.matmul(ftest, y_query_one_hot) # torch.Size([1, 75, 64, 1])\n",
    "        ftest = ftest.view(-1, self.m) # torch.Size([75, 64])\n",
    "\n",
    "        ftest = ftest.unsqueeze(2) # torch.Size([75, 64, 1])\n",
    "\n",
    "        ytest = self.linear(ftest) # torch.Size([75, 64, 59])\n",
    "        ytest = ytest.transpose(2, 1) # torch.Size([75, 59, 64])\n",
    "        y_true_query = y_true_query.reshape(-1) #torch.Size([75])\n",
    "        criterion = CrossEntropyLoss()\n",
    "        l2 = criterion(ytest, y_true_query)\n",
    "\n",
    "        loss = (l1 + l2) / 2\n",
    "\n",
    "        return loss\n",
    "    def fusion_layer(self, z):\n",
    "        \"\"\"\n",
    "        Generates cross attention map A\n",
    "        :param R: [n_dim,n_dim]\n",
    "        :return: A  [n_dim]\n",
    "        \"\"\"\n",
    "\n",
    "        GAP = torch.mean(z, dim=-2)\n",
    "\n",
    "        w = self.w2(self.activation(self.w1(GAP)))\n",
    "\n",
    "\n",
    "        fusion = z * w.unsqueeze(2)\n",
    "\n",
    "        conv = torch.mean(fusion,dim=-1)\n",
    "\n",
    "        A = self.softmax(conv/self.temperature)\n",
    "\n",
    "        return A\n",
    "\n",
    "    def cross_attention_module(self, z_support, z_query):\n",
    "        \"\"\"\n",
    "        TODO: do this operation for all pairs at once instead of looping, look at base code\n",
    "        Takes 1 support embedding and 1 query embedding and returns cross-attentioned embeddings\n",
    "        :param z_support: [n_dim]\n",
    "        :param z_query: [n_dim]\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        def correlation_layer(z_support, z_query): \n",
    "            \"\"\"\n",
    "            Takes 1 support embedding and 1 query embedding and returns correlation map\n",
    "            ie. P and Q in the paper. P is [P1, P2, ..., Pn] where n is the dimension of the embeddings, same for Q.\n",
    "            :param z_support: [n_dim] ie. P\n",
    "            :param z_query: [n_dim] ie. Q\n",
    "            :return: correlation_map: [n_dim, n_dim]. Note: we use R^q = correlation_map and R^p = correlation_map.T \n",
    "            \"\"\"\n",
    "\n",
    "            # compute cosine similarity between support and query embeddings\n",
    "            #print(\"z_support_shape\", z_support.shape)\n",
    "            #print(\"z_query_shape\", z_query.shape)\n",
    "\n",
    "            #P = z_support / torch.linalg.norm(z_support, ord=2)\n",
    "            #Q = z_query / torch.linalg.norm(z_query, ord=2)\n",
    "            P = F.normalize(z_support, p=2, dim=-1, eps=1e-12)\n",
    "            Q = F.normalize(z_query, p=2, dim=-1, eps=1e-12)\n",
    "            P = z_support\n",
    "            Q = z_query\n",
    "\n",
    "            #print(\"P shape after norm\", P.shape)\n",
    "            #print(\"Q shape after norm\", Q.shape)\n",
    "            # P is of dim (n_dim) and Q is of dim (ndim)\n",
    "            # we need to change P to (n_dim, 1) and Q to (ndim, 1)\n",
    "            #P = P.reshape(P.shape, 1)\n",
    "            #Q = Q.reshape(Q.shape, 1)\n",
    "            #print(\"P shape after reshape\", P.shape)\n",
    "            #print(\"Q shape after reshape\", Q.shape)\n",
    "            correlation_map = torch.einsum(\"ij,kl->ikjl\",P,Q)  # dim: [n_dim, n_dim]\n",
    "            #print(\"correlation shape\", correlation_map.shape)\n",
    "\n",
    "            return correlation_map\n",
    "\n",
    "        P_k = z_support\n",
    "        Q_b = z_query\n",
    "\n",
    "        # compute correlation map\n",
    "        R_p = correlation_layer(P_k, Q_b)\n",
    "        R_q = R_p.transpose(2, 3)\n",
    "\n",
    "        # compute fusion layer\n",
    "        A_p = self.fusion_layer(R_p)\n",
    "        A_q = self.fusion_layer(R_q)\n",
    "\n",
    "        \"\"\"\n",
    "        A_p = A_p * P_k\n",
    "        P_bk = A_p + P_k\n",
    "\n",
    "        A_q = A_q * Q_b\n",
    "        Q_bk = A_q + Q_b\n",
    "        \"\"\"\n",
    "        P_bk = P_k.unsqueeze(1) * (1 + A_p)\n",
    "        Q_bk = Q_b.unsqueeze(0) * (1 + A_q)\n",
    "\n",
    "        return P_bk, Q_bk\n",
    "\n",
    "    def train_loop(self, epoch, train_loader, optimizer):\n",
    "        print_freq = 10\n",
    "\n",
    "        avg_loss = 0\n",
    "        self.train()\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            y_all = y.cuda()\n",
    "\n",
    "            # y true query is the global labels for the query set\n",
    "            y_true_query = y_all[:, self.n_support:]\n",
    "\n",
    "            self.n_query = x.size(1) - self.n_support\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.set_forward_loss(x, y_true_query)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss = avg_loss + loss.item()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                print('Epoch {:d} | Batch {:d}/{:d} | Loss {:f}'.format(epoch, i, len(train_loader),\n",
    "                                                                        avg_loss / float(i + 1)))\n",
    "        return avg_loss/len(train_loader)\n",
    "def euclidean_dist( x, y):\n",
    "    # x: N x D\n",
    "    # y: M x D\n",
    "    n = x.size(0)\n",
    "    m = y.size(0)\n",
    "    d = x.size(1)\n",
    "    assert d == y.size(1)\n",
    "\n",
    "    x = x.unsqueeze(1).expand(n, m, d)\n",
    "    y = y.unsqueeze(0).expand(n, m, d)\n",
    "\n",
    "    return torch.pow(x - y, 2).sum(2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Special class for few-shot dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.check_init()\n",
    "\n",
    "    def check_init(self):\n",
    "        \"\"\"\n",
    "        Convenience function to check that the FewShotDataset is properly configured.\n",
    "        \"\"\"\n",
    "        required_attrs = ['_dataset_name', '_data_dir']\n",
    "        for attr in required_attrs:\n",
    "            if not hasattr(self, attr):\n",
    "                raise ValueError(f'FewShotDataset must have attribute {attr}.')\n",
    "\n",
    "        if not os.path.exists(self._data_dir):\n",
    "            raise ValueError(\n",
    "                f'{self._data_dir} does not exist yet. Please generate/download the dataset first.')\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def __getitem__(self, i):\n",
    "        return NotImplemented\n",
    "\n",
    "    @abstractmethod\n",
    "    def __len__(self):\n",
    "        return NotImplemented\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def dim(self):\n",
    "        return NotImplemented\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_data_loader(self, mode='train') -> DataLoader:\n",
    "        return NotImplemented\n",
    "\n",
    "    @property\n",
    "    def dataset_name(self):\n",
    "        \"\"\"\n",
    "        A string that identifies the dataset, e.g., 'swissprot'\n",
    "        \"\"\"\n",
    "        return self._dataset_name\n",
    "\n",
    "    @property\n",
    "    def data_dir(self):\n",
    "        return self._data_dir\n",
    "\n",
    "    def initialize_data_dir(self, root_dir):\n",
    "        os.makedirs(root_dir, exist_ok=True)\n",
    "        #self._data_dir = os.path.join(root_dir, self._dataset_name)\n",
    "        self._data_dir = \"data/swissprot\"\n",
    "\n",
    "class SPDataset(FewShotDataset, ABC):\n",
    "    _dataset_name = 'swissprot'\n",
    "\n",
    "    def load_swissprot(self, level = 5, mode='train', min_samples = 20):\n",
    "        samples = get_samples_using_ic(root = self.data_dir)\n",
    "        samples = check_min_samples(samples, min_samples)\n",
    "        unique_ids = set(get_mode_ids(samples)[mode])\n",
    "        return [sample for sample in samples if sample.annot in unique_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROTDIM = 1280\n",
    "\n",
    "class SubDataset(Dataset):\n",
    "    def __init__(self, samples, data_dir):\n",
    "        self.samples = samples\n",
    "        self.encoder = encodings(data_dir)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        sample = self.samples[i]\n",
    "        return sample.input_seq, self.encoder[sample.annot]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return PROTDIM\n",
    "\n",
    "class EpisodicBatchSampler(object):\n",
    "    def __init__(self, n_classes, n_way, n_episodes):\n",
    "        self.n_classes = n_classes\n",
    "        self.n_way = n_way\n",
    "        self.n_episodes = n_episodes\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_episodes\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(self.n_episodes):\n",
    "            yield torch.randperm(self.n_classes)[:self.n_way]\n",
    "\n",
    "class SPSetDataset(SPDataset):\n",
    "    def __init__(self, n_way, n_support, n_query, n_episode=100, root='./data', mode='train'):\n",
    "        self.initialize_data_dir(root)\n",
    "\n",
    "        self.n_way = n_way\n",
    "        self.n_episode = n_episode\n",
    "        min_samples = n_support + n_query\n",
    "        self.encoder = encodings(self.data_dir)\n",
    "\n",
    "        # check if samples_all.pkl exists\n",
    "        if os.path.exists('samples_all.pkl'):\n",
    "            # load samples_all using pickle\n",
    "            with open('samples_all.pkl', 'rb') as f:\n",
    "                samples_all = pickle.load(f)\n",
    "        else:\n",
    "            samples_all = self.load_swissprot(mode = mode, min_samples = min_samples)\n",
    "\n",
    "            # save samples_all using pickle\n",
    "            with open('samples_all.pkl', 'wb') as f:\n",
    "                pickle.dump(samples_all, f)\n",
    "            \n",
    "\n",
    "\n",
    "        self.categories = get_ids(samples_all) # Unique annotations\n",
    "        self.x_dim = PROTDIM\n",
    "\n",
    "        self.sub_dataloader = []\n",
    "\n",
    "        sub_data_loader_params = dict(batch_size=min_samples,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=0,  # use main thread only or may receive multiple batches\n",
    "                                      pin_memory=False)\n",
    "\n",
    "        # Create the sub datasets for each annotation of the categories and collect all the dataloaders in `self.sub_dataloader`.\n",
    "        for annotation in self.categories:\n",
    "            samples = [sample for sample in samples_all if sample.annot == annotation]\n",
    "            sub_dataset = SubDataset(samples, self.data_dir)\n",
    "            self.sub_dataloader.append(torch.utils.data.DataLoader(sub_dataset, **sub_data_loader_params))\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return next(iter(self.sub_dataloader[i]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.categories)\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self.x_dim\n",
    "\n",
    "    def get_data_loader(self) -> DataLoader:\n",
    "        sampler = EpisodicBatchSampler(len(self), self.n_way, self.n_episode)\n",
    "        data_loader_params = dict(batch_sampler=sampler, num_workers=0, pin_memory=True)\n",
    "        \n",
    "        # check if data_loader.pkl exists\n",
    "        if os.path.exists('data_loader.pkl'):\n",
    "            # load data_loader using pickle\n",
    "            with open('data_loader.pkl', 'rb') as f:\n",
    "                data_loader = pickle.load(f)\n",
    "        else:\n",
    "            data_loader = torch.utils.data.DataLoader(self, **data_loader_params)\n",
    "        \n",
    "            # save data_loader using pickle\n",
    "            with open('data_loader.pkl', 'wb') as f:\n",
    "                pickle.dump(data_loader, f)\n",
    "        return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_way, n_support, n_query, n_train_episode):\n",
    "    # Load train dataset. Remember to use make use of functions defined in the `SPSetDataset`.\n",
    "    \n",
    "    if os.path.exists('train_dataset.pkl'):\n",
    "        # load train_dataset using pickle\n",
    "        with open('train_dataset.pkl', 'rb') as f:\n",
    "            train_dataset = pickle.load(f)\n",
    "    else:\n",
    "        train_dataset = SPSetDataset(n_way, n_support, n_query, n_episode=n_train_episode, root='./data', mode='train')\n",
    "        # save as pickle\n",
    "        with open('train_dataset.pkl', 'wb') as f:\n",
    "            pickle.dump(train_dataset, f)\n",
    "    train_loader = train_dataset.get_data_loader()\n",
    "\n",
    "    # Load test dataset. Remember to use make use of functions defined in the `SPSetDataset`.\n",
    "    if os.path.exists('test_dataset.pkl'):\n",
    "        # load test_dataset using pickle\n",
    "        with open('test_dataset.pkl', 'rb') as f:\n",
    "            test_dataset = pickle.load(f)\n",
    "    else:\n",
    "        test_dataset = SPSetDataset(n_way, n_support, n_query, n_episode=100, root='./data', mode='test')\n",
    "        # save as pickle\n",
    "        with open('test_dataset.pkl', 'wb') as f:\n",
    "            pickle.dump(test_dataset, f)\n",
    "    test_loader =  test_dataset.get_data_loader()\n",
    "\n",
    "    # Initialize a fully connected network `FCNet` in `fcnet.py` with two hidden layers of 512 units each as feature extractor.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    backbone = FCNet(train_dataset.dim).to(device)\n",
    "\n",
    "\n",
    "    # Initialize model using the backbone and the optimizer.\n",
    "    model = CanNet(backbone, n_way, n_support).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    test_accs = []; train_losses = []\n",
    "    for epoch in range(1000):\n",
    "        model.train()\n",
    "\n",
    "        # Implement training of the model. Remember to make use of functions defined in the `ProtoNet` and `MetaTemplate` class.\n",
    "        epoch_loss = model.train_loop(epoch, train_loader, optimizer)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # Evaluate test performance for epoch. Remember to make use of functions defined in the `ProtoNet` and `MetaTemplate` class.\n",
    "        test_acc = model.test_loop(epoch, test_loader)\n",
    "        test_accs.append(test_acc)\n",
    "        print(f'Epoch {epoch} | Train Loss {epoch_loss} | Test Acc {test_acc}')\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 2.5))\n",
    "    ax1.plot(range(len(train_losses)), train_losses)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Train Loss')\n",
    "    ax1.grid()\n",
    "\n",
    "    ax2.plot(range(len(test_accs)), test_accs)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Test Accuracy')\n",
    "    ax2.grid()\n",
    "    fig.suptitle(f\"n_way={n_way}, n_support={n_support}, n_query={n_query}, n_train_episode={n_train_episode}\")\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_way': 5, 'n_support': 5, 'n_query': 15, 'n_train_episode': 5}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Batch 0/5 | Loss 7.775368\n",
      "Epoch 0 | Test Acc = 55.47% +- 6.96%\n",
      "Epoch 0 | Train Loss 7.077247142791748 | Test Acc 55.46666666666666\n",
      "Epoch 1 | Batch 0/5 | Loss 7.517126\n",
      "Epoch 1 | Test Acc = 46.67% +- 5.48%\n",
      "Epoch 1 | Train Loss 6.859349060058594 | Test Acc 46.666666666666664\n",
      "Epoch 2 | Batch 0/5 | Loss 5.888073\n",
      "Epoch 2 | Test Acc = 51.47% +- 11.70%\n",
      "Epoch 2 | Train Loss 6.472174549102784 | Test Acc 51.46666666666666\n",
      "Epoch 3 | Batch 0/5 | Loss 7.090170\n",
      "Epoch 3 | Test Acc = 61.33% +- 7.96%\n",
      "Epoch 3 | Train Loss 6.040739631652832 | Test Acc 61.333333333333336\n",
      "Epoch 4 | Batch 0/5 | Loss 5.680331\n",
      "Epoch 4 | Test Acc = 53.33% +- 6.36%\n",
      "Epoch 4 | Train Loss 5.970000743865967 | Test Acc 53.33333333333333\n",
      "Epoch 5 | Batch 0/5 | Loss 6.288441\n",
      "Epoch 5 | Test Acc = 68.80% +- 8.38%\n",
      "Epoch 5 | Train Loss 5.843266105651855 | Test Acc 68.8\n",
      "Epoch 6 | Batch 0/5 | Loss 5.988137\n",
      "Epoch 6 | Test Acc = 58.13% +- 8.41%\n",
      "Epoch 6 | Train Loss 6.034820938110352 | Test Acc 58.133333333333326\n",
      "Epoch 7 | Batch 0/5 | Loss 6.701214\n",
      "Epoch 7 | Test Acc = 52.00% +- 5.07%\n",
      "Epoch 7 | Train Loss 6.387932682037354 | Test Acc 52.0\n",
      "Epoch 8 | Batch 0/5 | Loss 6.035815\n",
      "Epoch 8 | Test Acc = 59.73% +- 2.71%\n",
      "Epoch 8 | Train Loss 5.793489933013916 | Test Acc 59.733333333333334\n",
      "Epoch 9 | Batch 0/5 | Loss 5.199542\n",
      "Epoch 9 | Test Acc = 58.67% +- 4.90%\n",
      "Epoch 9 | Train Loss 5.798392009735108 | Test Acc 58.666666666666664\n",
      "Epoch 10 | Batch 0/5 | Loss 6.705461\n",
      "Epoch 10 | Test Acc = 55.47% +- 13.64%\n",
      "Epoch 10 | Train Loss 6.16519193649292 | Test Acc 55.46666666666666\n",
      "Epoch 11 | Batch 0/5 | Loss 5.595891\n",
      "Epoch 11 | Test Acc = 67.20% +- 9.19%\n",
      "Epoch 11 | Train Loss 5.717748546600342 | Test Acc 67.2\n",
      "Epoch 12 | Batch 0/5 | Loss 6.328405\n",
      "Epoch 12 | Test Acc = 54.93% +- 12.42%\n",
      "Epoch 12 | Train Loss 5.815187644958496 | Test Acc 54.93333333333334\n",
      "Epoch 13 | Batch 0/5 | Loss 5.340666\n",
      "Epoch 13 | Test Acc = 60.00% +- 11.57%\n",
      "Epoch 13 | Train Loss 5.580306911468506 | Test Acc 59.999999999999986\n",
      "Epoch 14 | Batch 0/5 | Loss 5.247659\n",
      "Epoch 14 | Test Acc = 54.40% +- 10.90%\n",
      "Epoch 14 | Train Loss 6.018911552429199 | Test Acc 54.4\n",
      "Epoch 15 | Batch 0/5 | Loss 5.633334\n",
      "Epoch 15 | Test Acc = 57.07% +- 10.09%\n",
      "Epoch 15 | Train Loss 5.749449253082275 | Test Acc 57.06666666666666\n",
      "Epoch 16 | Batch 0/5 | Loss 6.115064\n",
      "Epoch 16 | Test Acc = 63.47% +- 12.25%\n",
      "Epoch 16 | Train Loss 5.4801122665405275 | Test Acc 63.466666666666676\n",
      "Epoch 17 | Batch 0/5 | Loss 5.339301\n",
      "Epoch 17 | Test Acc = 68.00% +- 8.00%\n",
      "Epoch 17 | Train Loss 5.808202075958252 | Test Acc 68.0\n",
      "Epoch 18 | Batch 0/5 | Loss 5.203885\n",
      "Epoch 18 | Test Acc = 58.13% +- 6.21%\n",
      "Epoch 18 | Train Loss 5.367036819458008 | Test Acc 58.133333333333326\n",
      "Epoch 19 | Batch 0/5 | Loss 5.583125\n",
      "Epoch 19 | Test Acc = 67.47% +- 5.36%\n",
      "Epoch 19 | Train Loss 5.640384864807129 | Test Acc 67.46666666666667\n",
      "Epoch 20 | Batch 0/5 | Loss 5.648829\n",
      "Epoch 20 | Test Acc = 62.13% +- 10.15%\n",
      "Epoch 20 | Train Loss 5.841331481933594 | Test Acc 62.13333333333334\n",
      "Epoch 21 | Batch 0/5 | Loss 5.873281\n",
      "Epoch 21 | Test Acc = 67.47% +- 11.93%\n",
      "Epoch 21 | Train Loss 5.816914653778076 | Test Acc 67.46666666666667\n",
      "Epoch 22 | Batch 0/5 | Loss 5.516127\n",
      "Epoch 22 | Test Acc = 60.00% +- 7.57%\n",
      "Epoch 22 | Train Loss 5.509232139587402 | Test Acc 60.0\n",
      "Epoch 23 | Batch 0/5 | Loss 6.275352\n",
      "Epoch 23 | Test Acc = 63.20% +- 6.80%\n",
      "Epoch 23 | Train Loss 5.4076769828796385 | Test Acc 63.2\n",
      "Epoch 24 | Batch 0/5 | Loss 5.086857\n",
      "Epoch 24 | Test Acc = 54.93% +- 12.27%\n",
      "Epoch 24 | Train Loss 5.6087648391723635 | Test Acc 54.93333333333332\n",
      "Epoch 25 | Batch 0/5 | Loss 5.578717\n",
      "Epoch 25 | Test Acc = 59.47% +- 10.80%\n",
      "Epoch 25 | Train Loss 5.605212211608887 | Test Acc 59.466666666666676\n",
      "Epoch 26 | Batch 0/5 | Loss 5.607851\n",
      "Epoch 26 | Test Acc = 58.40% +- 10.20%\n",
      "Epoch 26 | Train Loss 5.62603759765625 | Test Acc 58.4\n",
      "Epoch 27 | Batch 0/5 | Loss 5.469687\n",
      "Epoch 27 | Test Acc = 64.27% +- 9.18%\n",
      "Epoch 27 | Train Loss 5.527887344360352 | Test Acc 64.26666666666668\n",
      "Epoch 28 | Batch 0/5 | Loss 5.283484\n",
      "Epoch 28 | Test Acc = 66.13% +- 6.64%\n",
      "Epoch 28 | Train Loss 5.37230224609375 | Test Acc 66.13333333333334\n",
      "Epoch 29 | Batch 0/5 | Loss 5.350481\n",
      "Epoch 29 | Test Acc = 65.60% +- 12.79%\n",
      "Epoch 29 | Train Loss 5.506292629241943 | Test Acc 65.6\n",
      "Epoch 30 | Batch 0/5 | Loss 5.214957\n",
      "Epoch 30 | Test Acc = 65.60% +- 7.22%\n",
      "Epoch 30 | Train Loss 5.262903308868408 | Test Acc 65.6\n",
      "Epoch 31 | Batch 0/5 | Loss 5.057445\n",
      "Epoch 31 | Test Acc = 65.60% +- 6.71%\n",
      "Epoch 31 | Train Loss 5.43609733581543 | Test Acc 65.60000000000001\n",
      "Epoch 32 | Batch 0/5 | Loss 5.332644\n",
      "Epoch 32 | Test Acc = 59.20% +- 7.26%\n",
      "Epoch 32 | Train Loss 5.125739669799804 | Test Acc 59.2\n",
      "Epoch 33 | Batch 0/5 | Loss 5.530476\n",
      "Epoch 33 | Test Acc = 59.73% +- 10.09%\n",
      "Epoch 33 | Train Loss 5.5000568389892575 | Test Acc 59.733333333333334\n",
      "Epoch 34 | Batch 0/5 | Loss 5.459435\n",
      "Epoch 34 | Test Acc = 52.00% +- 14.16%\n",
      "Epoch 34 | Train Loss 5.539593982696533 | Test Acc 52.0\n",
      "Epoch 35 | Batch 0/5 | Loss 5.338002\n",
      "Epoch 35 | Test Acc = 68.80% +- 13.82%\n",
      "Epoch 35 | Train Loss 5.200153350830078 | Test Acc 68.8\n",
      "Epoch 36 | Batch 0/5 | Loss 5.188905\n",
      "Epoch 36 | Test Acc = 59.47% +- 4.88%\n",
      "Epoch 36 | Train Loss 5.1734692573547365 | Test Acc 59.46666666666666\n",
      "Epoch 37 | Batch 0/5 | Loss 5.677326\n",
      "Epoch 37 | Test Acc = 66.40% +- 9.79%\n",
      "Epoch 37 | Train Loss 5.618331336975098 | Test Acc 66.4\n",
      "Epoch 38 | Batch 0/5 | Loss 5.458901\n",
      "Epoch 38 | Test Acc = 66.40% +- 6.79%\n",
      "Epoch 38 | Train Loss 5.355746555328369 | Test Acc 66.4\n",
      "Epoch 39 | Batch 0/5 | Loss 5.433154\n",
      "Epoch 39 | Test Acc = 60.27% +- 12.36%\n",
      "Epoch 39 | Train Loss 5.419009399414063 | Test Acc 60.26666666666667\n",
      "Epoch 40 | Batch 0/5 | Loss 5.045746\n",
      "Epoch 40 | Test Acc = 70.67% +- 8.84%\n",
      "Epoch 40 | Train Loss 5.279097270965576 | Test Acc 70.66666666666666\n",
      "Epoch 41 | Batch 0/5 | Loss 5.205945\n",
      "Epoch 41 | Test Acc = 68.53% +- 4.59%\n",
      "Epoch 41 | Train Loss 5.2655029296875 | Test Acc 68.53333333333333\n",
      "Epoch 42 | Batch 0/5 | Loss 5.190736\n",
      "Epoch 42 | Test Acc = 69.07% +- 7.77%\n",
      "Epoch 42 | Train Loss 5.1095959663391115 | Test Acc 69.06666666666666\n",
      "Epoch 43 | Batch 0/5 | Loss 5.428113\n",
      "Epoch 43 | Test Acc = 64.27% +- 14.74%\n",
      "Epoch 43 | Train Loss 5.370479965209961 | Test Acc 64.26666666666667\n",
      "Epoch 44 | Batch 0/5 | Loss 5.128795\n",
      "Epoch 44 | Test Acc = 69.07% +- 8.63%\n",
      "Epoch 44 | Train Loss 5.217144584655761 | Test Acc 69.06666666666666\n",
      "Epoch 45 | Batch 0/5 | Loss 5.235903\n",
      "Epoch 45 | Test Acc = 76.00% +- 8.59%\n",
      "Epoch 45 | Train Loss 5.2037077903747555 | Test Acc 76.00000000000001\n",
      "Epoch 46 | Batch 0/5 | Loss 5.039352\n",
      "Epoch 46 | Test Acc = 67.73% +- 9.42%\n",
      "Epoch 46 | Train Loss 4.98935718536377 | Test Acc 67.73333333333332\n",
      "Epoch 47 | Batch 0/5 | Loss 5.331020\n",
      "Epoch 47 | Test Acc = 65.33% +- 5.28%\n",
      "Epoch 47 | Train Loss 5.362049579620361 | Test Acc 65.33333333333334\n",
      "Epoch 48 | Batch 0/5 | Loss 5.023836\n",
      "Epoch 48 | Test Acc = 70.40% +- 6.02%\n",
      "Epoch 48 | Train Loss 5.1293456077575685 | Test Acc 70.4\n",
      "Epoch 49 | Batch 0/5 | Loss 4.733817\n",
      "Epoch 49 | Test Acc = 60.80% +- 4.28%\n",
      "Epoch 49 | Train Loss 5.105904960632325 | Test Acc 60.79999999999999\n",
      "Epoch 50 | Batch 0/5 | Loss 5.149532\n",
      "Epoch 50 | Test Acc = 68.53% +- 8.22%\n",
      "Epoch 50 | Train Loss 5.223512363433838 | Test Acc 68.53333333333333\n",
      "Epoch 51 | Batch 0/5 | Loss 4.766670\n",
      "Epoch 51 | Test Acc = 81.07% +- 6.33%\n",
      "Epoch 51 | Train Loss 5.089518928527832 | Test Acc 81.06666666666668\n",
      "Epoch 52 | Batch 0/5 | Loss 4.749581\n",
      "Epoch 52 | Test Acc = 69.07% +- 6.16%\n",
      "Epoch 52 | Train Loss 5.085194301605225 | Test Acc 69.06666666666666\n",
      "Epoch 53 | Batch 0/5 | Loss 5.565775\n",
      "Epoch 53 | Test Acc = 65.07% +- 2.27%\n",
      "Epoch 53 | Train Loss 4.9775796890258786 | Test Acc 65.06666666666668\n",
      "Epoch 54 | Batch 0/5 | Loss 5.010477\n",
      "Epoch 54 | Test Acc = 68.00% +- 13.45%\n",
      "Epoch 54 | Train Loss 5.136876106262207 | Test Acc 68.0\n",
      "Epoch 55 | Batch 0/5 | Loss 4.914644\n",
      "Epoch 55 | Test Acc = 69.87% +- 6.03%\n",
      "Epoch 55 | Train Loss 5.066090202331543 | Test Acc 69.86666666666665\n",
      "Epoch 56 | Batch 0/5 | Loss 5.003057\n",
      "Epoch 56 | Test Acc = 68.27% +- 4.08%\n",
      "Epoch 56 | Train Loss 5.147234725952148 | Test Acc 68.26666666666668\n",
      "Epoch 57 | Batch 0/5 | Loss 4.906207\n",
      "Epoch 57 | Test Acc = 68.00% +- 9.29%\n",
      "Epoch 57 | Train Loss 5.240596199035645 | Test Acc 68.0\n",
      "Epoch 58 | Batch 0/5 | Loss 5.086254\n",
      "Epoch 58 | Test Acc = 68.80% +- 7.19%\n",
      "Epoch 58 | Train Loss 4.942606258392334 | Test Acc 68.80000000000001\n",
      "Epoch 59 | Batch 0/5 | Loss 4.520973\n",
      "Epoch 59 | Test Acc = 71.20% +- 8.35%\n",
      "Epoch 59 | Train Loss 4.751017379760742 | Test Acc 71.19999999999999\n",
      "Epoch 60 | Batch 0/5 | Loss 4.577039\n",
      "Epoch 60 | Test Acc = 65.33% +- 12.01%\n",
      "Epoch 60 | Train Loss 4.805962753295899 | Test Acc 65.33333333333334\n",
      "Epoch 61 | Batch 0/5 | Loss 4.978005\n",
      "Epoch 61 | Test Acc = 69.60% +- 8.63%\n",
      "Epoch 61 | Train Loss 4.989606666564941 | Test Acc 69.6\n",
      "Epoch 62 | Batch 0/5 | Loss 5.005191\n",
      "Epoch 62 | Test Acc = 58.93% +- 7.29%\n",
      "Epoch 62 | Train Loss 4.851620483398437 | Test Acc 58.93333333333332\n",
      "Epoch 63 | Batch 0/5 | Loss 4.737906\n",
      "Epoch 63 | Test Acc = 75.47% +- 4.53%\n",
      "Epoch 63 | Train Loss 4.73101224899292 | Test Acc 75.46666666666667\n",
      "Epoch 64 | Batch 0/5 | Loss 4.825920\n",
      "Epoch 64 | Test Acc = 73.33% +- 13.06%\n",
      "Epoch 64 | Train Loss 4.9114048957824705 | Test Acc 73.33333333333333\n",
      "Epoch 65 | Batch 0/5 | Loss 4.707118\n",
      "Epoch 65 | Test Acc = 63.20% +- 9.19%\n",
      "Epoch 65 | Train Loss 4.7848801612854 | Test Acc 63.2\n",
      "Epoch 66 | Batch 0/5 | Loss 5.245863\n",
      "Epoch 66 | Test Acc = 66.67% +- 6.14%\n",
      "Epoch 66 | Train Loss 4.887761116027832 | Test Acc 66.66666666666666\n",
      "Epoch 67 | Batch 0/5 | Loss 5.273410\n",
      "Epoch 67 | Test Acc = 71.73% +- 5.20%\n",
      "Epoch 67 | Train Loss 4.9709371566772464 | Test Acc 71.73333333333333\n",
      "Epoch 68 | Batch 0/5 | Loss 4.674839\n",
      "Epoch 68 | Test Acc = 65.60% +- 7.55%\n",
      "Epoch 68 | Train Loss 4.848374748229981 | Test Acc 65.6\n",
      "Epoch 69 | Batch 0/5 | Loss 4.981337\n",
      "Epoch 69 | Test Acc = 62.13% +- 9.01%\n",
      "Epoch 69 | Train Loss 4.850145626068115 | Test Acc 62.133333333333326\n",
      "Epoch 70 | Batch 0/5 | Loss 4.562765\n",
      "Epoch 70 | Test Acc = 72.80% +- 6.17%\n",
      "Epoch 70 | Train Loss 4.955349349975586 | Test Acc 72.80000000000001\n",
      "Epoch 71 | Batch 0/5 | Loss 4.456597\n",
      "Epoch 71 | Test Acc = 69.07% +- 6.16%\n",
      "Epoch 71 | Train Loss 4.85963077545166 | Test Acc 69.06666666666666\n",
      "Epoch 72 | Batch 0/5 | Loss 5.064314\n",
      "Epoch 72 | Test Acc = 69.87% +- 13.84%\n",
      "Epoch 72 | Train Loss 4.937505149841309 | Test Acc 69.86666666666666\n",
      "Epoch 73 | Batch 0/5 | Loss 4.962391\n",
      "Epoch 73 | Test Acc = 69.07% +- 7.22%\n",
      "Epoch 73 | Train Loss 4.806230068206787 | Test Acc 69.06666666666666\n",
      "Epoch 74 | Batch 0/5 | Loss 5.304030\n",
      "Epoch 74 | Test Acc = 71.73% +- 6.02%\n",
      "Epoch 74 | Train Loss 4.773518753051758 | Test Acc 71.73333333333332\n",
      "Epoch 75 | Batch 0/5 | Loss 4.801096\n",
      "Epoch 75 | Test Acc = 66.93% +- 5.89%\n",
      "Epoch 75 | Train Loss 4.796536064147949 | Test Acc 66.93333333333332\n",
      "Epoch 76 | Batch 0/5 | Loss 4.789908\n",
      "Epoch 76 | Test Acc = 72.27% +- 6.50%\n",
      "Epoch 76 | Train Loss 4.755321788787842 | Test Acc 72.26666666666665\n",
      "Epoch 77 | Batch 0/5 | Loss 4.816892\n",
      "Epoch 77 | Test Acc = 62.93% +- 10.15%\n",
      "Epoch 77 | Train Loss 4.748824977874756 | Test Acc 62.93333333333334\n",
      "Epoch 78 | Batch 0/5 | Loss 4.857782\n",
      "Epoch 78 | Test Acc = 72.27% +- 11.20%\n",
      "Epoch 78 | Train Loss 4.967516613006592 | Test Acc 72.26666666666667\n",
      "Epoch 79 | Batch 0/5 | Loss 5.071047\n",
      "Epoch 79 | Test Acc = 66.93% +- 11.88%\n",
      "Epoch 79 | Train Loss 4.932013702392578 | Test Acc 66.93333333333332\n",
      "Epoch 80 | Batch 0/5 | Loss 4.857103\n",
      "Epoch 80 | Test Acc = 64.00% +- 9.89%\n",
      "Epoch 80 | Train Loss 4.906302452087402 | Test Acc 64.0\n",
      "Epoch 81 | Batch 0/5 | Loss 4.325483\n",
      "Epoch 81 | Test Acc = 68.00% +- 8.13%\n",
      "Epoch 81 | Train Loss 4.478721618652344 | Test Acc 68.0\n",
      "Epoch 82 | Batch 0/5 | Loss 4.875324\n",
      "Epoch 82 | Test Acc = 82.13% +- 5.85%\n",
      "Epoch 82 | Train Loss 4.722114849090576 | Test Acc 82.13333333333333\n",
      "Epoch 83 | Batch 0/5 | Loss 4.738850\n",
      "Epoch 83 | Test Acc = 68.00% +- 2.96%\n",
      "Epoch 83 | Train Loss 4.909039688110352 | Test Acc 68.0\n",
      "Epoch 84 | Batch 0/5 | Loss 5.059611\n",
      "Epoch 84 | Test Acc = 72.27% +- 9.93%\n",
      "Epoch 84 | Train Loss 4.777956104278564 | Test Acc 72.26666666666668\n",
      "Epoch 85 | Batch 0/5 | Loss 5.045771\n",
      "Epoch 85 | Test Acc = 70.93% +- 9.82%\n",
      "Epoch 85 | Train Loss 4.87791223526001 | Test Acc 70.93333333333334\n",
      "Epoch 86 | Batch 0/5 | Loss 4.622555\n",
      "Epoch 86 | Test Acc = 73.33% +- 8.84%\n",
      "Epoch 86 | Train Loss 4.686349296569825 | Test Acc 73.33333333333333\n",
      "Epoch 87 | Batch 0/5 | Loss 4.350383\n",
      "Epoch 87 | Test Acc = 79.20% +- 4.88%\n",
      "Epoch 87 | Train Loss 4.600679492950439 | Test Acc 79.2\n",
      "Epoch 88 | Batch 0/5 | Loss 4.545613\n",
      "Epoch 88 | Test Acc = 67.47% +- 5.56%\n",
      "Epoch 88 | Train Loss 4.711347007751465 | Test Acc 67.46666666666667\n",
      "Epoch 89 | Batch 0/5 | Loss 4.784354\n",
      "Epoch 89 | Test Acc = 71.20% +- 4.71%\n",
      "Epoch 89 | Train Loss 4.787939643859863 | Test Acc 71.2\n",
      "Epoch 90 | Batch 0/5 | Loss 4.638945\n",
      "Epoch 90 | Test Acc = 76.27% +- 5.50%\n",
      "Epoch 90 | Train Loss 4.561303615570068 | Test Acc 76.26666666666665\n",
      "Epoch 91 | Batch 0/5 | Loss 4.921260\n",
      "Epoch 91 | Test Acc = 62.40% +- 9.62%\n",
      "Epoch 91 | Train Loss 4.730975914001465 | Test Acc 62.4\n",
      "Epoch 92 | Batch 0/5 | Loss 4.602211\n",
      "Epoch 92 | Test Acc = 68.53% +- 10.36%\n",
      "Epoch 92 | Train Loss 4.740740394592285 | Test Acc 68.53333333333333\n",
      "Epoch 93 | Batch 0/5 | Loss 4.618755\n",
      "Epoch 93 | Test Acc = 64.80% +- 4.59%\n",
      "Epoch 93 | Train Loss 4.637221145629883 | Test Acc 64.79999999999998\n",
      "Epoch 94 | Batch 0/5 | Loss 4.792818\n",
      "Epoch 94 | Test Acc = 72.00% +- 8.46%\n",
      "Epoch 94 | Train Loss 4.511132526397705 | Test Acc 72.0\n",
      "Epoch 95 | Batch 0/5 | Loss 4.700686\n",
      "Epoch 95 | Test Acc = 77.87% +- 7.88%\n",
      "Epoch 95 | Train Loss 4.673195171356201 | Test Acc 77.86666666666666\n",
      "Epoch 96 | Batch 0/5 | Loss 4.502542\n",
      "Epoch 96 | Test Acc = 74.93% +- 11.48%\n",
      "Epoch 96 | Train Loss 4.752546501159668 | Test Acc 74.93333333333332\n",
      "Epoch 97 | Batch 0/5 | Loss 4.585090\n",
      "Epoch 97 | Test Acc = 77.07% +- 8.57%\n",
      "Epoch 97 | Train Loss 4.555125713348389 | Test Acc 77.06666666666668\n",
      "Epoch 98 | Batch 0/5 | Loss 4.627100\n",
      "Epoch 98 | Test Acc = 79.20% +- 2.82%\n",
      "Epoch 98 | Train Loss 4.51874942779541 | Test Acc 79.2\n",
      "Epoch 99 | Batch 0/5 | Loss 4.557236\n",
      "Epoch 99 | Test Acc = 78.13% +- 9.25%\n",
      "Epoch 99 | Train Loss 4.621438598632812 | Test Acc 78.13333333333334\n",
      "Epoch 100 | Batch 0/5 | Loss 4.587934\n",
      "Epoch 100 | Test Acc = 82.13% +- 8.02%\n",
      "Epoch 100 | Train Loss 4.638335037231445 | Test Acc 82.13333333333333\n",
      "Epoch 101 | Batch 0/5 | Loss 4.333709\n",
      "Epoch 101 | Test Acc = 71.20% +- 10.95%\n",
      "Epoch 101 | Train Loss 4.547823715209961 | Test Acc 71.2\n",
      "Epoch 102 | Batch 0/5 | Loss 4.613153\n",
      "Epoch 102 | Test Acc = 80.53% +- 14.08%\n",
      "Epoch 102 | Train Loss 4.684305953979492 | Test Acc 80.53333333333333\n",
      "Epoch 103 | Batch 0/5 | Loss 4.656389\n",
      "Epoch 103 | Test Acc = 75.73% +- 9.68%\n",
      "Epoch 103 | Train Loss 4.543871784210205 | Test Acc 75.73333333333333\n",
      "Epoch 104 | Batch 0/5 | Loss 4.740807\n",
      "Epoch 104 | Test Acc = 80.80% +- 10.83%\n",
      "Epoch 104 | Train Loss 4.600208950042725 | Test Acc 80.8\n",
      "Epoch 105 | Batch 0/5 | Loss 4.724480\n",
      "Epoch 105 | Test Acc = 70.40% +- 12.02%\n",
      "Epoch 105 | Train Loss 4.538197994232178 | Test Acc 70.4\n",
      "Epoch 106 | Batch 0/5 | Loss 4.687895\n",
      "Epoch 106 | Test Acc = 74.93% +- 4.52%\n",
      "Epoch 106 | Train Loss 4.6371338844299315 | Test Acc 74.93333333333332\n",
      "Epoch 107 | Batch 0/5 | Loss 4.608832\n",
      "Epoch 107 | Test Acc = 70.13% +- 9.65%\n",
      "Epoch 107 | Train Loss 4.65012378692627 | Test Acc 70.13333333333333\n",
      "Epoch 108 | Batch 0/5 | Loss 4.775372\n",
      "Epoch 108 | Test Acc = 76.80% +- 7.00%\n",
      "Epoch 108 | Train Loss 4.690192127227784 | Test Acc 76.8\n",
      "Epoch 109 | Batch 0/5 | Loss 4.618933\n",
      "Epoch 109 | Test Acc = 74.13% +- 10.42%\n",
      "Epoch 109 | Train Loss 4.47606258392334 | Test Acc 74.13333333333334\n",
      "Epoch 110 | Batch 0/5 | Loss 4.655839\n",
      "Epoch 110 | Test Acc = 62.40% +- 6.95%\n",
      "Epoch 110 | Train Loss 4.723605537414551 | Test Acc 62.4\n",
      "Epoch 111 | Batch 0/5 | Loss 4.124154\n",
      "Epoch 111 | Test Acc = 79.73% +- 7.37%\n",
      "Epoch 111 | Train Loss 4.493339347839355 | Test Acc 79.73333333333333\n",
      "Epoch 112 | Batch 0/5 | Loss 4.343828\n",
      "Epoch 112 | Test Acc = 59.47% +- 13.42%\n",
      "Epoch 112 | Train Loss 4.573505401611328 | Test Acc 59.466666666666676\n",
      "Epoch 113 | Batch 0/5 | Loss 4.499325\n",
      "Epoch 113 | Test Acc = 77.60% +- 4.64%\n",
      "Epoch 113 | Train Loss 4.6549224853515625 | Test Acc 77.6\n",
      "Epoch 114 | Batch 0/5 | Loss 4.433257\n",
      "Epoch 114 | Test Acc = 81.60% +- 8.41%\n",
      "Epoch 114 | Train Loss 4.527699756622314 | Test Acc 81.6\n",
      "Epoch 115 | Batch 0/5 | Loss 4.479419\n",
      "Epoch 115 | Test Acc = 80.80% +- 4.99%\n",
      "Epoch 115 | Train Loss 4.574980735778809 | Test Acc 80.8\n",
      "Epoch 116 | Batch 0/5 | Loss 4.558428\n",
      "Epoch 116 | Test Acc = 69.33% +- 6.97%\n",
      "Epoch 116 | Train Loss 4.575949668884277 | Test Acc 69.33333333333333\n",
      "Epoch 117 | Batch 0/5 | Loss 4.169397\n",
      "Epoch 117 | Test Acc = 74.93% +- 9.36%\n",
      "Epoch 117 | Train Loss 4.624779510498047 | Test Acc 74.93333333333334\n",
      "Epoch 118 | Batch 0/5 | Loss 4.725270\n",
      "Epoch 118 | Test Acc = 79.73% +- 9.68%\n",
      "Epoch 118 | Train Loss 4.689209365844727 | Test Acc 79.73333333333332\n",
      "Epoch 119 | Batch 0/5 | Loss 4.419805\n",
      "Epoch 119 | Test Acc = 76.53% +- 8.92%\n",
      "Epoch 119 | Train Loss 4.581970691680908 | Test Acc 76.53333333333333\n",
      "Epoch 120 | Batch 0/5 | Loss 4.726923\n",
      "Epoch 120 | Test Acc = 82.40% +- 7.87%\n",
      "Epoch 120 | Train Loss 4.574722480773926 | Test Acc 82.4\n",
      "Epoch 121 | Batch 0/5 | Loss 4.596035\n",
      "Epoch 121 | Test Acc = 71.47% +- 12.60%\n",
      "Epoch 121 | Train Loss 4.584100151062012 | Test Acc 71.46666666666667\n",
      "Epoch 122 | Batch 0/5 | Loss 4.525696\n",
      "Epoch 122 | Test Acc = 73.33% +- 9.41%\n",
      "Epoch 122 | Train Loss 4.576537704467773 | Test Acc 73.33333333333334\n",
      "Epoch 123 | Batch 0/5 | Loss 4.333656\n",
      "Epoch 123 | Test Acc = 73.60% +- 7.59%\n",
      "Epoch 123 | Train Loss 4.476350212097168 | Test Acc 73.6\n",
      "Epoch 124 | Batch 0/5 | Loss 4.462814\n",
      "Epoch 124 | Test Acc = 82.40% +- 4.52%\n",
      "Epoch 124 | Train Loss 4.550443935394287 | Test Acc 82.4\n",
      "Epoch 125 | Batch 0/5 | Loss 4.401595\n",
      "Epoch 125 | Test Acc = 77.87% +- 7.77%\n",
      "Epoch 125 | Train Loss 4.43479356765747 | Test Acc 77.86666666666667\n",
      "Epoch 126 | Batch 0/5 | Loss 4.311444\n",
      "Epoch 126 | Test Acc = 81.87% +- 7.23%\n",
      "Epoch 126 | Train Loss 4.493863201141357 | Test Acc 81.86666666666666\n",
      "Epoch 127 | Batch 0/5 | Loss 4.594890\n",
      "Epoch 127 | Test Acc = 74.67% +- 6.93%\n",
      "Epoch 127 | Train Loss 4.4901374816894535 | Test Acc 74.66666666666666\n",
      "Epoch 128 | Batch 0/5 | Loss 4.449761\n",
      "Epoch 128 | Test Acc = 65.60% +- 5.09%\n",
      "Epoch 128 | Train Loss 4.5567632675170895 | Test Acc 65.6\n",
      "Epoch 129 | Batch 0/5 | Loss 4.234387\n",
      "Epoch 129 | Test Acc = 75.47% +- 9.71%\n",
      "Epoch 129 | Train Loss 4.3933250427246096 | Test Acc 75.46666666666667\n",
      "Epoch 130 | Batch 0/5 | Loss 4.404388\n",
      "Epoch 130 | Test Acc = 76.00% +- 8.81%\n",
      "Epoch 130 | Train Loss 4.590434932708741 | Test Acc 76.0\n",
      "Epoch 131 | Batch 0/5 | Loss 4.510137\n",
      "Epoch 131 | Test Acc = 72.53% +- 9.99%\n",
      "Epoch 131 | Train Loss 4.376480770111084 | Test Acc 72.53333333333333\n",
      "Epoch 132 | Batch 0/5 | Loss 4.407290\n",
      "Epoch 132 | Test Acc = 79.20% +- 6.51%\n",
      "Epoch 132 | Train Loss 4.407880401611328 | Test Acc 79.2\n",
      "Epoch 133 | Batch 0/5 | Loss 4.397762\n",
      "Epoch 133 | Test Acc = 80.80% +- 9.19%\n",
      "Epoch 133 | Train Loss 4.46513090133667 | Test Acc 80.8\n",
      "Epoch 134 | Batch 0/5 | Loss 4.315811\n",
      "Epoch 134 | Test Acc = 71.47% +- 8.05%\n",
      "Epoch 134 | Train Loss 4.467819404602051 | Test Acc 71.46666666666667\n",
      "Epoch 135 | Batch 0/5 | Loss 4.602719\n",
      "Epoch 135 | Test Acc = 78.93% +- 6.67%\n",
      "Epoch 135 | Train Loss 4.439598274230957 | Test Acc 78.93333333333332\n",
      "Epoch 136 | Batch 0/5 | Loss 4.080474\n",
      "Epoch 136 | Test Acc = 76.80% +- 7.98%\n",
      "Epoch 136 | Train Loss 4.469802761077881 | Test Acc 76.8\n",
      "Epoch 137 | Batch 0/5 | Loss 4.669820\n",
      "Epoch 137 | Test Acc = 75.20% +- 10.18%\n",
      "Epoch 137 | Train Loss 4.513567066192627 | Test Acc 75.2\n",
      "Epoch 138 | Batch 0/5 | Loss 4.596162\n",
      "Epoch 138 | Test Acc = 75.47% +- 5.99%\n",
      "Epoch 138 | Train Loss 4.351454257965088 | Test Acc 75.46666666666667\n",
      "Epoch 139 | Batch 0/5 | Loss 4.192037\n",
      "Epoch 139 | Test Acc = 79.73% +- 2.99%\n",
      "Epoch 139 | Train Loss 4.340833950042724 | Test Acc 79.73333333333335\n",
      "Epoch 140 | Batch 0/5 | Loss 4.390770\n",
      "Epoch 140 | Test Acc = 71.73% +- 7.80%\n",
      "Epoch 140 | Train Loss 4.44777660369873 | Test Acc 71.73333333333332\n",
      "Epoch 141 | Batch 0/5 | Loss 4.430191\n",
      "Epoch 141 | Test Acc = 78.13% +- 5.71%\n",
      "Epoch 141 | Train Loss 4.511759853363037 | Test Acc 78.13333333333334\n",
      "Epoch 142 | Batch 0/5 | Loss 4.342304\n",
      "Epoch 142 | Test Acc = 68.53% +- 11.10%\n",
      "Epoch 142 | Train Loss 4.465359115600586 | Test Acc 68.53333333333333\n",
      "Epoch 143 | Batch 0/5 | Loss 4.504412\n",
      "Epoch 143 | Test Acc = 73.87% +- 7.84%\n",
      "Epoch 143 | Train Loss 4.465264511108399 | Test Acc 73.86666666666667\n",
      "Epoch 144 | Batch 0/5 | Loss 4.300797\n",
      "Epoch 144 | Test Acc = 79.20% +- 6.25%\n",
      "Epoch 144 | Train Loss 4.352376079559326 | Test Acc 79.2\n",
      "Epoch 145 | Batch 0/5 | Loss 4.773657\n",
      "Epoch 145 | Test Acc = 79.20% +- 7.04%\n",
      "Epoch 145 | Train Loss 4.493795776367188 | Test Acc 79.2\n",
      "Epoch 146 | Batch 0/5 | Loss 4.309808\n",
      "Epoch 146 | Test Acc = 80.27% +- 9.36%\n",
      "Epoch 146 | Train Loss 4.298211002349854 | Test Acc 80.26666666666668\n",
      "Epoch 147 | Batch 0/5 | Loss 4.156098\n",
      "Epoch 147 | Test Acc = 74.40% +- 9.62%\n",
      "Epoch 147 | Train Loss 4.346942329406739 | Test Acc 74.4\n",
      "Epoch 148 | Batch 0/5 | Loss 4.193106\n",
      "Epoch 148 | Test Acc = 84.27% +- 7.52%\n",
      "Epoch 148 | Train Loss 4.406896877288818 | Test Acc 84.26666666666668\n",
      "Epoch 149 | Batch 0/5 | Loss 4.444738\n",
      "Epoch 149 | Test Acc = 64.80% +- 6.96%\n",
      "Epoch 149 | Train Loss 4.448845672607422 | Test Acc 64.8\n",
      "Epoch 150 | Batch 0/5 | Loss 4.412195\n",
      "Epoch 150 | Test Acc = 77.60% +- 7.48%\n",
      "Epoch 150 | Train Loss 4.365737819671631 | Test Acc 77.6\n",
      "Epoch 151 | Batch 0/5 | Loss 3.868573\n",
      "Epoch 151 | Test Acc = 77.33% +- 5.96%\n",
      "Epoch 151 | Train Loss 4.281779527664185 | Test Acc 77.33333333333334\n",
      "Epoch 152 | Batch 0/5 | Loss 4.409458\n",
      "Epoch 152 | Test Acc = 71.47% +- 9.71%\n",
      "Epoch 152 | Train Loss 4.47909984588623 | Test Acc 71.46666666666667\n",
      "Epoch 153 | Batch 0/5 | Loss 4.342136\n",
      "Epoch 153 | Test Acc = 84.53% +- 5.15%\n",
      "Epoch 153 | Train Loss 4.29708833694458 | Test Acc 84.53333333333333\n",
      "Epoch 154 | Batch 0/5 | Loss 4.459833\n",
      "Epoch 154 | Test Acc = 82.93% +- 10.33%\n",
      "Epoch 154 | Train Loss 4.3951029777526855 | Test Acc 82.93333333333332\n",
      "Epoch 155 | Batch 0/5 | Loss 4.431174\n",
      "Epoch 155 | Test Acc = 76.00% +- 8.52%\n",
      "Epoch 155 | Train Loss 4.3403801918029785 | Test Acc 76.0\n",
      "Epoch 156 | Batch 0/5 | Loss 4.426009\n",
      "Epoch 156 | Test Acc = 76.53% +- 9.25%\n",
      "Epoch 156 | Train Loss 4.380403995513916 | Test Acc 76.53333333333333\n",
      "Epoch 157 | Batch 0/5 | Loss 4.244594\n",
      "Epoch 157 | Test Acc = 77.07% +- 8.08%\n",
      "Epoch 157 | Train Loss 4.4341785430908205 | Test Acc 77.06666666666666\n",
      "Epoch 158 | Batch 0/5 | Loss 4.535785\n",
      "Epoch 158 | Test Acc = 73.87% +- 9.71%\n",
      "Epoch 158 | Train Loss 4.425499629974365 | Test Acc 73.86666666666666\n",
      "Epoch 159 | Batch 0/5 | Loss 4.308067\n",
      "Epoch 159 | Test Acc = 77.07% +- 9.76%\n",
      "Epoch 159 | Train Loss 4.373135185241699 | Test Acc 77.06666666666668\n",
      "Epoch 160 | Batch 0/5 | Loss 4.517683\n",
      "Epoch 160 | Test Acc = 77.60% +- 8.73%\n",
      "Epoch 160 | Train Loss 4.319027233123779 | Test Acc 77.6\n",
      "Epoch 161 | Batch 0/5 | Loss 4.496724\n",
      "Epoch 161 | Test Acc = 83.47% +- 5.66%\n",
      "Epoch 161 | Train Loss 4.407752990722656 | Test Acc 83.46666666666667\n",
      "Epoch 162 | Batch 0/5 | Loss 4.472582\n",
      "Epoch 162 | Test Acc = 85.87% +- 4.16%\n",
      "Epoch 162 | Train Loss 4.341580009460449 | Test Acc 85.86666666666667\n",
      "Epoch 163 | Batch 0/5 | Loss 4.134640\n",
      "Epoch 163 | Test Acc = 81.07% +- 7.29%\n",
      "Epoch 163 | Train Loss 4.203234577178955 | Test Acc 81.06666666666666\n",
      "Epoch 164 | Batch 0/5 | Loss 4.395541\n",
      "Epoch 164 | Test Acc = 86.40% +- 6.16%\n",
      "Epoch 164 | Train Loss 4.364874362945557 | Test Acc 86.4\n",
      "Epoch 165 | Batch 0/5 | Loss 4.338458\n",
      "Epoch 165 | Test Acc = 83.73% +- 4.52%\n",
      "Epoch 165 | Train Loss 4.319052791595459 | Test Acc 83.73333333333332\n",
      "Epoch 166 | Batch 0/5 | Loss 4.529869\n",
      "Epoch 166 | Test Acc = 71.47% +- 10.50%\n",
      "Epoch 166 | Train Loss 4.286008834838867 | Test Acc 71.46666666666667\n",
      "Epoch 167 | Batch 0/5 | Loss 4.190489\n",
      "Epoch 167 | Test Acc = 83.73% +- 2.50%\n",
      "Epoch 167 | Train Loss 4.243630981445312 | Test Acc 83.73333333333332\n",
      "Epoch 168 | Batch 0/5 | Loss 4.434878\n",
      "Epoch 168 | Test Acc = 77.33% +- 14.16%\n",
      "Epoch 168 | Train Loss 4.463999080657959 | Test Acc 77.33333333333334\n",
      "Epoch 169 | Batch 0/5 | Loss 4.220940\n",
      "Epoch 169 | Test Acc = 75.47% +- 10.68%\n",
      "Epoch 169 | Train Loss 4.376089000701905 | Test Acc 75.46666666666667\n",
      "Epoch 170 | Batch 0/5 | Loss 4.256505\n",
      "Epoch 170 | Test Acc = 76.00% +- 7.65%\n",
      "Epoch 170 | Train Loss 4.35863676071167 | Test Acc 76.0\n",
      "Epoch 171 | Batch 0/5 | Loss 4.016704\n",
      "Epoch 171 | Test Acc = 81.60% +- 6.29%\n",
      "Epoch 171 | Train Loss 4.301442241668701 | Test Acc 81.6\n",
      "Epoch 172 | Batch 0/5 | Loss 4.385968\n",
      "Epoch 172 | Test Acc = 84.00% +- 8.03%\n",
      "Epoch 172 | Train Loss 4.332269620895386 | Test Acc 84.0\n",
      "Epoch 173 | Batch 0/5 | Loss 4.094602\n",
      "Epoch 173 | Test Acc = 84.27% +- 3.87%\n",
      "Epoch 173 | Train Loss 4.1456990242004395 | Test Acc 84.26666666666667\n",
      "Epoch 174 | Batch 0/5 | Loss 4.545124\n",
      "Epoch 174 | Test Acc = 75.73% +- 8.14%\n",
      "Epoch 174 | Train Loss 4.344497394561768 | Test Acc 75.73333333333333\n",
      "Epoch 175 | Batch 0/5 | Loss 4.289046\n",
      "Epoch 175 | Test Acc = 72.27% +- 5.50%\n",
      "Epoch 175 | Train Loss 4.270481967926026 | Test Acc 72.26666666666667\n",
      "Epoch 176 | Batch 0/5 | Loss 4.193680\n",
      "Epoch 176 | Test Acc = 79.20% +- 8.76%\n",
      "Epoch 176 | Train Loss 4.148701810836792 | Test Acc 79.2\n",
      "Epoch 177 | Batch 0/5 | Loss 4.451136\n",
      "Epoch 177 | Test Acc = 79.47% +- 12.07%\n",
      "Epoch 177 | Train Loss 4.253048038482666 | Test Acc 79.46666666666665\n",
      "Epoch 178 | Batch 0/5 | Loss 4.239477\n",
      "Epoch 178 | Test Acc = 86.67% +- 5.87%\n",
      "Epoch 178 | Train Loss 4.247849464416504 | Test Acc 86.66666666666667\n",
      "Epoch 179 | Batch 0/5 | Loss 4.030272\n",
      "Epoch 179 | Test Acc = 76.27% +- 12.77%\n",
      "Epoch 179 | Train Loss 4.128894233703614 | Test Acc 76.26666666666667\n",
      "Epoch 180 | Batch 0/5 | Loss 4.192161\n",
      "Epoch 180 | Test Acc = 86.93% +- 4.52%\n",
      "Epoch 180 | Train Loss 4.25609827041626 | Test Acc 86.93333333333332\n",
      "Epoch 181 | Batch 0/5 | Loss 4.275209\n",
      "Epoch 181 | Test Acc = 73.60% +- 13.15%\n",
      "Epoch 181 | Train Loss 4.1659660816192625 | Test Acc 73.6\n",
      "Epoch 182 | Batch 0/5 | Loss 4.300734\n",
      "Epoch 182 | Test Acc = 85.33% +- 4.12%\n",
      "Epoch 182 | Train Loss 4.175161933898925 | Test Acc 85.33333333333333\n",
      "Epoch 183 | Batch 0/5 | Loss 4.445041\n",
      "Epoch 183 | Test Acc = 84.53% +- 7.26%\n",
      "Epoch 183 | Train Loss 4.382297229766846 | Test Acc 84.53333333333333\n",
      "Epoch 184 | Batch 0/5 | Loss 4.395677\n",
      "Epoch 184 | Test Acc = 75.47% +- 9.31%\n",
      "Epoch 184 | Train Loss 4.312718677520752 | Test Acc 75.46666666666667\n",
      "Epoch 185 | Batch 0/5 | Loss 4.153296\n",
      "Epoch 185 | Test Acc = 76.80% +- 7.00%\n",
      "Epoch 185 | Train Loss 4.137876319885254 | Test Acc 76.8\n",
      "Epoch 186 | Batch 0/5 | Loss 4.139907\n",
      "Epoch 186 | Test Acc = 82.67% +- 11.38%\n",
      "Epoch 186 | Train Loss 4.269209098815918 | Test Acc 82.66666666666667\n",
      "Epoch 187 | Batch 0/5 | Loss 4.175445\n",
      "Epoch 187 | Test Acc = 78.67% +- 6.49%\n",
      "Epoch 187 | Train Loss 4.200783061981201 | Test Acc 78.66666666666666\n",
      "Epoch 188 | Batch 0/5 | Loss 4.193926\n",
      "Epoch 188 | Test Acc = 73.07% +- 11.81%\n",
      "Epoch 188 | Train Loss 4.220101261138916 | Test Acc 73.06666666666668\n",
      "Epoch 189 | Batch 0/5 | Loss 4.333332\n",
      "Epoch 189 | Test Acc = 77.07% +- 5.14%\n",
      "Epoch 189 | Train Loss 4.165157508850098 | Test Acc 77.06666666666668\n",
      "Epoch 190 | Batch 0/5 | Loss 4.102207\n",
      "Epoch 190 | Test Acc = 76.53% +- 7.77%\n",
      "Epoch 190 | Train Loss 4.185480213165283 | Test Acc 76.53333333333333\n",
      "Epoch 191 | Batch 0/5 | Loss 4.310921\n",
      "Epoch 191 | Test Acc = 81.07% +- 7.80%\n",
      "Epoch 191 | Train Loss 4.206688594818115 | Test Acc 81.06666666666666\n",
      "Epoch 192 | Batch 0/5 | Loss 4.228455\n",
      "Epoch 192 | Test Acc = 77.33% +- 9.72%\n",
      "Epoch 192 | Train Loss 4.152697372436523 | Test Acc 77.33333333333333\n",
      "Epoch 193 | Batch 0/5 | Loss 4.066558\n",
      "Epoch 193 | Test Acc = 77.87% +- 9.65%\n",
      "Epoch 193 | Train Loss 4.140086317062378 | Test Acc 77.86666666666666\n",
      "Epoch 194 | Batch 0/5 | Loss 4.104235\n",
      "Epoch 194 | Test Acc = 72.80% +- 15.18%\n",
      "Epoch 194 | Train Loss 4.201034069061279 | Test Acc 72.80000000000001\n",
      "Epoch 195 | Batch 0/5 | Loss 3.811092\n",
      "Epoch 195 | Test Acc = 84.00% +- 8.03%\n",
      "Epoch 195 | Train Loss 4.106304121017456 | Test Acc 84.0\n",
      "Epoch 196 | Batch 0/5 | Loss 4.358172\n",
      "Epoch 196 | Test Acc = 85.33% +- 4.90%\n",
      "Epoch 196 | Train Loss 4.217116355895996 | Test Acc 85.33333333333334\n",
      "Epoch 197 | Batch 0/5 | Loss 4.167767\n",
      "Epoch 197 | Test Acc = 84.00% +- 4.73%\n",
      "Epoch 197 | Train Loss 4.135715532302856 | Test Acc 84.0\n",
      "Epoch 198 | Batch 0/5 | Loss 4.191860\n",
      "Epoch 198 | Test Acc = 89.07% +- 2.14%\n",
      "Epoch 198 | Train Loss 4.102414608001709 | Test Acc 89.06666666666668\n",
      "Epoch 199 | Batch 0/5 | Loss 4.493226\n",
      "Epoch 199 | Test Acc = 87.47% +- 7.56%\n",
      "Epoch 199 | Train Loss 4.248063564300537 | Test Acc 87.46666666666667\n",
      "Epoch 200 | Batch 0/5 | Loss 4.000633\n",
      "Epoch 200 | Test Acc = 82.93% +- 2.90%\n",
      "Epoch 200 | Train Loss 4.064172220230103 | Test Acc 82.93333333333334\n",
      "Epoch 201 | Batch 0/5 | Loss 3.925240\n",
      "Epoch 201 | Test Acc = 80.27% +- 8.88%\n",
      "Epoch 201 | Train Loss 4.179929590225219 | Test Acc 80.26666666666668\n",
      "Epoch 202 | Batch 0/5 | Loss 3.974370\n",
      "Epoch 202 | Test Acc = 72.53% +- 6.96%\n",
      "Epoch 202 | Train Loss 4.113749694824219 | Test Acc 72.53333333333335\n",
      "Epoch 203 | Batch 0/5 | Loss 3.810305\n",
      "Epoch 203 | Test Acc = 69.87% +- 5.26%\n",
      "Epoch 203 | Train Loss 4.03613657951355 | Test Acc 69.86666666666667\n",
      "Epoch 204 | Batch 0/5 | Loss 3.936185\n",
      "Epoch 204 | Test Acc = 81.87% +- 12.43%\n",
      "Epoch 204 | Train Loss 4.074913454055786 | Test Acc 81.86666666666667\n",
      "Epoch 205 | Batch 0/5 | Loss 4.266271\n",
      "Epoch 205 | Test Acc = 72.80% +- 8.38%\n",
      "Epoch 205 | Train Loss 4.26637020111084 | Test Acc 72.80000000000001\n",
      "Epoch 206 | Batch 0/5 | Loss 4.056189\n",
      "Epoch 206 | Test Acc = 82.40% +- 6.16%\n",
      "Epoch 206 | Train Loss 4.106991100311279 | Test Acc 82.4\n",
      "Epoch 207 | Batch 0/5 | Loss 4.335237\n",
      "Epoch 207 | Test Acc = 84.53% +- 4.16%\n",
      "Epoch 207 | Train Loss 4.291095638275147 | Test Acc 84.53333333333333\n",
      "Epoch 208 | Batch 0/5 | Loss 4.418808\n",
      "Epoch 208 | Test Acc = 82.67% +- 6.18%\n",
      "Epoch 208 | Train Loss 4.177436447143554 | Test Acc 82.66666666666666\n",
      "Epoch 209 | Batch 0/5 | Loss 4.131950\n",
      "Epoch 209 | Test Acc = 80.27% +- 7.73%\n",
      "Epoch 209 | Train Loss 4.231364059448242 | Test Acc 80.26666666666667\n",
      "Epoch 210 | Batch 0/5 | Loss 4.105384\n",
      "Epoch 210 | Test Acc = 81.33% +- 8.06%\n",
      "Epoch 210 | Train Loss 4.04700722694397 | Test Acc 81.33333333333333\n",
      "Epoch 211 | Batch 0/5 | Loss 3.987032\n",
      "Epoch 211 | Test Acc = 71.47% +- 8.18%\n",
      "Epoch 211 | Train Loss 4.120724725723266 | Test Acc 71.46666666666667\n",
      "Epoch 212 | Batch 0/5 | Loss 4.054262\n",
      "Epoch 212 | Test Acc = 82.40% +- 5.89%\n",
      "Epoch 212 | Train Loss 4.08637228012085 | Test Acc 82.4\n",
      "Epoch 213 | Batch 0/5 | Loss 4.016715\n",
      "Epoch 213 | Test Acc = 80.80% +- 8.45%\n",
      "Epoch 213 | Train Loss 4.07845721244812 | Test Acc 80.8\n",
      "Epoch 214 | Batch 0/5 | Loss 3.927609\n",
      "Epoch 214 | Test Acc = 82.93% +- 6.99%\n",
      "Epoch 214 | Train Loss 4.014184188842774 | Test Acc 82.93333333333332\n",
      "Epoch 215 | Batch 0/5 | Loss 3.963931\n",
      "Epoch 215 | Test Acc = 81.33% +- 8.10%\n",
      "Epoch 215 | Train Loss 4.059964323043824 | Test Acc 81.33333333333333\n",
      "Epoch 216 | Batch 0/5 | Loss 4.177634\n",
      "Epoch 216 | Test Acc = 76.00% +- 11.57%\n",
      "Epoch 216 | Train Loss 4.132424211502075 | Test Acc 76.0\n",
      "Epoch 217 | Batch 0/5 | Loss 3.848215\n",
      "Epoch 217 | Test Acc = 74.67% +- 7.43%\n",
      "Epoch 217 | Train Loss 4.094370794296265 | Test Acc 74.66666666666666\n",
      "Epoch 218 | Batch 0/5 | Loss 3.916145\n",
      "Epoch 218 | Test Acc = 74.40% +- 10.90%\n",
      "Epoch 218 | Train Loss 4.097309160232544 | Test Acc 74.4\n",
      "Epoch 219 | Batch 0/5 | Loss 4.169437\n",
      "Epoch 219 | Test Acc = 79.20% +- 7.52%\n",
      "Epoch 219 | Train Loss 4.09380407333374 | Test Acc 79.2\n",
      "Epoch 220 | Batch 0/5 | Loss 4.254019\n",
      "Epoch 220 | Test Acc = 81.87% +- 4.22%\n",
      "Epoch 220 | Train Loss 4.043314981460571 | Test Acc 81.86666666666666\n",
      "Epoch 221 | Batch 0/5 | Loss 4.184598\n",
      "Epoch 221 | Test Acc = 79.47% +- 8.67%\n",
      "Epoch 221 | Train Loss 4.115049123764038 | Test Acc 79.46666666666667\n",
      "Epoch 222 | Batch 0/5 | Loss 3.939141\n",
      "Epoch 222 | Test Acc = 85.60% +- 3.73%\n",
      "Epoch 222 | Train Loss 4.0867187023162845 | Test Acc 85.6\n",
      "Epoch 223 | Batch 0/5 | Loss 4.138137\n",
      "Epoch 223 | Test Acc = 80.80% +- 9.19%\n",
      "Epoch 223 | Train Loss 4.2141951560974125 | Test Acc 80.8\n",
      "Epoch 224 | Batch 0/5 | Loss 4.137753\n",
      "Epoch 224 | Test Acc = 86.67% +- 4.50%\n",
      "Epoch 224 | Train Loss 4.023288011550903 | Test Acc 86.66666666666666\n",
      "Epoch 225 | Batch 0/5 | Loss 4.092978\n",
      "Epoch 225 | Test Acc = 81.87% +- 6.76%\n",
      "Epoch 225 | Train Loss 4.067837381362915 | Test Acc 81.86666666666666\n",
      "Epoch 226 | Batch 0/5 | Loss 3.783921\n",
      "Epoch 226 | Test Acc = 79.47% +- 10.07%\n",
      "Epoch 226 | Train Loss 3.9187389373779298 | Test Acc 79.46666666666667\n",
      "Epoch 227 | Batch 0/5 | Loss 3.854805\n",
      "Epoch 227 | Test Acc = 85.33% +- 8.33%\n",
      "Epoch 227 | Train Loss 3.9676657199859617 | Test Acc 85.33333333333333\n",
      "Epoch 228 | Batch 0/5 | Loss 4.075816\n",
      "Epoch 228 | Test Acc = 79.20% +- 6.64%\n",
      "Epoch 228 | Train Loss 4.081064224243164 | Test Acc 79.2\n",
      "Epoch 229 | Batch 0/5 | Loss 3.982948\n",
      "Epoch 229 | Test Acc = 83.20% +- 4.35%\n",
      "Epoch 229 | Train Loss 3.9869478702545167 | Test Acc 83.2\n",
      "Epoch 230 | Batch 0/5 | Loss 3.793914\n",
      "Epoch 230 | Test Acc = 78.67% +- 8.56%\n",
      "Epoch 230 | Train Loss 3.9662206172943115 | Test Acc 78.66666666666666\n",
      "Epoch 231 | Batch 0/5 | Loss 4.288140\n",
      "Epoch 231 | Test Acc = 86.13% +- 7.19%\n",
      "Epoch 231 | Train Loss 4.15721116065979 | Test Acc 86.13333333333333\n",
      "Epoch 232 | Batch 0/5 | Loss 3.695189\n",
      "Epoch 232 | Test Acc = 75.20% +- 7.49%\n",
      "Epoch 232 | Train Loss 3.9690876960754395 | Test Acc 75.2\n",
      "Epoch 233 | Batch 0/5 | Loss 4.024261\n",
      "Epoch 233 | Test Acc = 78.13% +- 9.71%\n",
      "Epoch 233 | Train Loss 4.079402065277099 | Test Acc 78.13333333333333\n",
      "Epoch 234 | Batch 0/5 | Loss 4.140555\n",
      "Epoch 234 | Test Acc = 81.60% +- 5.09%\n",
      "Epoch 234 | Train Loss 4.029121017456054 | Test Acc 81.6\n",
      "Epoch 235 | Batch 0/5 | Loss 4.089520\n",
      "Epoch 235 | Test Acc = 83.73% +- 7.84%\n",
      "Epoch 235 | Train Loss 3.981577157974243 | Test Acc 83.73333333333333\n",
      "Epoch 236 | Batch 0/5 | Loss 3.959901\n",
      "Epoch 236 | Test Acc = 84.80% +- 6.76%\n",
      "Epoch 236 | Train Loss 4.0606261730194095 | Test Acc 84.8\n",
      "Epoch 237 | Batch 0/5 | Loss 4.066503\n",
      "Epoch 237 | Test Acc = 81.33% +- 7.39%\n",
      "Epoch 237 | Train Loss 3.990185260772705 | Test Acc 81.33333333333333\n",
      "Epoch 238 | Batch 0/5 | Loss 3.797381\n",
      "Epoch 238 | Test Acc = 84.27% +- 12.51%\n",
      "Epoch 238 | Train Loss 3.9235743999481203 | Test Acc 84.26666666666668\n",
      "Epoch 239 | Batch 0/5 | Loss 3.870746\n",
      "Epoch 239 | Test Acc = 87.47% +- 4.88%\n",
      "Epoch 239 | Train Loss 3.9749502182006835 | Test Acc 87.46666666666665\n",
      "Epoch 240 | Batch 0/5 | Loss 3.745574\n",
      "Epoch 240 | Test Acc = 76.00% +- 9.92%\n",
      "Epoch 240 | Train Loss 3.981571674346924 | Test Acc 76.0\n",
      "Epoch 241 | Batch 0/5 | Loss 3.876166\n",
      "Epoch 241 | Test Acc = 79.20% +- 6.88%\n",
      "Epoch 241 | Train Loss 3.861088752746582 | Test Acc 79.2\n",
      "Epoch 242 | Batch 0/5 | Loss 4.390557\n",
      "Epoch 242 | Test Acc = 88.27% +- 6.50%\n",
      "Epoch 242 | Train Loss 4.110892677307129 | Test Acc 88.26666666666668\n",
      "Epoch 243 | Batch 0/5 | Loss 4.050094\n",
      "Epoch 243 | Test Acc = 77.87% +- 9.91%\n",
      "Epoch 243 | Train Loss 3.9357964038848876 | Test Acc 77.86666666666666\n",
      "Epoch 244 | Batch 0/5 | Loss 4.177398\n",
      "Epoch 244 | Test Acc = 86.93% +- 7.22%\n",
      "Epoch 244 | Train Loss 3.955436182022095 | Test Acc 86.93333333333332\n",
      "Epoch 245 | Batch 0/5 | Loss 4.223619\n",
      "Epoch 245 | Test Acc = 80.80% +- 3.01%\n",
      "Epoch 245 | Train Loss 4.007830619812012 | Test Acc 80.8\n",
      "Epoch 246 | Batch 0/5 | Loss 3.612646\n",
      "Epoch 246 | Test Acc = 75.73% +- 4.87%\n",
      "Epoch 246 | Train Loss 3.95558648109436 | Test Acc 75.73333333333333\n",
      "Epoch 247 | Batch 0/5 | Loss 3.839388\n",
      "Epoch 247 | Test Acc = 85.87% +- 4.65%\n",
      "Epoch 247 | Train Loss 3.779241418838501 | Test Acc 85.86666666666666\n",
      "Epoch 248 | Batch 0/5 | Loss 3.900110\n",
      "Epoch 248 | Test Acc = 84.53% +- 5.80%\n",
      "Epoch 248 | Train Loss 3.859226703643799 | Test Acc 84.53333333333333\n",
      "Epoch 249 | Batch 0/5 | Loss 4.028007\n",
      "Epoch 249 | Test Acc = 82.13% +- 9.48%\n",
      "Epoch 249 | Train Loss 4.0117041110992435 | Test Acc 82.13333333333334\n",
      "Epoch 250 | Batch 0/5 | Loss 3.771783\n",
      "Epoch 250 | Test Acc = 74.93% +- 9.33%\n",
      "Epoch 250 | Train Loss 4.010175657272339 | Test Acc 74.93333333333334\n",
      "Epoch 251 | Batch 0/5 | Loss 3.887647\n",
      "Epoch 251 | Test Acc = 78.13% +- 12.02%\n",
      "Epoch 251 | Train Loss 3.9323732376098635 | Test Acc 78.13333333333333\n",
      "Epoch 252 | Batch 0/5 | Loss 4.023925\n",
      "Epoch 252 | Test Acc = 84.80% +- 3.51%\n",
      "Epoch 252 | Train Loss 3.9129570960998534 | Test Acc 84.79999999999998\n",
      "Epoch 253 | Batch 0/5 | Loss 3.717846\n",
      "Epoch 253 | Test Acc = 77.60% +- 6.59%\n",
      "Epoch 253 | Train Loss 3.9084466457366944 | Test Acc 77.6\n",
      "Epoch 254 | Batch 0/5 | Loss 4.166064\n",
      "Epoch 254 | Test Acc = 88.27% +- 5.14%\n",
      "Epoch 254 | Train Loss 3.960837459564209 | Test Acc 88.26666666666667\n",
      "Epoch 255 | Batch 0/5 | Loss 3.973505\n",
      "Epoch 255 | Test Acc = 77.07% +- 9.79%\n",
      "Epoch 255 | Train Loss 3.93577184677124 | Test Acc 77.06666666666666\n",
      "Epoch 256 | Batch 0/5 | Loss 3.893891\n",
      "Epoch 256 | Test Acc = 89.87% +- 7.15%\n",
      "Epoch 256 | Train Loss 3.847366523742676 | Test Acc 89.86666666666666\n",
      "Epoch 257 | Batch 0/5 | Loss 3.791234\n",
      "Epoch 257 | Test Acc = 79.47% +- 11.75%\n",
      "Epoch 257 | Train Loss 3.9550454139709474 | Test Acc 79.46666666666667\n",
      "Epoch 258 | Batch 0/5 | Loss 3.752653\n",
      "Epoch 258 | Test Acc = 78.67% +- 5.87%\n",
      "Epoch 258 | Train Loss 3.856437110900879 | Test Acc 78.66666666666666\n",
      "Epoch 259 | Batch 0/5 | Loss 4.052397\n",
      "Epoch 259 | Test Acc = 80.53% +- 8.18%\n",
      "Epoch 259 | Train Loss 3.9677492141723634 | Test Acc 80.53333333333333\n",
      "Epoch 260 | Batch 0/5 | Loss 3.851809\n",
      "Epoch 260 | Test Acc = 86.67% +- 5.07%\n",
      "Epoch 260 | Train Loss 3.897964906692505 | Test Acc 86.66666666666666\n",
      "Epoch 261 | Batch 0/5 | Loss 3.695336\n",
      "Epoch 261 | Test Acc = 81.60% +- 8.21%\n",
      "Epoch 261 | Train Loss 3.819395351409912 | Test Acc 81.6\n",
      "Epoch 262 | Batch 0/5 | Loss 3.864148\n",
      "Epoch 262 | Test Acc = 81.33% +- 5.91%\n",
      "Epoch 262 | Train Loss 3.8806410789489747 | Test Acc 81.33333333333334\n",
      "Epoch 263 | Batch 0/5 | Loss 3.908526\n",
      "Epoch 263 | Test Acc = 87.73% +- 5.79%\n",
      "Epoch 263 | Train Loss 3.8564117908477784 | Test Acc 87.73333333333333\n",
      "Epoch 264 | Batch 0/5 | Loss 3.757279\n",
      "Epoch 264 | Test Acc = 74.67% +- 6.57%\n",
      "Epoch 264 | Train Loss 3.857079553604126 | Test Acc 74.66666666666666\n",
      "Epoch 265 | Batch 0/5 | Loss 3.863012\n",
      "Epoch 265 | Test Acc = 89.33% +- 3.14%\n",
      "Epoch 265 | Train Loss 3.9243001461029055 | Test Acc 89.33333333333334\n",
      "Epoch 266 | Batch 0/5 | Loss 3.834708\n",
      "Epoch 266 | Test Acc = 79.20% +- 4.94%\n",
      "Epoch 266 | Train Loss 3.911303234100342 | Test Acc 79.2\n",
      "Epoch 267 | Batch 0/5 | Loss 3.727326\n",
      "Epoch 267 | Test Acc = 78.13% +- 11.79%\n",
      "Epoch 267 | Train Loss 3.9618048667907715 | Test Acc 78.13333333333334\n",
      "Epoch 268 | Batch 0/5 | Loss 3.736426\n",
      "Epoch 268 | Test Acc = 82.40% +- 5.70%\n",
      "Epoch 268 | Train Loss 3.8674025535583496 | Test Acc 82.39999999999999\n",
      "Epoch 269 | Batch 0/5 | Loss 4.035417\n",
      "Epoch 269 | Test Acc = 90.67% +- 4.25%\n",
      "Epoch 269 | Train Loss 3.9086604595184324 | Test Acc 90.66666666666667\n",
      "Epoch 270 | Batch 0/5 | Loss 4.181540\n",
      "Epoch 270 | Test Acc = 84.53% +- 4.59%\n",
      "Epoch 270 | Train Loss 4.087713813781738 | Test Acc 84.53333333333335\n",
      "Epoch 271 | Batch 0/5 | Loss 3.933560\n",
      "Epoch 271 | Test Acc = 86.40% +- 6.91%\n",
      "Epoch 271 | Train Loss 3.8760214328765867 | Test Acc 86.4\n",
      "Epoch 272 | Batch 0/5 | Loss 3.730606\n",
      "Epoch 272 | Test Acc = 88.80% +- 5.66%\n",
      "Epoch 272 | Train Loss 3.8145030975341796 | Test Acc 88.79999999999998\n",
      "Epoch 273 | Batch 0/5 | Loss 3.813823\n",
      "Epoch 273 | Test Acc = 82.93% +- 6.33%\n",
      "Epoch 273 | Train Loss 3.8461886405944825 | Test Acc 82.93333333333332\n",
      "Epoch 274 | Batch 0/5 | Loss 4.166651\n",
      "Epoch 274 | Test Acc = 80.53% +- 2.62%\n",
      "Epoch 274 | Train Loss 3.9025941848754884 | Test Acc 80.53333333333333\n",
      "Epoch 275 | Batch 0/5 | Loss 3.751611\n",
      "Epoch 275 | Test Acc = 78.40% +- 13.37%\n",
      "Epoch 275 | Train Loss 3.848887491226196 | Test Acc 78.4\n",
      "Epoch 276 | Batch 0/5 | Loss 3.776628\n",
      "Epoch 276 | Test Acc = 74.93% +- 8.21%\n",
      "Epoch 276 | Train Loss 3.828193998336792 | Test Acc 74.93333333333334\n",
      "Epoch 277 | Batch 0/5 | Loss 3.647851\n",
      "Epoch 277 | Test Acc = 84.00% +- 5.91%\n",
      "Epoch 277 | Train Loss 3.8632081031799315 | Test Acc 84.0\n",
      "Epoch 278 | Batch 0/5 | Loss 3.801063\n",
      "Epoch 278 | Test Acc = 80.27% +- 5.79%\n",
      "Epoch 278 | Train Loss 3.9207199573516847 | Test Acc 80.26666666666668\n",
      "Epoch 279 | Batch 0/5 | Loss 4.170600\n",
      "Epoch 279 | Test Acc = 86.93% +- 4.76%\n",
      "Epoch 279 | Train Loss 3.9294293403625487 | Test Acc 86.93333333333334\n",
      "Epoch 280 | Batch 0/5 | Loss 3.611858\n",
      "Epoch 280 | Test Acc = 82.40% +- 4.87%\n",
      "Epoch 280 | Train Loss 3.769800043106079 | Test Acc 82.4\n",
      "Epoch 281 | Batch 0/5 | Loss 3.686838\n",
      "Epoch 281 | Test Acc = 79.20% +- 7.52%\n",
      "Epoch 281 | Train Loss 3.8620880603790284 | Test Acc 79.2\n",
      "Epoch 282 | Batch 0/5 | Loss 3.770492\n",
      "Epoch 282 | Test Acc = 80.80% +- 9.65%\n",
      "Epoch 282 | Train Loss 3.862474250793457 | Test Acc 80.8\n",
      "Epoch 283 | Batch 0/5 | Loss 3.752620\n",
      "Epoch 283 | Test Acc = 82.93% +- 6.42%\n",
      "Epoch 283 | Train Loss 3.9687057971954345 | Test Acc 82.93333333333334\n",
      "Epoch 284 | Batch 0/5 | Loss 3.952143\n",
      "Epoch 284 | Test Acc = 88.80% +- 4.88%\n",
      "Epoch 284 | Train Loss 3.7721689224243162 | Test Acc 88.8\n",
      "Epoch 285 | Batch 0/5 | Loss 3.859061\n",
      "Epoch 285 | Test Acc = 84.27% +- 5.20%\n",
      "Epoch 285 | Train Loss 3.7991436958312987 | Test Acc 84.26666666666667\n",
      "Epoch 286 | Batch 0/5 | Loss 3.628052\n",
      "Epoch 286 | Test Acc = 82.13% +- 5.71%\n",
      "Epoch 286 | Train Loss 3.729993534088135 | Test Acc 82.13333333333333\n",
      "Epoch 287 | Batch 0/5 | Loss 3.798547\n",
      "Epoch 287 | Test Acc = 82.40% +- 6.07%\n",
      "Epoch 287 | Train Loss 3.7686636447906494 | Test Acc 82.4\n",
      "Epoch 288 | Batch 0/5 | Loss 3.868611\n",
      "Epoch 288 | Test Acc = 78.67% +- 8.62%\n",
      "Epoch 288 | Train Loss 3.8120184898376466 | Test Acc 78.66666666666667\n",
      "Epoch 289 | Batch 0/5 | Loss 3.739346\n",
      "Epoch 289 | Test Acc = 82.40% +- 5.70%\n",
      "Epoch 289 | Train Loss 3.823039674758911 | Test Acc 82.4\n",
      "Epoch 290 | Batch 0/5 | Loss 3.897948\n",
      "Epoch 290 | Test Acc = 90.40% +- 3.73%\n",
      "Epoch 290 | Train Loss 3.8557621479034423 | Test Acc 90.4\n",
      "Epoch 291 | Batch 0/5 | Loss 3.924447\n",
      "Epoch 291 | Test Acc = 88.80% +- 3.01%\n",
      "Epoch 291 | Train Loss 3.834389066696167 | Test Acc 88.8\n",
      "Epoch 292 | Batch 0/5 | Loss 3.594473\n",
      "Epoch 292 | Test Acc = 78.13% +- 10.23%\n",
      "Epoch 292 | Train Loss 3.7066152572631834 | Test Acc 78.13333333333334\n",
      "Epoch 293 | Batch 0/5 | Loss 3.644485\n",
      "Epoch 293 | Test Acc = 89.87% +- 2.73%\n",
      "Epoch 293 | Train Loss 3.6554150581359863 | Test Acc 89.86666666666666\n",
      "Epoch 294 | Batch 0/5 | Loss 3.727848\n",
      "Epoch 294 | Test Acc = 92.27% +- 3.58%\n",
      "Epoch 294 | Train Loss 3.8064296722412108 | Test Acc 92.26666666666668\n",
      "Epoch 295 | Batch 0/5 | Loss 4.048237\n",
      "Epoch 295 | Test Acc = 84.53% +- 8.51%\n",
      "Epoch 295 | Train Loss 3.7956010818481447 | Test Acc 84.53333333333333\n",
      "Epoch 296 | Batch 0/5 | Loss 3.616312\n",
      "Epoch 296 | Test Acc = 84.00% +- 3.14%\n",
      "Epoch 296 | Train Loss 3.697619676589966 | Test Acc 84.00000000000001\n",
      "Epoch 297 | Batch 0/5 | Loss 4.039644\n",
      "Epoch 297 | Test Acc = 78.93% +- 5.30%\n",
      "Epoch 297 | Train Loss 3.801143836975098 | Test Acc 78.93333333333334\n",
      "Epoch 298 | Batch 0/5 | Loss 3.520134\n",
      "Epoch 298 | Test Acc = 85.33% +- 7.50%\n",
      "Epoch 298 | Train Loss 3.7568294525146486 | Test Acc 85.33333333333334\n",
      "Epoch 299 | Batch 0/5 | Loss 3.623700\n",
      "Epoch 299 | Test Acc = 80.00% +- 9.72%\n",
      "Epoch 299 | Train Loss 3.751832389831543 | Test Acc 80.0\n",
      "Epoch 300 | Batch 0/5 | Loss 3.655340\n",
      "Epoch 300 | Test Acc = 79.73% +- 8.63%\n",
      "Epoch 300 | Train Loss 3.8015854358673096 | Test Acc 79.73333333333333\n",
      "Epoch 301 | Batch 0/5 | Loss 3.683906\n",
      "Epoch 301 | Test Acc = 78.40% +- 10.20%\n",
      "Epoch 301 | Train Loss 3.7574209213256835 | Test Acc 78.39999999999999\n",
      "Epoch 302 | Batch 0/5 | Loss 3.768097\n",
      "Epoch 302 | Test Acc = 83.73% +- 6.67%\n",
      "Epoch 302 | Train Loss 3.7651342868804933 | Test Acc 83.73333333333333\n",
      "Epoch 303 | Batch 0/5 | Loss 3.724937\n",
      "Epoch 303 | Test Acc = 82.40% +- 9.82%\n",
      "Epoch 303 | Train Loss 3.8123433589935303 | Test Acc 82.40000000000002\n",
      "Epoch 304 | Batch 0/5 | Loss 3.575351\n",
      "Epoch 304 | Test Acc = 89.07% +- 3.73%\n",
      "Epoch 304 | Train Loss 3.7392269134521485 | Test Acc 89.06666666666668\n",
      "Epoch 305 | Batch 0/5 | Loss 3.585433\n",
      "Epoch 305 | Test Acc = 81.60% +- 10.82%\n",
      "Epoch 305 | Train Loss 3.7026302814483643 | Test Acc 81.6\n",
      "Epoch 306 | Batch 0/5 | Loss 3.760072\n",
      "Epoch 306 | Test Acc = 84.53% +- 8.45%\n",
      "Epoch 306 | Train Loss 3.664880323410034 | Test Acc 84.53333333333333\n",
      "Epoch 307 | Batch 0/5 | Loss 3.620880\n",
      "Epoch 307 | Test Acc = 82.40% +- 6.67%\n",
      "Epoch 307 | Train Loss 3.7088229656219482 | Test Acc 82.4\n",
      "Epoch 308 | Batch 0/5 | Loss 3.666934\n",
      "Epoch 308 | Test Acc = 82.13% +- 6.96%\n",
      "Epoch 308 | Train Loss 3.6761401653289796 | Test Acc 82.13333333333334\n",
      "Epoch 309 | Batch 0/5 | Loss 3.531964\n",
      "Epoch 309 | Test Acc = 80.80% +- 7.70%\n",
      "Epoch 309 | Train Loss 3.7386546611785887 | Test Acc 80.8\n",
      "Epoch 310 | Batch 0/5 | Loss 3.714313\n",
      "Epoch 310 | Test Acc = 83.20% +- 6.30%\n",
      "Epoch 310 | Train Loss 3.693281412124634 | Test Acc 83.2\n",
      "Epoch 311 | Batch 0/5 | Loss 3.603741\n",
      "Epoch 311 | Test Acc = 89.07% +- 3.73%\n",
      "Epoch 311 | Train Loss 3.711247444152832 | Test Acc 89.06666666666665\n",
      "Epoch 312 | Batch 0/5 | Loss 3.444191\n",
      "Epoch 312 | Test Acc = 81.33% +- 9.26%\n",
      "Epoch 312 | Train Loss 3.6164782524108885 | Test Acc 81.33333333333333\n",
      "Epoch 313 | Batch 0/5 | Loss 3.714775\n",
      "Epoch 313 | Test Acc = 90.67% +- 3.84%\n",
      "Epoch 313 | Train Loss 3.777089166641235 | Test Acc 90.66666666666666\n",
      "Epoch 314 | Batch 0/5 | Loss 4.103399\n",
      "Epoch 314 | Test Acc = 84.27% +- 6.33%\n",
      "Epoch 314 | Train Loss 3.7710334777832033 | Test Acc 84.26666666666667\n",
      "Epoch 315 | Batch 0/5 | Loss 3.839875\n",
      "Epoch 315 | Test Acc = 87.20% +- 10.98%\n",
      "Epoch 315 | Train Loss 3.6183313369750976 | Test Acc 87.2\n",
      "Epoch 316 | Batch 0/5 | Loss 3.847653\n",
      "Epoch 316 | Test Acc = 88.00% +- 3.98%\n",
      "Epoch 316 | Train Loss 3.5509900569915773 | Test Acc 87.99999999999999\n",
      "Epoch 317 | Batch 0/5 | Loss 3.879529\n",
      "Epoch 317 | Test Acc = 84.53% +- 7.52%\n",
      "Epoch 317 | Train Loss 3.630137586593628 | Test Acc 84.53333333333333\n",
      "Epoch 318 | Batch 0/5 | Loss 3.674901\n",
      "Epoch 318 | Test Acc = 84.27% +- 9.27%\n",
      "Epoch 318 | Train Loss 3.6456889629364015 | Test Acc 84.26666666666665\n",
      "Epoch 319 | Batch 0/5 | Loss 3.748053\n",
      "Epoch 319 | Test Acc = 76.80% +- 9.28%\n",
      "Epoch 319 | Train Loss 3.671063184738159 | Test Acc 76.8\n",
      "Epoch 320 | Batch 0/5 | Loss 3.721989\n",
      "Epoch 320 | Test Acc = 77.87% +- 8.02%\n",
      "Epoch 320 | Train Loss 3.6490417957305907 | Test Acc 77.86666666666666\n",
      "Epoch 321 | Batch 0/5 | Loss 3.707933\n",
      "Epoch 321 | Test Acc = 76.27% +- 11.53%\n",
      "Epoch 321 | Train Loss 3.5605278491973875 | Test Acc 76.26666666666667\n",
      "Epoch 322 | Batch 0/5 | Loss 3.899077\n",
      "Epoch 322 | Test Acc = 84.53% +- 7.11%\n",
      "Epoch 322 | Train Loss 3.6346060276031493 | Test Acc 84.53333333333333\n",
      "Epoch 323 | Batch 0/5 | Loss 3.788661\n",
      "Epoch 323 | Test Acc = 83.47% +- 5.26%\n",
      "Epoch 323 | Train Loss 3.6836297512054443 | Test Acc 83.46666666666665\n",
      "Epoch 324 | Batch 0/5 | Loss 3.692166\n",
      "Epoch 324 | Test Acc = 86.93% +- 5.70%\n",
      "Epoch 324 | Train Loss 3.7138427257537843 | Test Acc 86.93333333333334\n",
      "Epoch 325 | Batch 0/5 | Loss 3.708872\n",
      "Epoch 325 | Test Acc = 78.13% +- 9.85%\n",
      "Epoch 325 | Train Loss 3.593972635269165 | Test Acc 78.13333333333334\n",
      "Epoch 326 | Batch 0/5 | Loss 3.494306\n",
      "Epoch 326 | Test Acc = 84.27% +- 4.98%\n",
      "Epoch 326 | Train Loss 3.627958297729492 | Test Acc 84.26666666666668\n",
      "Epoch 327 | Batch 0/5 | Loss 3.466555\n",
      "Epoch 327 | Test Acc = 88.80% +- 5.15%\n",
      "Epoch 327 | Train Loss 3.5744436740875245 | Test Acc 88.80000000000001\n",
      "Epoch 328 | Batch 0/5 | Loss 3.638961\n",
      "Epoch 328 | Test Acc = 86.40% +- 7.11%\n",
      "Epoch 328 | Train Loss 3.6658652782440186 | Test Acc 86.4\n",
      "Epoch 329 | Batch 0/5 | Loss 3.893783\n",
      "Epoch 329 | Test Acc = 91.47% +- 1.75%\n",
      "Epoch 329 | Train Loss 3.745020627975464 | Test Acc 91.46666666666667\n",
      "Epoch 330 | Batch 0/5 | Loss 3.435127\n",
      "Epoch 330 | Test Acc = 84.80% +- 8.18%\n",
      "Epoch 330 | Train Loss 3.5402812004089355 | Test Acc 84.8\n",
      "Epoch 331 | Batch 0/5 | Loss 3.553984\n",
      "Epoch 331 | Test Acc = 82.67% +- 3.98%\n",
      "Epoch 331 | Train Loss 3.6918008804321287 | Test Acc 82.66666666666666\n",
      "Epoch 332 | Batch 0/5 | Loss 3.431779\n",
      "Epoch 332 | Test Acc = 76.53% +- 6.55%\n",
      "Epoch 332 | Train Loss 3.6855541706085204 | Test Acc 76.53333333333333\n",
      "Epoch 333 | Batch 0/5 | Loss 3.308223\n",
      "Epoch 333 | Test Acc = 88.00% +- 9.38%\n",
      "Epoch 333 | Train Loss 3.574628210067749 | Test Acc 88.0\n",
      "Epoch 334 | Batch 0/5 | Loss 3.676122\n",
      "Epoch 334 | Test Acc = 84.80% +- 6.72%\n",
      "Epoch 334 | Train Loss 3.6427400588989256 | Test Acc 84.8\n",
      "Epoch 335 | Batch 0/5 | Loss 3.354117\n",
      "Epoch 335 | Test Acc = 84.00% +- 10.51%\n",
      "Epoch 335 | Train Loss 3.5243301391601562 | Test Acc 84.0\n",
      "Epoch 336 | Batch 0/5 | Loss 3.860642\n",
      "Epoch 336 | Test Acc = 82.13% +- 8.18%\n",
      "Epoch 336 | Train Loss 3.7422711849212646 | Test Acc 82.13333333333333\n",
      "Epoch 337 | Batch 0/5 | Loss 3.570938\n",
      "Epoch 337 | Test Acc = 85.07% +- 5.79%\n",
      "Epoch 337 | Train Loss 3.6646021366119386 | Test Acc 85.06666666666665\n",
      "Epoch 338 | Batch 0/5 | Loss 3.563178\n",
      "Epoch 338 | Test Acc = 87.73% +- 5.70%\n",
      "Epoch 338 | Train Loss 3.5216322422027586 | Test Acc 87.73333333333332\n",
      "Epoch 339 | Batch 0/5 | Loss 3.488991\n",
      "Epoch 339 | Test Acc = 82.93% +- 9.45%\n",
      "Epoch 339 | Train Loss 3.4441818714141847 | Test Acc 82.93333333333334\n",
      "Epoch 340 | Batch 0/5 | Loss 3.812510\n",
      "Epoch 340 | Test Acc = 82.93% +- 9.03%\n",
      "Epoch 340 | Train Loss 3.7775694847106935 | Test Acc 82.93333333333332\n",
      "Epoch 341 | Batch 0/5 | Loss 3.597075\n",
      "Epoch 341 | Test Acc = 86.40% +- 10.12%\n",
      "Epoch 341 | Train Loss 3.553303050994873 | Test Acc 86.4\n",
      "Epoch 342 | Batch 0/5 | Loss 3.649538\n",
      "Epoch 342 | Test Acc = 82.93% +- 5.70%\n",
      "Epoch 342 | Train Loss 3.595793056488037 | Test Acc 82.93333333333332\n",
      "Epoch 343 | Batch 0/5 | Loss 3.610702\n",
      "Epoch 343 | Test Acc = 84.00% +- 11.36%\n",
      "Epoch 343 | Train Loss 3.7495365142822266 | Test Acc 84.00000000000001\n",
      "Epoch 344 | Batch 0/5 | Loss 3.570793\n",
      "Epoch 344 | Test Acc = 88.27% +- 2.50%\n",
      "Epoch 344 | Train Loss 3.570328950881958 | Test Acc 88.26666666666667\n",
      "Epoch 345 | Batch 0/5 | Loss 3.892292\n",
      "Epoch 345 | Test Acc = 86.93% +- 3.42%\n",
      "Epoch 345 | Train Loss 3.6514784336090087 | Test Acc 86.93333333333332\n",
      "Epoch 346 | Batch 0/5 | Loss 3.695089\n",
      "Epoch 346 | Test Acc = 85.33% +- 5.33%\n",
      "Epoch 346 | Train Loss 3.5710259437561036 | Test Acc 85.33333333333334\n",
      "Epoch 347 | Batch 0/5 | Loss 3.528839\n",
      "Epoch 347 | Test Acc = 83.47% +- 12.69%\n",
      "Epoch 347 | Train Loss 3.5763144493103027 | Test Acc 83.46666666666667\n",
      "Epoch 348 | Batch 0/5 | Loss 3.653103\n",
      "Epoch 348 | Test Acc = 82.40% +- 6.59%\n",
      "Epoch 348 | Train Loss 3.627772808074951 | Test Acc 82.4\n",
      "Epoch 349 | Batch 0/5 | Loss 3.800300\n",
      "Epoch 349 | Test Acc = 84.00% +- 8.81%\n",
      "Epoch 349 | Train Loss 3.6988768100738527 | Test Acc 84.0\n",
      "Epoch 350 | Batch 0/5 | Loss 3.496473\n",
      "Epoch 350 | Test Acc = 86.40% +- 4.21%\n",
      "Epoch 350 | Train Loss 3.660862445831299 | Test Acc 86.4\n",
      "Epoch 351 | Batch 0/5 | Loss 3.661743\n",
      "Epoch 351 | Test Acc = 82.93% +- 8.91%\n",
      "Epoch 351 | Train Loss 3.528438425064087 | Test Acc 82.93333333333332\n",
      "Epoch 352 | Batch 0/5 | Loss 3.585987\n",
      "Epoch 352 | Test Acc = 89.33% +- 7.57%\n",
      "Epoch 352 | Train Loss 3.56111626625061 | Test Acc 89.33333333333334\n",
      "Epoch 353 | Batch 0/5 | Loss 3.745330\n",
      "Epoch 353 | Test Acc = 78.13% +- 11.15%\n",
      "Epoch 353 | Train Loss 3.5869492053985597 | Test Acc 78.13333333333334\n",
      "Epoch 354 | Batch 0/5 | Loss 3.495832\n",
      "Epoch 354 | Test Acc = 85.60% +- 5.20%\n",
      "Epoch 354 | Train Loss 3.359106159210205 | Test Acc 85.6\n",
      "Epoch 355 | Batch 0/5 | Loss 3.745345\n",
      "Epoch 355 | Test Acc = 89.60% +- 5.65%\n",
      "Epoch 355 | Train Loss 3.5672964572906496 | Test Acc 89.6\n",
      "Epoch 356 | Batch 0/5 | Loss 3.689646\n",
      "Epoch 356 | Test Acc = 76.27% +- 11.74%\n",
      "Epoch 356 | Train Loss 3.6312649726867674 | Test Acc 76.26666666666668\n",
      "Epoch 357 | Batch 0/5 | Loss 3.483283\n",
      "Epoch 357 | Test Acc = 79.20% +- 5.41%\n",
      "Epoch 357 | Train Loss 3.5568030834198 | Test Acc 79.2\n",
      "Epoch 358 | Batch 0/5 | Loss 3.528874\n",
      "Epoch 358 | Test Acc = 91.20% +- 7.74%\n",
      "Epoch 358 | Train Loss 3.5192838668823243 | Test Acc 91.2\n",
      "Epoch 359 | Batch 0/5 | Loss 3.421754\n",
      "Epoch 359 | Test Acc = 90.13% +- 4.16%\n",
      "Epoch 359 | Train Loss 3.5149102210998535 | Test Acc 90.13333333333334\n",
      "Epoch 360 | Batch 0/5 | Loss 3.409521\n",
      "Epoch 360 | Test Acc = 83.73% +- 8.38%\n",
      "Epoch 360 | Train Loss 3.5115817546844483 | Test Acc 83.73333333333332\n",
      "Epoch 361 | Batch 0/5 | Loss 3.571407\n",
      "Epoch 361 | Test Acc = 80.27% +- 12.29%\n",
      "Epoch 361 | Train Loss 3.5429819107055662 | Test Acc 80.26666666666668\n",
      "Epoch 362 | Batch 0/5 | Loss 4.095940\n",
      "Epoch 362 | Test Acc = 82.13% +- 3.10%\n",
      "Epoch 362 | Train Loss 3.6885854244232177 | Test Acc 82.13333333333333\n",
      "Epoch 363 | Batch 0/5 | Loss 3.495896\n",
      "Epoch 363 | Test Acc = 85.60% +- 10.04%\n",
      "Epoch 363 | Train Loss 3.5299380302429197 | Test Acc 85.6\n",
      "Epoch 364 | Batch 0/5 | Loss 3.625750\n",
      "Epoch 364 | Test Acc = 85.33% +- 4.79%\n",
      "Epoch 364 | Train Loss 3.5628782749176025 | Test Acc 85.33333333333334\n",
      "Epoch 365 | Batch 0/5 | Loss 3.409587\n",
      "Epoch 365 | Test Acc = 81.07% +- 7.97%\n",
      "Epoch 365 | Train Loss 3.492001438140869 | Test Acc 81.06666666666668\n",
      "Epoch 366 | Batch 0/5 | Loss 3.388743\n",
      "Epoch 366 | Test Acc = 90.93% +- 2.71%\n",
      "Epoch 366 | Train Loss 3.3611871719360353 | Test Acc 90.93333333333332\n",
      "Epoch 367 | Batch 0/5 | Loss 3.555192\n",
      "Epoch 367 | Test Acc = 88.27% +- 4.98%\n",
      "Epoch 367 | Train Loss 3.5176655769348146 | Test Acc 88.26666666666667\n",
      "Epoch 368 | Batch 0/5 | Loss 3.287789\n",
      "Epoch 368 | Test Acc = 84.27% +- 7.80%\n",
      "Epoch 368 | Train Loss 3.39255690574646 | Test Acc 84.26666666666667\n",
      "Epoch 369 | Batch 0/5 | Loss 3.306278\n",
      "Epoch 369 | Test Acc = 82.93% +- 8.91%\n",
      "Epoch 369 | Train Loss 3.4268914222717286 | Test Acc 82.93333333333334\n",
      "Epoch 370 | Batch 0/5 | Loss 3.886261\n",
      "Epoch 370 | Test Acc = 82.40% +- 8.21%\n",
      "Epoch 370 | Train Loss 3.573863315582275 | Test Acc 82.4\n",
      "Epoch 371 | Batch 0/5 | Loss 3.677901\n",
      "Epoch 371 | Test Acc = 91.20% +- 1.59%\n",
      "Epoch 371 | Train Loss 3.587505340576172 | Test Acc 91.19999999999999\n",
      "Epoch 372 | Batch 0/5 | Loss 3.591507\n",
      "Epoch 372 | Test Acc = 85.07% +- 5.89%\n",
      "Epoch 372 | Train Loss 3.546398401260376 | Test Acc 85.06666666666668\n",
      "Epoch 373 | Batch 0/5 | Loss 3.708502\n",
      "Epoch 373 | Test Acc = 83.73% +- 5.14%\n",
      "Epoch 373 | Train Loss 3.4956900119781493 | Test Acc 83.73333333333332\n",
      "Epoch 374 | Batch 0/5 | Loss 3.339279\n",
      "Epoch 374 | Test Acc = 73.33% +- 12.26%\n",
      "Epoch 374 | Train Loss 3.4349148750305174 | Test Acc 73.33333333333333\n",
      "Epoch 375 | Batch 0/5 | Loss 3.332273\n",
      "Epoch 375 | Test Acc = 82.13% +- 7.26%\n",
      "Epoch 375 | Train Loss 3.505806732177734 | Test Acc 82.13333333333334\n",
      "Epoch 376 | Batch 0/5 | Loss 3.679564\n",
      "Epoch 376 | Test Acc = 85.07% +- 6.07%\n",
      "Epoch 376 | Train Loss 3.6062942504882813 | Test Acc 85.06666666666666\n",
      "Epoch 377 | Batch 0/5 | Loss 3.829347\n",
      "Epoch 377 | Test Acc = 85.60% +- 8.73%\n",
      "Epoch 377 | Train Loss 3.700853109359741 | Test Acc 85.6\n",
      "Epoch 378 | Batch 0/5 | Loss 3.200269\n",
      "Epoch 378 | Test Acc = 78.93% +- 7.26%\n",
      "Epoch 378 | Train Loss 3.3408100605010986 | Test Acc 78.93333333333332\n",
      "Epoch 379 | Batch 0/5 | Loss 3.196967\n",
      "Epoch 379 | Test Acc = 84.53% +- 2.41%\n",
      "Epoch 379 | Train Loss 3.4148613452911376 | Test Acc 84.53333333333333\n",
      "Epoch 380 | Batch 0/5 | Loss 3.445601\n",
      "Epoch 380 | Test Acc = 88.53% +- 4.77%\n",
      "Epoch 380 | Train Loss 3.6187633514404296 | Test Acc 88.53333333333333\n",
      "Epoch 381 | Batch 0/5 | Loss 3.604570\n",
      "Epoch 381 | Test Acc = 89.33% +- 3.84%\n",
      "Epoch 381 | Train Loss 3.535681629180908 | Test Acc 89.33333333333333\n",
      "Epoch 382 | Batch 0/5 | Loss 3.298443\n",
      "Epoch 382 | Test Acc = 86.13% +- 3.51%\n",
      "Epoch 382 | Train Loss 3.4203696727752684 | Test Acc 86.13333333333334\n",
      "Epoch 383 | Batch 0/5 | Loss 3.073746\n",
      "Epoch 383 | Test Acc = 78.67% +- 8.99%\n",
      "Epoch 383 | Train Loss 3.4950207233428956 | Test Acc 78.66666666666666\n",
      "Epoch 384 | Batch 0/5 | Loss 3.947484\n",
      "Epoch 384 | Test Acc = 81.33% +- 6.18%\n",
      "Epoch 384 | Train Loss 3.5417169094085694 | Test Acc 81.33333333333333\n",
      "Epoch 385 | Batch 0/5 | Loss 3.414416\n",
      "Epoch 385 | Test Acc = 88.27% +- 5.74%\n",
      "Epoch 385 | Train Loss 3.4876195430755614 | Test Acc 88.26666666666667\n",
      "Epoch 386 | Batch 0/5 | Loss 3.526274\n",
      "Epoch 386 | Test Acc = 79.73% +- 9.53%\n",
      "Epoch 386 | Train Loss 3.4209296703338623 | Test Acc 79.73333333333333\n",
      "Epoch 387 | Batch 0/5 | Loss 3.508428\n",
      "Epoch 387 | Test Acc = 91.73% +- 4.14%\n",
      "Epoch 387 | Train Loss 3.3711732387542725 | Test Acc 91.73333333333333\n",
      "Epoch 388 | Batch 0/5 | Loss 3.286572\n",
      "Epoch 388 | Test Acc = 81.60% +- 8.41%\n",
      "Epoch 388 | Train Loss 3.3151463985443117 | Test Acc 81.6\n",
      "Epoch 389 | Batch 0/5 | Loss 3.088742\n",
      "Epoch 389 | Test Acc = 82.67% +- 10.32%\n",
      "Epoch 389 | Train Loss 3.3424277305603027 | Test Acc 82.66666666666667\n",
      "Epoch 390 | Batch 0/5 | Loss 3.660609\n",
      "Epoch 390 | Test Acc = 79.73% +- 7.94%\n",
      "Epoch 390 | Train Loss 3.534505605697632 | Test Acc 79.73333333333332\n",
      "Epoch 391 | Batch 0/5 | Loss 3.357709\n",
      "Epoch 391 | Test Acc = 81.87% +- 9.99%\n",
      "Epoch 391 | Train Loss 3.406834602355957 | Test Acc 81.86666666666666\n",
      "Epoch 392 | Batch 0/5 | Loss 3.486067\n",
      "Epoch 392 | Test Acc = 85.60% +- 5.25%\n",
      "Epoch 392 | Train Loss 3.4365635395050047 | Test Acc 85.6\n",
      "Epoch 393 | Batch 0/5 | Loss 3.148373\n",
      "Epoch 393 | Test Acc = 84.27% +- 7.03%\n",
      "Epoch 393 | Train Loss 3.4832091331481934 | Test Acc 84.26666666666667\n",
      "Epoch 394 | Batch 0/5 | Loss 3.334139\n",
      "Epoch 394 | Test Acc = 85.07% +- 8.41%\n",
      "Epoch 394 | Train Loss 3.330099010467529 | Test Acc 85.06666666666669\n",
      "Epoch 395 | Batch 0/5 | Loss 3.719491\n",
      "Epoch 395 | Test Acc = 82.67% +- 11.55%\n",
      "Epoch 395 | Train Loss 3.541144275665283 | Test Acc 82.66666666666666\n",
      "Epoch 396 | Batch 0/5 | Loss 3.448390\n",
      "Epoch 396 | Test Acc = 84.27% +- 7.29%\n",
      "Epoch 396 | Train Loss 3.4921645641326906 | Test Acc 84.26666666666668\n",
      "Epoch 397 | Batch 0/5 | Loss 3.383789\n",
      "Epoch 397 | Test Acc = 86.40% +- 5.98%\n",
      "Epoch 397 | Train Loss 3.40130352973938 | Test Acc 86.39999999999999\n",
      "Epoch 398 | Batch 0/5 | Loss 3.552104\n",
      "Epoch 398 | Test Acc = 88.80% +- 3.35%\n",
      "Epoch 398 | Train Loss 3.318077802658081 | Test Acc 88.8\n",
      "Epoch 399 | Batch 0/5 | Loss 3.743242\n",
      "Epoch 399 | Test Acc = 86.40% +- 6.16%\n",
      "Epoch 399 | Train Loss 3.482639932632446 | Test Acc 86.4\n",
      "Epoch 400 | Batch 0/5 | Loss 3.297796\n",
      "Epoch 400 | Test Acc = 94.13% +- 2.17%\n",
      "Epoch 400 | Train Loss 3.2606537342071533 | Test Acc 94.13333333333335\n",
      "Epoch 401 | Batch 0/5 | Loss 3.777388\n",
      "Epoch 401 | Test Acc = 78.93% +- 9.96%\n",
      "Epoch 401 | Train Loss 3.4871105670928957 | Test Acc 78.93333333333332\n",
      "Epoch 402 | Batch 0/5 | Loss 3.975243\n",
      "Epoch 402 | Test Acc = 89.60% +- 6.33%\n",
      "Epoch 402 | Train Loss 3.5277413368225097 | Test Acc 89.6\n",
      "Epoch 403 | Batch 0/5 | Loss 3.187514\n",
      "Epoch 403 | Test Acc = 86.13% +- 7.15%\n",
      "Epoch 403 | Train Loss 3.3325444221496583 | Test Acc 86.13333333333333\n",
      "Epoch 404 | Batch 0/5 | Loss 3.431600\n",
      "Epoch 404 | Test Acc = 84.00% +- 6.05%\n",
      "Epoch 404 | Train Loss 3.350487470626831 | Test Acc 84.0\n",
      "Epoch 405 | Batch 0/5 | Loss 3.343981\n",
      "Epoch 405 | Test Acc = 80.27% +- 5.79%\n",
      "Epoch 405 | Train Loss 3.3457600593566896 | Test Acc 80.26666666666668\n",
      "Epoch 406 | Batch 0/5 | Loss 3.364640\n",
      "Epoch 406 | Test Acc = 80.27% +- 4.98%\n",
      "Epoch 406 | Train Loss 3.393934488296509 | Test Acc 80.26666666666668\n",
      "Epoch 407 | Batch 0/5 | Loss 3.324082\n",
      "Epoch 407 | Test Acc = 80.00% +- 5.48%\n",
      "Epoch 407 | Train Loss 3.291246461868286 | Test Acc 80.0\n",
      "Epoch 408 | Batch 0/5 | Loss 3.310839\n",
      "Epoch 408 | Test Acc = 87.20% +- 5.41%\n",
      "Epoch 408 | Train Loss 3.2890252590179445 | Test Acc 87.2\n",
      "Epoch 409 | Batch 0/5 | Loss 3.583306\n",
      "Epoch 409 | Test Acc = 74.13% +- 14.23%\n",
      "Epoch 409 | Train Loss 3.3031036853790283 | Test Acc 74.13333333333333\n",
      "Epoch 410 | Batch 0/5 | Loss 3.585233\n",
      "Epoch 410 | Test Acc = 89.87% +- 5.56%\n",
      "Epoch 410 | Train Loss 3.4743667125701903 | Test Acc 89.86666666666667\n",
      "Epoch 411 | Batch 0/5 | Loss 3.569350\n",
      "Epoch 411 | Test Acc = 90.93% +- 5.30%\n",
      "Epoch 411 | Train Loss 3.338617467880249 | Test Acc 90.93333333333332\n",
      "Epoch 412 | Batch 0/5 | Loss 3.040587\n",
      "Epoch 412 | Test Acc = 87.47% +- 7.63%\n",
      "Epoch 412 | Train Loss 3.225063705444336 | Test Acc 87.46666666666667\n",
      "Epoch 413 | Batch 0/5 | Loss 3.513398\n",
      "Epoch 413 | Test Acc = 84.00% +- 4.90%\n",
      "Epoch 413 | Train Loss 3.3879159927368163 | Test Acc 84.00000000000001\n",
      "Epoch 414 | Batch 0/5 | Loss 3.361004\n",
      "Epoch 414 | Test Acc = 82.13% +- 3.95%\n",
      "Epoch 414 | Train Loss 3.460707664489746 | Test Acc 82.13333333333334\n",
      "Epoch 415 | Batch 0/5 | Loss 3.368201\n",
      "Epoch 415 | Test Acc = 81.87% +- 12.43%\n",
      "Epoch 415 | Train Loss 3.4746146202087402 | Test Acc 81.86666666666666\n",
      "Epoch 416 | Batch 0/5 | Loss 3.246326\n",
      "Epoch 416 | Test Acc = 88.80% +- 5.61%\n",
      "Epoch 416 | Train Loss 3.2125218868255616 | Test Acc 88.8\n",
      "Epoch 417 | Batch 0/5 | Loss 3.241332\n",
      "Epoch 417 | Test Acc = 82.67% +- 5.48%\n",
      "Epoch 417 | Train Loss 3.29189395904541 | Test Acc 82.66666666666666\n",
      "Epoch 418 | Batch 0/5 | Loss 3.205386\n",
      "Epoch 418 | Test Acc = 87.20% +- 5.89%\n",
      "Epoch 418 | Train Loss 3.279445838928223 | Test Acc 87.2\n",
      "Epoch 419 | Batch 0/5 | Loss 3.387560\n",
      "Epoch 419 | Test Acc = 87.73% +- 6.16%\n",
      "Epoch 419 | Train Loss 3.401564598083496 | Test Acc 87.73333333333333\n",
      "Epoch 420 | Batch 0/5 | Loss 3.295226\n",
      "Epoch 420 | Test Acc = 82.67% +- 5.68%\n",
      "Epoch 420 | Train Loss 3.3590711116790772 | Test Acc 82.66666666666667\n",
      "Epoch 421 | Batch 0/5 | Loss 3.305894\n",
      "Epoch 421 | Test Acc = 87.73% +- 4.81%\n",
      "Epoch 421 | Train Loss 3.3616841793060304 | Test Acc 87.73333333333332\n",
      "Epoch 422 | Batch 0/5 | Loss 3.438086\n",
      "Epoch 422 | Test Acc = 83.73% +- 7.70%\n",
      "Epoch 422 | Train Loss 3.424918365478516 | Test Acc 83.73333333333333\n",
      "Epoch 423 | Batch 0/5 | Loss 3.351314\n",
      "Epoch 423 | Test Acc = 92.00% +- 5.12%\n",
      "Epoch 423 | Train Loss 3.331720781326294 | Test Acc 92.00000000000001\n",
      "Epoch 424 | Batch 0/5 | Loss 3.457874\n",
      "Epoch 424 | Test Acc = 91.20% +- 4.77%\n",
      "Epoch 424 | Train Loss 3.3936225891113283 | Test Acc 91.2\n",
      "Epoch 425 | Batch 0/5 | Loss 3.290412\n",
      "Epoch 425 | Test Acc = 92.80% +- 3.67%\n",
      "Epoch 425 | Train Loss 3.3229068756103515 | Test Acc 92.8\n",
      "Epoch 426 | Batch 0/5 | Loss 3.411223\n",
      "Epoch 426 | Test Acc = 88.00% +- 7.01%\n",
      "Epoch 426 | Train Loss 3.330735921859741 | Test Acc 88.0\n",
      "Epoch 427 | Batch 0/5 | Loss 3.082468\n",
      "Epoch 427 | Test Acc = 88.00% +- 7.13%\n",
      "Epoch 427 | Train Loss 3.291223907470703 | Test Acc 88.0\n",
      "Epoch 428 | Batch 0/5 | Loss 3.282872\n",
      "Epoch 428 | Test Acc = 85.33% +- 8.81%\n",
      "Epoch 428 | Train Loss 3.2245026588439942 | Test Acc 85.33333333333333\n",
      "Epoch 429 | Batch 0/5 | Loss 3.430297\n",
      "Epoch 429 | Test Acc = 80.53% +- 8.76%\n",
      "Epoch 429 | Train Loss 3.3737951278686524 | Test Acc 80.53333333333333\n",
      "Epoch 430 | Batch 0/5 | Loss 3.746717\n",
      "Epoch 430 | Test Acc = 82.13% +- 5.56%\n",
      "Epoch 430 | Train Loss 3.515113925933838 | Test Acc 82.13333333333333\n",
      "Epoch 431 | Batch 0/5 | Loss 3.292764\n",
      "Epoch 431 | Test Acc = 89.87% +- 6.80%\n",
      "Epoch 431 | Train Loss 3.328333616256714 | Test Acc 89.86666666666665\n",
      "Epoch 432 | Batch 0/5 | Loss 3.349421\n",
      "Epoch 432 | Test Acc = 88.27% +- 2.38%\n",
      "Epoch 432 | Train Loss 3.387355613708496 | Test Acc 88.26666666666667\n",
      "Epoch 433 | Batch 0/5 | Loss 2.864613\n",
      "Epoch 433 | Test Acc = 88.27% +- 6.07%\n",
      "Epoch 433 | Train Loss 3.1620798110961914 | Test Acc 88.26666666666667\n",
      "Epoch 434 | Batch 0/5 | Loss 3.474973\n",
      "Epoch 434 | Test Acc = 93.07% +- 4.14%\n",
      "Epoch 434 | Train Loss 3.311237096786499 | Test Acc 93.06666666666666\n",
      "Epoch 435 | Batch 0/5 | Loss 3.272449\n",
      "Epoch 435 | Test Acc = 88.53% +- 3.51%\n",
      "Epoch 435 | Train Loss 3.2972270488739013 | Test Acc 88.53333333333333\n",
      "Epoch 436 | Batch 0/5 | Loss 3.366377\n",
      "Epoch 436 | Test Acc = 83.47% +- 9.96%\n",
      "Epoch 436 | Train Loss 3.313307285308838 | Test Acc 83.46666666666667\n",
      "Epoch 437 | Batch 0/5 | Loss 3.185953\n",
      "Epoch 437 | Test Acc = 89.60% +- 3.50%\n",
      "Epoch 437 | Train Loss 3.363844299316406 | Test Acc 89.6\n",
      "Epoch 438 | Batch 0/5 | Loss 3.258265\n",
      "Epoch 438 | Test Acc = 85.87% +- 6.64%\n",
      "Epoch 438 | Train Loss 3.3480616569519044 | Test Acc 85.86666666666667\n",
      "Epoch 439 | Batch 0/5 | Loss 3.364528\n",
      "Epoch 439 | Test Acc = 87.73% +- 5.50%\n",
      "Epoch 439 | Train Loss 3.251874017715454 | Test Acc 87.73333333333335\n",
      "Epoch 440 | Batch 0/5 | Loss 3.369275\n",
      "Epoch 440 | Test Acc = 95.20% +- 1.75%\n",
      "Epoch 440 | Train Loss 3.250499200820923 | Test Acc 95.2\n",
      "Epoch 441 | Batch 0/5 | Loss 3.125424\n",
      "Epoch 441 | Test Acc = 88.53% +- 4.59%\n",
      "Epoch 441 | Train Loss 3.1690704822540283 | Test Acc 88.53333333333333\n",
      "Epoch 442 | Batch 0/5 | Loss 3.169722\n",
      "Epoch 442 | Test Acc = 81.87% +- 7.77%\n",
      "Epoch 442 | Train Loss 3.152624416351318 | Test Acc 81.86666666666666\n",
      "Epoch 443 | Batch 0/5 | Loss 3.096899\n",
      "Epoch 443 | Test Acc = 84.00% +- 0.74%\n",
      "Epoch 443 | Train Loss 3.3100540161132814 | Test Acc 84.0\n",
      "Epoch 444 | Batch 0/5 | Loss 3.016156\n",
      "Epoch 444 | Test Acc = 85.07% +- 11.17%\n",
      "Epoch 444 | Train Loss 3.237989902496338 | Test Acc 85.06666666666668\n",
      "Epoch 445 | Batch 0/5 | Loss 3.514160\n",
      "Epoch 445 | Test Acc = 88.00% +- 4.43%\n",
      "Epoch 445 | Train Loss 3.2907108783721926 | Test Acc 88.0\n",
      "Epoch 446 | Batch 0/5 | Loss 3.198903\n",
      "Epoch 446 | Test Acc = 90.13% +- 7.30%\n",
      "Epoch 446 | Train Loss 3.2668010711669924 | Test Acc 90.13333333333333\n",
      "Epoch 447 | Batch 0/5 | Loss 3.124109\n",
      "Epoch 447 | Test Acc = 86.13% +- 6.38%\n",
      "Epoch 447 | Train Loss 3.2283501625061035 | Test Acc 86.13333333333334\n",
      "Epoch 448 | Batch 0/5 | Loss 3.381292\n",
      "Epoch 448 | Test Acc = 84.80% +- 1.40%\n",
      "Epoch 448 | Train Loss 3.3640074729919434 | Test Acc 84.80000000000001\n",
      "Epoch 449 | Batch 0/5 | Loss 3.439183\n",
      "Epoch 449 | Test Acc = 86.93% +- 6.59%\n",
      "Epoch 449 | Train Loss 3.16274356842041 | Test Acc 86.93333333333332\n",
      "Epoch 450 | Batch 0/5 | Loss 3.137744\n",
      "Epoch 450 | Test Acc = 86.40% +- 9.53%\n",
      "Epoch 450 | Train Loss 3.2281333446502685 | Test Acc 86.4\n",
      "Epoch 451 | Batch 0/5 | Loss 2.970059\n",
      "Epoch 451 | Test Acc = 91.47% +- 4.53%\n",
      "Epoch 451 | Train Loss 3.175578546524048 | Test Acc 91.46666666666665\n",
      "Epoch 452 | Batch 0/5 | Loss 3.124484\n",
      "Epoch 452 | Test Acc = 82.13% +- 6.25%\n",
      "Epoch 452 | Train Loss 3.140131425857544 | Test Acc 82.13333333333333\n",
      "Epoch 453 | Batch 0/5 | Loss 3.378078\n",
      "Epoch 453 | Test Acc = 85.87% +- 3.51%\n",
      "Epoch 453 | Train Loss 3.309936285018921 | Test Acc 85.86666666666667\n",
      "Epoch 454 | Batch 0/5 | Loss 3.422855\n",
      "Epoch 454 | Test Acc = 82.40% +- 9.03%\n",
      "Epoch 454 | Train Loss 3.2435789585113524 | Test Acc 82.4\n",
      "Epoch 455 | Batch 0/5 | Loss 3.339981\n",
      "Epoch 455 | Test Acc = 83.73% +- 4.40%\n",
      "Epoch 455 | Train Loss 3.1227793216705324 | Test Acc 83.73333333333333\n",
      "Epoch 456 | Batch 0/5 | Loss 3.000594\n",
      "Epoch 456 | Test Acc = 86.40% +- 6.16%\n",
      "Epoch 456 | Train Loss 3.1560688495635985 | Test Acc 86.4\n",
      "Epoch 457 | Batch 0/5 | Loss 3.235350\n",
      "Epoch 457 | Test Acc = 86.67% +- 5.28%\n",
      "Epoch 457 | Train Loss 3.2970598697662354 | Test Acc 86.66666666666666\n",
      "Epoch 458 | Batch 0/5 | Loss 2.862092\n",
      "Epoch 458 | Test Acc = 85.33% +- 5.33%\n",
      "Epoch 458 | Train Loss 3.222162294387817 | Test Acc 85.33333333333334\n",
      "Epoch 459 | Batch 0/5 | Loss 3.391289\n",
      "Epoch 459 | Test Acc = 81.33% +- 2.77%\n",
      "Epoch 459 | Train Loss 3.2616745948791506 | Test Acc 81.33333333333334\n",
      "Epoch 460 | Batch 0/5 | Loss 3.199431\n",
      "Epoch 460 | Test Acc = 72.80% +- 10.18%\n",
      "Epoch 460 | Train Loss 3.20872015953064 | Test Acc 72.8\n",
      "Epoch 461 | Batch 0/5 | Loss 3.171475\n",
      "Epoch 461 | Test Acc = 88.80% +- 3.44%\n",
      "Epoch 461 | Train Loss 3.2499523639678953 | Test Acc 88.79999999999998\n",
      "Epoch 462 | Batch 0/5 | Loss 3.111311\n",
      "Epoch 462 | Test Acc = 80.80% +- 6.96%\n",
      "Epoch 462 | Train Loss 3.2470537662506103 | Test Acc 80.8\n",
      "Epoch 463 | Batch 0/5 | Loss 3.264657\n",
      "Epoch 463 | Test Acc = 92.80% +- 3.35%\n",
      "Epoch 463 | Train Loss 3.1665709972381593 | Test Acc 92.8\n",
      "Epoch 464 | Batch 0/5 | Loss 3.201949\n",
      "Epoch 464 | Test Acc = 89.60% +- 6.25%\n",
      "Epoch 464 | Train Loss 3.13051118850708 | Test Acc 89.6\n",
      "Epoch 465 | Batch 0/5 | Loss 3.309140\n",
      "Epoch 465 | Test Acc = 81.07% +- 5.98%\n",
      "Epoch 465 | Train Loss 3.2168631553649902 | Test Acc 81.06666666666666\n",
      "Epoch 466 | Batch 0/5 | Loss 3.437331\n",
      "Epoch 466 | Test Acc = 86.13% +- 8.32%\n",
      "Epoch 466 | Train Loss 3.2522435188293457 | Test Acc 86.13333333333333\n",
      "Epoch 467 | Batch 0/5 | Loss 3.131778\n",
      "Epoch 467 | Test Acc = 89.07% +- 6.02%\n",
      "Epoch 467 | Train Loss 3.2126049995422363 | Test Acc 89.06666666666666\n",
      "Epoch 468 | Batch 0/5 | Loss 3.140113\n",
      "Epoch 468 | Test Acc = 86.67% +- 4.96%\n",
      "Epoch 468 | Train Loss 3.1964273929595945 | Test Acc 86.66666666666667\n",
      "Epoch 469 | Batch 0/5 | Loss 3.264812\n",
      "Epoch 469 | Test Acc = 83.20% +- 4.02%\n",
      "Epoch 469 | Train Loss 3.179373836517334 | Test Acc 83.2\n",
      "Epoch 470 | Batch 0/5 | Loss 3.190702\n",
      "Epoch 470 | Test Acc = 93.33% +- 3.91%\n",
      "Epoch 470 | Train Loss 3.1161319732666017 | Test Acc 93.33333333333334\n",
      "Epoch 471 | Batch 0/5 | Loss 3.099328\n",
      "Epoch 471 | Test Acc = 89.60% +- 4.21%\n",
      "Epoch 471 | Train Loss 3.128734588623047 | Test Acc 89.6\n",
      "Epoch 472 | Batch 0/5 | Loss 3.534197\n",
      "Epoch 472 | Test Acc = 86.93% +- 3.08%\n",
      "Epoch 472 | Train Loss 3.215590238571167 | Test Acc 86.93333333333334\n",
      "Epoch 473 | Batch 0/5 | Loss 2.969430\n",
      "Epoch 473 | Test Acc = 81.60% +- 8.44%\n",
      "Epoch 473 | Train Loss 3.0981329917907714 | Test Acc 81.6\n",
      "Epoch 474 | Batch 0/5 | Loss 3.145257\n",
      "Epoch 474 | Test Acc = 85.07% +- 7.18%\n",
      "Epoch 474 | Train Loss 3.328646945953369 | Test Acc 85.06666666666666\n",
      "Epoch 475 | Batch 0/5 | Loss 3.198069\n",
      "Epoch 475 | Test Acc = 91.73% +- 5.50%\n",
      "Epoch 475 | Train Loss 3.129327392578125 | Test Acc 91.73333333333333\n",
      "Epoch 476 | Batch 0/5 | Loss 3.317307\n",
      "Epoch 476 | Test Acc = 88.80% +- 4.53%\n",
      "Epoch 476 | Train Loss 3.165319013595581 | Test Acc 88.8\n",
      "Epoch 477 | Batch 0/5 | Loss 3.235703\n",
      "Epoch 477 | Test Acc = 88.53% +- 3.51%\n",
      "Epoch 477 | Train Loss 3.1707828998565675 | Test Acc 88.53333333333333\n",
      "Epoch 478 | Batch 0/5 | Loss 3.024369\n",
      "Epoch 478 | Test Acc = 83.20% +- 8.83%\n",
      "Epoch 478 | Train Loss 3.117892789840698 | Test Acc 83.2\n",
      "Epoch 479 | Batch 0/5 | Loss 3.129351\n",
      "Epoch 479 | Test Acc = 90.93% +- 6.67%\n",
      "Epoch 479 | Train Loss 3.057080125808716 | Test Acc 90.93333333333334\n",
      "Epoch 480 | Batch 0/5 | Loss 3.017679\n",
      "Epoch 480 | Test Acc = 87.47% +- 14.38%\n",
      "Epoch 480 | Train Loss 3.078236484527588 | Test Acc 87.46666666666667\n",
      "Epoch 481 | Batch 0/5 | Loss 3.054533\n",
      "Epoch 481 | Test Acc = 87.73% +- 7.87%\n",
      "Epoch 481 | Train Loss 3.1784712791442873 | Test Acc 87.73333333333332\n",
      "Epoch 482 | Batch 0/5 | Loss 2.922844\n",
      "Epoch 482 | Test Acc = 83.47% +- 7.11%\n",
      "Epoch 482 | Train Loss 3.117672395706177 | Test Acc 83.46666666666667\n",
      "Epoch 483 | Batch 0/5 | Loss 3.112663\n",
      "Epoch 483 | Test Acc = 77.60% +- 14.21%\n",
      "Epoch 483 | Train Loss 3.2671660423278808 | Test Acc 77.6\n",
      "Epoch 484 | Batch 0/5 | Loss 3.064501\n",
      "Epoch 484 | Test Acc = 84.27% +- 7.59%\n",
      "Epoch 484 | Train Loss 3.0347288131713865 | Test Acc 84.26666666666668\n",
      "Epoch 485 | Batch 0/5 | Loss 2.956293\n",
      "Epoch 485 | Test Acc = 86.93% +- 5.50%\n",
      "Epoch 485 | Train Loss 3.0949859619140625 | Test Acc 86.93333333333334\n",
      "Epoch 486 | Batch 0/5 | Loss 3.069860\n",
      "Epoch 486 | Test Acc = 90.13% +- 4.16%\n",
      "Epoch 486 | Train Loss 3.191463327407837 | Test Acc 90.13333333333334\n",
      "Epoch 487 | Batch 0/5 | Loss 3.271524\n",
      "Epoch 487 | Test Acc = 86.67% +- 7.50%\n",
      "Epoch 487 | Train Loss 3.0115659713745115 | Test Acc 86.66666666666667\n",
      "Epoch 488 | Batch 0/5 | Loss 3.043434\n",
      "Epoch 488 | Test Acc = 88.27% +- 7.59%\n",
      "Epoch 488 | Train Loss 3.128491020202637 | Test Acc 88.26666666666668\n",
      "Epoch 489 | Batch 0/5 | Loss 3.019474\n",
      "Epoch 489 | Test Acc = 88.27% +- 5.70%\n",
      "Epoch 489 | Train Loss 3.0835846424102784 | Test Acc 88.26666666666667\n",
      "Epoch 490 | Batch 0/5 | Loss 3.238283\n",
      "Epoch 490 | Test Acc = 85.33% +- 6.49%\n",
      "Epoch 490 | Train Loss 3.132926416397095 | Test Acc 85.33333333333333\n",
      "Epoch 491 | Batch 0/5 | Loss 3.173722\n",
      "Epoch 491 | Test Acc = 85.33% +- 3.70%\n",
      "Epoch 491 | Train Loss 3.1278891563415527 | Test Acc 85.33333333333333\n",
      "Epoch 492 | Batch 0/5 | Loss 3.081980\n",
      "Epoch 492 | Test Acc = 84.00% +- 4.50%\n",
      "Epoch 492 | Train Loss 3.122864007949829 | Test Acc 84.0\n",
      "Epoch 493 | Batch 0/5 | Loss 2.799244\n",
      "Epoch 493 | Test Acc = 88.53% +- 6.21%\n",
      "Epoch 493 | Train Loss 3.0444870948791505 | Test Acc 88.53333333333333\n",
      "Epoch 494 | Batch 0/5 | Loss 3.004804\n",
      "Epoch 494 | Test Acc = 88.27% +- 2.50%\n",
      "Epoch 494 | Train Loss 3.021042251586914 | Test Acc 88.26666666666668\n",
      "Epoch 495 | Batch 0/5 | Loss 3.161865\n",
      "Epoch 495 | Test Acc = 88.00% +- 3.84%\n",
      "Epoch 495 | Train Loss 3.1683573722839355 | Test Acc 87.99999999999999\n",
      "Epoch 496 | Batch 0/5 | Loss 3.187893\n",
      "Epoch 496 | Test Acc = 86.40% +- 5.50%\n",
      "Epoch 496 | Train Loss 3.146089553833008 | Test Acc 86.4\n",
      "Epoch 497 | Batch 0/5 | Loss 2.887119\n",
      "Epoch 497 | Test Acc = 86.93% +- 2.38%\n",
      "Epoch 497 | Train Loss 3.1253698825836183 | Test Acc 86.93333333333334\n",
      "Epoch 498 | Batch 0/5 | Loss 3.046103\n",
      "Epoch 498 | Test Acc = 87.20% +- 5.15%\n",
      "Epoch 498 | Train Loss 3.162695074081421 | Test Acc 87.2\n",
      "Epoch 499 | Batch 0/5 | Loss 2.788951\n",
      "Epoch 499 | Test Acc = 84.53% +- 8.58%\n",
      "Epoch 499 | Train Loss 3.027584171295166 | Test Acc 84.53333333333335\n",
      "Epoch 500 | Batch 0/5 | Loss 3.201063\n",
      "Epoch 500 | Test Acc = 89.60% +- 3.42%\n",
      "Epoch 500 | Train Loss 3.1252497673034667 | Test Acc 89.6\n",
      "Epoch 501 | Batch 0/5 | Loss 3.229663\n",
      "Epoch 501 | Test Acc = 85.33% +- 4.56%\n",
      "Epoch 501 | Train Loss 3.1843752384185793 | Test Acc 85.33333333333334\n",
      "Epoch 502 | Batch 0/5 | Loss 2.896483\n",
      "Epoch 502 | Test Acc = 87.20% +- 4.35%\n",
      "Epoch 502 | Train Loss 3.014796733856201 | Test Acc 87.2\n",
      "Epoch 503 | Batch 0/5 | Loss 3.220311\n",
      "Epoch 503 | Test Acc = 79.47% +- 8.18%\n",
      "Epoch 503 | Train Loss 3.0765867710113524 | Test Acc 79.46666666666667\n",
      "Epoch 504 | Batch 0/5 | Loss 3.064101\n",
      "Epoch 504 | Test Acc = 88.80% +- 3.88%\n",
      "Epoch 504 | Train Loss 3.0901279926300047 | Test Acc 88.8\n",
      "Epoch 505 | Batch 0/5 | Loss 3.128859\n",
      "Epoch 505 | Test Acc = 91.20% +- 5.26%\n",
      "Epoch 505 | Train Loss 3.0395289421081544 | Test Acc 91.2\n",
      "Epoch 506 | Batch 0/5 | Loss 2.988276\n",
      "Epoch 506 | Test Acc = 88.80% +- 8.73%\n",
      "Epoch 506 | Train Loss 3.067564821243286 | Test Acc 88.8\n",
      "Epoch 507 | Batch 0/5 | Loss 2.921276\n",
      "Epoch 507 | Test Acc = 84.27% +- 13.19%\n",
      "Epoch 507 | Train Loss 2.95793194770813 | Test Acc 84.26666666666668\n",
      "Epoch 508 | Batch 0/5 | Loss 3.009508\n",
      "Epoch 508 | Test Acc = 83.47% +- 11.51%\n",
      "Epoch 508 | Train Loss 3.0887448310852053 | Test Acc 83.46666666666667\n",
      "Epoch 509 | Batch 0/5 | Loss 3.474190\n",
      "Epoch 509 | Test Acc = 89.60% +- 6.16%\n",
      "Epoch 509 | Train Loss 3.2381834030151366 | Test Acc 89.60000000000002\n",
      "Epoch 510 | Batch 0/5 | Loss 2.981601\n",
      "Epoch 510 | Test Acc = 90.67% +- 5.58%\n",
      "Epoch 510 | Train Loss 2.985482931137085 | Test Acc 90.66666666666666\n",
      "Epoch 511 | Batch 0/5 | Loss 3.146293\n",
      "Epoch 511 | Test Acc = 88.53% +- 8.18%\n",
      "Epoch 511 | Train Loss 3.0979826927185057 | Test Acc 88.53333333333333\n",
      "Epoch 512 | Batch 0/5 | Loss 2.917669\n",
      "Epoch 512 | Test Acc = 84.53% +- 9.85%\n",
      "Epoch 512 | Train Loss 3.0363385677337646 | Test Acc 84.53333333333333\n",
      "Epoch 513 | Batch 0/5 | Loss 3.427031\n",
      "Epoch 513 | Test Acc = 92.27% +- 4.21%\n",
      "Epoch 513 | Train Loss 3.064240074157715 | Test Acc 92.26666666666667\n",
      "Epoch 514 | Batch 0/5 | Loss 3.013305\n",
      "Epoch 514 | Test Acc = 85.60% +- 5.79%\n",
      "Epoch 514 | Train Loss 3.0306047439575194 | Test Acc 85.6\n",
      "Epoch 515 | Batch 0/5 | Loss 3.065695\n",
      "Epoch 515 | Test Acc = 87.20% +- 3.27%\n",
      "Epoch 515 | Train Loss 3.020286226272583 | Test Acc 87.2\n",
      "Epoch 516 | Batch 0/5 | Loss 3.034926\n",
      "Epoch 516 | Test Acc = 89.33% +- 5.96%\n",
      "Epoch 516 | Train Loss 3.0347306728363037 | Test Acc 89.33333333333334\n",
      "Epoch 517 | Batch 0/5 | Loss 2.768230\n",
      "Epoch 517 | Test Acc = 92.00% +- 3.54%\n",
      "Epoch 517 | Train Loss 2.9747836112976076 | Test Acc 92.0\n",
      "Epoch 518 | Batch 0/5 | Loss 3.189777\n",
      "Epoch 518 | Test Acc = 91.20% +- 3.51%\n",
      "Epoch 518 | Train Loss 3.00335578918457 | Test Acc 91.20000000000002\n",
      "Epoch 519 | Batch 0/5 | Loss 3.015279\n",
      "Epoch 519 | Test Acc = 84.53% +- 6.88%\n",
      "Epoch 519 | Train Loss 3.0253295421600344 | Test Acc 84.53333333333333\n",
      "Epoch 520 | Batch 0/5 | Loss 3.184877\n",
      "Epoch 520 | Test Acc = 81.87% +- 5.15%\n",
      "Epoch 520 | Train Loss 3.0633269786834716 | Test Acc 81.86666666666667\n",
      "Epoch 521 | Batch 0/5 | Loss 3.541493\n",
      "Epoch 521 | Test Acc = 93.33% +- 2.09%\n",
      "Epoch 521 | Train Loss 3.213505744934082 | Test Acc 93.33333333333334\n",
      "Epoch 522 | Batch 0/5 | Loss 3.400284\n",
      "Epoch 522 | Test Acc = 90.40% +- 2.71%\n",
      "Epoch 522 | Train Loss 3.0199073791503905 | Test Acc 90.4\n",
      "Epoch 523 | Batch 0/5 | Loss 3.129601\n",
      "Epoch 523 | Test Acc = 91.20% +- 1.40%\n",
      "Epoch 523 | Train Loss 3.1427390575408936 | Test Acc 91.19999999999999\n",
      "Epoch 524 | Batch 0/5 | Loss 2.985023\n",
      "Epoch 524 | Test Acc = 76.27% +- 9.10%\n",
      "Epoch 524 | Train Loss 2.9921911716461183 | Test Acc 76.26666666666667\n",
      "Epoch 525 | Batch 0/5 | Loss 3.086443\n",
      "Epoch 525 | Test Acc = 93.07% +- 2.01%\n",
      "Epoch 525 | Train Loss 3.0190897941589356 | Test Acc 93.06666666666666\n",
      "Epoch 526 | Batch 0/5 | Loss 2.941023\n",
      "Epoch 526 | Test Acc = 88.00% +- 3.14%\n",
      "Epoch 526 | Train Loss 3.0092693328857423 | Test Acc 88.0\n",
      "Epoch 527 | Batch 0/5 | Loss 2.966496\n",
      "Epoch 527 | Test Acc = 89.87% +- 4.65%\n",
      "Epoch 527 | Train Loss 2.8986650943756103 | Test Acc 89.86666666666667\n",
      "Epoch 528 | Batch 0/5 | Loss 2.993003\n",
      "Epoch 528 | Test Acc = 82.67% +- 5.73%\n",
      "Epoch 528 | Train Loss 2.9900578022003175 | Test Acc 82.66666666666667\n",
      "Epoch 529 | Batch 0/5 | Loss 3.304017\n",
      "Epoch 529 | Test Acc = 86.40% +- 10.28%\n",
      "Epoch 529 | Train Loss 3.0220209121704102 | Test Acc 86.4\n",
      "Epoch 530 | Batch 0/5 | Loss 3.069857\n",
      "Epoch 530 | Test Acc = 84.00% +- 7.39%\n",
      "Epoch 530 | Train Loss 3.065666675567627 | Test Acc 84.0\n",
      "Epoch 531 | Batch 0/5 | Loss 3.400980\n",
      "Epoch 531 | Test Acc = 92.80% +- 3.01%\n",
      "Epoch 531 | Train Loss 3.0070504665374758 | Test Acc 92.8\n",
      "Epoch 532 | Batch 0/5 | Loss 3.178381\n",
      "Epoch 532 | Test Acc = 90.67% +- 4.85%\n",
      "Epoch 532 | Train Loss 3.022628402709961 | Test Acc 90.66666666666667\n",
      "Epoch 533 | Batch 0/5 | Loss 2.809076\n",
      "Epoch 533 | Test Acc = 88.80% +- 4.59%\n",
      "Epoch 533 | Train Loss 2.9326366901397707 | Test Acc 88.80000000000001\n",
      "Epoch 534 | Batch 0/5 | Loss 2.960229\n",
      "Epoch 534 | Test Acc = 89.07% +- 7.18%\n",
      "Epoch 534 | Train Loss 3.082176923751831 | Test Acc 89.06666666666668\n",
      "Epoch 535 | Batch 0/5 | Loss 3.028020\n",
      "Epoch 535 | Test Acc = 86.13% +- 8.48%\n",
      "Epoch 535 | Train Loss 3.0031285762786863 | Test Acc 86.13333333333334\n",
      "Epoch 536 | Batch 0/5 | Loss 2.998335\n",
      "Epoch 536 | Test Acc = 87.73% +- 3.34%\n",
      "Epoch 536 | Train Loss 2.984369897842407 | Test Acc 87.73333333333332\n",
      "Epoch 537 | Batch 0/5 | Loss 2.793298\n",
      "Epoch 537 | Test Acc = 84.00% +- 8.36%\n",
      "Epoch 537 | Train Loss 2.9966608047485352 | Test Acc 84.0\n",
      "Epoch 538 | Batch 0/5 | Loss 3.125318\n",
      "Epoch 538 | Test Acc = 89.60% +- 3.94%\n",
      "Epoch 538 | Train Loss 3.055475091934204 | Test Acc 89.6\n",
      "Epoch 539 | Batch 0/5 | Loss 2.935820\n",
      "Epoch 539 | Test Acc = 83.47% +- 4.71%\n",
      "Epoch 539 | Train Loss 3.2040546894073487 | Test Acc 83.46666666666667\n",
      "Epoch 540 | Batch 0/5 | Loss 2.941102\n",
      "Epoch 540 | Test Acc = 89.07% +- 9.48%\n",
      "Epoch 540 | Train Loss 2.9287437438964843 | Test Acc 89.06666666666668\n",
      "Epoch 541 | Batch 0/5 | Loss 2.908242\n",
      "Epoch 541 | Test Acc = 80.53% +- 7.04%\n",
      "Epoch 541 | Train Loss 2.961188220977783 | Test Acc 80.53333333333333\n",
      "Epoch 542 | Batch 0/5 | Loss 3.039032\n",
      "Epoch 542 | Test Acc = 91.73% +- 6.33%\n",
      "Epoch 542 | Train Loss 2.9488150596618654 | Test Acc 91.73333333333332\n",
      "Epoch 543 | Batch 0/5 | Loss 2.803991\n",
      "Epoch 543 | Test Acc = 90.93% +- 6.75%\n",
      "Epoch 543 | Train Loss 2.8434871673583983 | Test Acc 90.93333333333332\n",
      "Epoch 544 | Batch 0/5 | Loss 3.032483\n",
      "Epoch 544 | Test Acc = 87.20% +- 5.75%\n",
      "Epoch 544 | Train Loss 3.0148733139038084 | Test Acc 87.19999999999999\n",
      "Epoch 545 | Batch 0/5 | Loss 3.129786\n",
      "Epoch 545 | Test Acc = 94.93% +- 1.72%\n",
      "Epoch 545 | Train Loss 3.1124924659729003 | Test Acc 94.93333333333334\n",
      "Epoch 546 | Batch 0/5 | Loss 2.968798\n",
      "Epoch 546 | Test Acc = 82.93% +- 4.27%\n",
      "Epoch 546 | Train Loss 2.9081116676330567 | Test Acc 82.93333333333332\n",
      "Epoch 547 | Batch 0/5 | Loss 3.401369\n",
      "Epoch 547 | Test Acc = 85.87% +- 7.56%\n",
      "Epoch 547 | Train Loss 2.9539702415466307 | Test Acc 85.86666666666667\n",
      "Epoch 548 | Batch 0/5 | Loss 2.938636\n",
      "Epoch 548 | Test Acc = 85.60% +- 5.40%\n",
      "Epoch 548 | Train Loss 2.9079934120178224 | Test Acc 85.6\n",
      "Epoch 549 | Batch 0/5 | Loss 2.949887\n",
      "Epoch 549 | Test Acc = 89.60% +- 3.17%\n",
      "Epoch 549 | Train Loss 2.966072940826416 | Test Acc 89.6\n",
      "Epoch 550 | Batch 0/5 | Loss 3.361535\n",
      "Epoch 550 | Test Acc = 92.00% +- 3.14%\n",
      "Epoch 550 | Train Loss 3.1604968070983888 | Test Acc 92.0\n",
      "Epoch 551 | Batch 0/5 | Loss 3.000935\n",
      "Epoch 551 | Test Acc = 78.13% +- 9.48%\n",
      "Epoch 551 | Train Loss 3.028873252868652 | Test Acc 78.13333333333334\n",
      "Epoch 552 | Batch 0/5 | Loss 3.089418\n",
      "Epoch 552 | Test Acc = 92.53% +- 3.27%\n",
      "Epoch 552 | Train Loss 2.8983959674835207 | Test Acc 92.53333333333333\n",
      "Epoch 553 | Batch 0/5 | Loss 3.203589\n",
      "Epoch 553 | Test Acc = 89.87% +- 3.95%\n",
      "Epoch 553 | Train Loss 3.0589586734771728 | Test Acc 89.86666666666665\n",
      "Epoch 554 | Batch 0/5 | Loss 2.925215\n",
      "Epoch 554 | Test Acc = 84.53% +- 8.58%\n",
      "Epoch 554 | Train Loss 2.922092390060425 | Test Acc 84.53333333333333\n",
      "Epoch 555 | Batch 0/5 | Loss 2.874527\n",
      "Epoch 555 | Test Acc = 94.93% +- 2.90%\n",
      "Epoch 555 | Train Loss 2.9538971424102782 | Test Acc 94.93333333333334\n",
      "Epoch 556 | Batch 0/5 | Loss 2.972257\n",
      "Epoch 556 | Test Acc = 84.80% +- 8.28%\n",
      "Epoch 556 | Train Loss 2.9640491008758545 | Test Acc 84.8\n",
      "Epoch 557 | Batch 0/5 | Loss 2.803424\n",
      "Epoch 557 | Test Acc = 84.53% +- 3.59%\n",
      "Epoch 557 | Train Loss 2.937945604324341 | Test Acc 84.53333333333333\n",
      "Epoch 558 | Batch 0/5 | Loss 3.106664\n",
      "Epoch 558 | Test Acc = 87.73% +- 4.14%\n",
      "Epoch 558 | Train Loss 2.903494644165039 | Test Acc 87.73333333333332\n",
      "Epoch 559 | Batch 0/5 | Loss 2.871139\n",
      "Epoch 559 | Test Acc = 77.07% +- 8.44%\n",
      "Epoch 559 | Train Loss 2.992881727218628 | Test Acc 77.06666666666666\n",
      "Epoch 560 | Batch 0/5 | Loss 3.113317\n",
      "Epoch 560 | Test Acc = 84.27% +- 9.10%\n",
      "Epoch 560 | Train Loss 2.9449285507202148 | Test Acc 84.26666666666665\n",
      "Epoch 561 | Batch 0/5 | Loss 2.979530\n",
      "Epoch 561 | Test Acc = 84.00% +- 9.05%\n",
      "Epoch 561 | Train Loss 3.023098659515381 | Test Acc 84.0\n",
      "Epoch 562 | Batch 0/5 | Loss 2.672264\n",
      "Epoch 562 | Test Acc = 81.87% +- 4.35%\n",
      "Epoch 562 | Train Loss 2.93633828163147 | Test Acc 81.86666666666666\n",
      "Epoch 563 | Batch 0/5 | Loss 2.943481\n",
      "Epoch 563 | Test Acc = 87.20% +- 6.51%\n",
      "Epoch 563 | Train Loss 3.08779559135437 | Test Acc 87.2\n",
      "Epoch 564 | Batch 0/5 | Loss 2.888819\n",
      "Epoch 564 | Test Acc = 91.47% +- 3.51%\n",
      "Epoch 564 | Train Loss 2.906699705123901 | Test Acc 91.46666666666667\n",
      "Epoch 565 | Batch 0/5 | Loss 2.956945\n",
      "Epoch 565 | Test Acc = 86.67% +- 5.12%\n",
      "Epoch 565 | Train Loss 2.9538622379302977 | Test Acc 86.66666666666667\n",
      "Epoch 566 | Batch 0/5 | Loss 2.626354\n",
      "Epoch 566 | Test Acc = 86.93% +- 5.25%\n",
      "Epoch 566 | Train Loss 2.885083818435669 | Test Acc 86.93333333333332\n",
      "Epoch 567 | Batch 0/5 | Loss 3.027320\n",
      "Epoch 567 | Test Acc = 87.47% +- 6.76%\n",
      "Epoch 567 | Train Loss 2.9310836791992188 | Test Acc 87.46666666666667\n",
      "Epoch 568 | Batch 0/5 | Loss 2.809497\n",
      "Epoch 568 | Test Acc = 92.53% +- 4.82%\n",
      "Epoch 568 | Train Loss 2.9734246253967287 | Test Acc 92.53333333333335\n",
      "Epoch 569 | Batch 0/5 | Loss 2.983124\n",
      "Epoch 569 | Test Acc = 85.07% +- 8.01%\n",
      "Epoch 569 | Train Loss 2.889944887161255 | Test Acc 85.06666666666666\n",
      "Epoch 570 | Batch 0/5 | Loss 2.944164\n",
      "Epoch 570 | Test Acc = 85.60% +- 4.14%\n",
      "Epoch 570 | Train Loss 2.965562868118286 | Test Acc 85.6\n",
      "Epoch 571 | Batch 0/5 | Loss 2.843949\n",
      "Epoch 571 | Test Acc = 85.07% +- 5.20%\n",
      "Epoch 571 | Train Loss 2.933156204223633 | Test Acc 85.06666666666666\n",
      "Epoch 572 | Batch 0/5 | Loss 2.893557\n",
      "Epoch 572 | Test Acc = 87.73% +- 7.87%\n",
      "Epoch 572 | Train Loss 3.009437990188599 | Test Acc 87.73333333333333\n",
      "Epoch 573 | Batch 0/5 | Loss 2.828568\n",
      "Epoch 573 | Test Acc = 86.67% +- 7.65%\n",
      "Epoch 573 | Train Loss 2.922392654418945 | Test Acc 86.66666666666667\n",
      "Epoch 574 | Batch 0/5 | Loss 2.759117\n",
      "Epoch 574 | Test Acc = 87.47% +- 7.45%\n",
      "Epoch 574 | Train Loss 2.9551934242248534 | Test Acc 87.46666666666667\n",
      "Epoch 575 | Batch 0/5 | Loss 2.967210\n",
      "Epoch 575 | Test Acc = 84.00% +- 8.13%\n",
      "Epoch 575 | Train Loss 2.8618439197540284 | Test Acc 84.0\n",
      "Epoch 576 | Batch 0/5 | Loss 2.710205\n",
      "Epoch 576 | Test Acc = 84.00% +- 3.70%\n",
      "Epoch 576 | Train Loss 2.8928118705749513 | Test Acc 84.0\n",
      "Epoch 577 | Batch 0/5 | Loss 2.659132\n",
      "Epoch 577 | Test Acc = 81.33% +- 4.12%\n",
      "Epoch 577 | Train Loss 2.8976393699645997 | Test Acc 81.33333333333333\n",
      "Epoch 578 | Batch 0/5 | Loss 2.724370\n",
      "Epoch 578 | Test Acc = 88.00% +- 1.65%\n",
      "Epoch 578 | Train Loss 2.835522985458374 | Test Acc 88.0\n",
      "Epoch 579 | Batch 0/5 | Loss 2.751169\n",
      "Epoch 579 | Test Acc = 88.27% +- 6.59%\n",
      "Epoch 579 | Train Loss 2.8252992153167726 | Test Acc 88.26666666666667\n",
      "Epoch 580 | Batch 0/5 | Loss 3.053119\n",
      "Epoch 580 | Test Acc = 90.40% +- 4.98%\n",
      "Epoch 580 | Train Loss 2.927508306503296 | Test Acc 90.4\n",
      "Epoch 581 | Batch 0/5 | Loss 2.699572\n",
      "Epoch 581 | Test Acc = 91.47% +- 4.59%\n",
      "Epoch 581 | Train Loss 2.9136690139770507 | Test Acc 91.46666666666667\n",
      "Epoch 582 | Batch 0/5 | Loss 2.649774\n",
      "Epoch 582 | Test Acc = 84.53% +- 6.88%\n",
      "Epoch 582 | Train Loss 2.846723175048828 | Test Acc 84.53333333333333\n",
      "Epoch 583 | Batch 0/5 | Loss 3.345538\n",
      "Epoch 583 | Test Acc = 89.33% +- 5.07%\n",
      "Epoch 583 | Train Loss 3.0515525341033936 | Test Acc 89.33333333333334\n",
      "Epoch 584 | Batch 0/5 | Loss 2.967776\n",
      "Epoch 584 | Test Acc = 88.00% +- 6.10%\n",
      "Epoch 584 | Train Loss 2.902360963821411 | Test Acc 88.0\n",
      "Epoch 585 | Batch 0/5 | Loss 3.126320\n",
      "Epoch 585 | Test Acc = 89.60% +- 7.52%\n",
      "Epoch 585 | Train Loss 2.9143961906433105 | Test Acc 89.6\n",
      "Epoch 586 | Batch 0/5 | Loss 2.985075\n",
      "Epoch 586 | Test Acc = 88.27% +- 6.38%\n",
      "Epoch 586 | Train Loss 2.866511344909668 | Test Acc 88.26666666666668\n",
      "Epoch 587 | Batch 0/5 | Loss 2.840226\n",
      "Epoch 587 | Test Acc = 94.40% +- 3.50%\n",
      "Epoch 587 | Train Loss 2.809528112411499 | Test Acc 94.39999999999999\n",
      "Epoch 588 | Batch 0/5 | Loss 2.849422\n",
      "Epoch 588 | Test Acc = 85.87% +- 12.02%\n",
      "Epoch 588 | Train Loss 2.9508470058441163 | Test Acc 85.86666666666667\n",
      "Epoch 589 | Batch 0/5 | Loss 3.041867\n",
      "Epoch 589 | Test Acc = 88.00% +- 6.27%\n",
      "Epoch 589 | Train Loss 2.8684813022613525 | Test Acc 88.0\n",
      "Epoch 590 | Batch 0/5 | Loss 2.887843\n",
      "Epoch 590 | Test Acc = 87.20% +- 8.18%\n",
      "Epoch 590 | Train Loss 2.8815215110778807 | Test Acc 87.2\n",
      "Epoch 591 | Batch 0/5 | Loss 2.910952\n",
      "Epoch 591 | Test Acc = 89.60% +- 7.07%\n",
      "Epoch 591 | Train Loss 2.8373427867889403 | Test Acc 89.6\n",
      "Epoch 592 | Batch 0/5 | Loss 2.748494\n",
      "Epoch 592 | Test Acc = 92.80% +- 2.82%\n",
      "Epoch 592 | Train Loss 2.8691285133361815 | Test Acc 92.8\n",
      "Epoch 593 | Batch 0/5 | Loss 2.756540\n",
      "Epoch 593 | Test Acc = 90.67% +- 7.43%\n",
      "Epoch 593 | Train Loss 2.908418130874634 | Test Acc 90.66666666666666\n",
      "Epoch 594 | Batch 0/5 | Loss 2.751906\n",
      "Epoch 594 | Test Acc = 90.93% +- 1.55%\n",
      "Epoch 594 | Train Loss 2.8726001262664793 | Test Acc 90.93333333333332\n",
      "Epoch 595 | Batch 0/5 | Loss 2.782206\n",
      "Epoch 595 | Test Acc = 84.53% +- 2.41%\n",
      "Epoch 595 | Train Loss 2.9379355907440186 | Test Acc 84.53333333333333\n",
      "Epoch 596 | Batch 0/5 | Loss 2.660728\n",
      "Epoch 596 | Test Acc = 87.20% +- 4.82%\n",
      "Epoch 596 | Train Loss 2.8693780422210695 | Test Acc 87.2\n",
      "Epoch 597 | Batch 0/5 | Loss 3.158686\n",
      "Epoch 597 | Test Acc = 88.80% +- 7.91%\n",
      "Epoch 597 | Train Loss 2.9753896236419677 | Test Acc 88.8\n",
      "Epoch 598 | Batch 0/5 | Loss 2.980810\n",
      "Epoch 598 | Test Acc = 84.80% +- 5.94%\n",
      "Epoch 598 | Train Loss 2.8464409828186037 | Test Acc 84.8\n",
      "Epoch 599 | Batch 0/5 | Loss 2.978061\n",
      "Epoch 599 | Test Acc = 85.33% +- 5.33%\n",
      "Epoch 599 | Train Loss 2.889856290817261 | Test Acc 85.33333333333334\n",
      "Epoch 600 | Batch 0/5 | Loss 2.677124\n",
      "Epoch 600 | Test Acc = 88.00% +- 2.86%\n",
      "Epoch 600 | Train Loss 2.7903984069824217 | Test Acc 88.0\n",
      "Epoch 601 | Batch 0/5 | Loss 2.867481\n",
      "Epoch 601 | Test Acc = 89.33% +- 1.28%\n",
      "Epoch 601 | Train Loss 2.863192558288574 | Test Acc 89.33333333333333\n",
      "Epoch 602 | Batch 0/5 | Loss 2.783478\n",
      "Epoch 602 | Test Acc = 86.67% +- 9.69%\n",
      "Epoch 602 | Train Loss 2.8069712638854982 | Test Acc 86.66666666666667\n",
      "Epoch 603 | Batch 0/5 | Loss 2.797502\n",
      "Epoch 603 | Test Acc = 81.33% +- 6.40%\n",
      "Epoch 603 | Train Loss 2.8636157512664795 | Test Acc 81.33333333333333\n",
      "Epoch 604 | Batch 0/5 | Loss 2.727046\n",
      "Epoch 604 | Test Acc = 89.60% +- 2.90%\n",
      "Epoch 604 | Train Loss 2.8032636642456055 | Test Acc 89.6\n",
      "Epoch 605 | Batch 0/5 | Loss 2.550788\n",
      "Epoch 605 | Test Acc = 89.87% +- 3.19%\n",
      "Epoch 605 | Train Loss 2.781954526901245 | Test Acc 89.86666666666666\n",
      "Epoch 606 | Batch 0/5 | Loss 2.807584\n",
      "Epoch 606 | Test Acc = 93.07% +- 1.55%\n",
      "Epoch 606 | Train Loss 2.7975876331329346 | Test Acc 93.06666666666666\n",
      "Epoch 607 | Batch 0/5 | Loss 2.802730\n",
      "Epoch 607 | Test Acc = 73.33% +- 6.97%\n",
      "Epoch 607 | Train Loss 2.8958248615264894 | Test Acc 73.33333333333333\n",
      "Epoch 608 | Batch 0/5 | Loss 2.907435\n",
      "Epoch 608 | Test Acc = 82.93% +- 9.62%\n",
      "Epoch 608 | Train Loss 2.9252685546875 | Test Acc 82.93333333333334\n",
      "Epoch 609 | Batch 0/5 | Loss 2.913341\n",
      "Epoch 609 | Test Acc = 89.07% +- 5.84%\n",
      "Epoch 609 | Train Loss 2.931122875213623 | Test Acc 89.06666666666666\n",
      "Epoch 610 | Batch 0/5 | Loss 2.715322\n",
      "Epoch 610 | Test Acc = 89.60% +- 2.50%\n",
      "Epoch 610 | Train Loss 2.8466707706451415 | Test Acc 89.6\n",
      "Epoch 611 | Batch 0/5 | Loss 2.931059\n",
      "Epoch 611 | Test Acc = 85.87% +- 8.54%\n",
      "Epoch 611 | Train Loss 2.9375089168548585 | Test Acc 85.86666666666665\n",
      "Epoch 612 | Batch 0/5 | Loss 2.722399\n",
      "Epoch 612 | Test Acc = 78.93% +- 8.97%\n",
      "Epoch 612 | Train Loss 2.8873233795166016 | Test Acc 78.93333333333332\n",
      "Epoch 613 | Batch 0/5 | Loss 2.586853\n",
      "Epoch 613 | Test Acc = 90.13% +- 2.73%\n",
      "Epoch 613 | Train Loss 2.7366257190704344 | Test Acc 90.13333333333334\n",
      "Epoch 614 | Batch 0/5 | Loss 2.640888\n",
      "Epoch 614 | Test Acc = 90.13% +- 4.71%\n",
      "Epoch 614 | Train Loss 2.8198058128356935 | Test Acc 90.13333333333333\n",
      "Epoch 615 | Batch 0/5 | Loss 2.757427\n",
      "Epoch 615 | Test Acc = 89.87% +- 8.70%\n",
      "Epoch 615 | Train Loss 2.728118896484375 | Test Acc 89.86666666666665\n",
      "Epoch 616 | Batch 0/5 | Loss 3.025610\n",
      "Epoch 616 | Test Acc = 88.80% +- 6.12%\n",
      "Epoch 616 | Train Loss 2.912882900238037 | Test Acc 88.8\n",
      "Epoch 617 | Batch 0/5 | Loss 3.179657\n",
      "Epoch 617 | Test Acc = 90.93% +- 3.94%\n",
      "Epoch 617 | Train Loss 2.8881028652191163 | Test Acc 90.93333333333334\n",
      "Epoch 618 | Batch 0/5 | Loss 2.801303\n",
      "Epoch 618 | Test Acc = 88.53% +- 4.35%\n",
      "Epoch 618 | Train Loss 2.7570685386657714 | Test Acc 88.53333333333333\n",
      "Epoch 619 | Batch 0/5 | Loss 2.637634\n",
      "Epoch 619 | Test Acc = 93.33% +- 2.09%\n",
      "Epoch 619 | Train Loss 2.81041898727417 | Test Acc 93.33333333333333\n",
      "Epoch 620 | Batch 0/5 | Loss 2.789139\n",
      "Epoch 620 | Test Acc = 92.53% +- 3.01%\n",
      "Epoch 620 | Train Loss 2.882724237442017 | Test Acc 92.53333333333333\n",
      "Epoch 621 | Batch 0/5 | Loss 2.885461\n",
      "Epoch 621 | Test Acc = 77.60% +- 12.24%\n",
      "Epoch 621 | Train Loss 2.785674285888672 | Test Acc 77.6\n",
      "Epoch 622 | Batch 0/5 | Loss 2.716495\n",
      "Epoch 622 | Test Acc = 97.33% +- 2.67%\n",
      "Epoch 622 | Train Loss 2.820812940597534 | Test Acc 97.33333333333334\n",
      "Epoch 623 | Batch 0/5 | Loss 2.766503\n",
      "Epoch 623 | Test Acc = 91.47% +- 6.03%\n",
      "Epoch 623 | Train Loss 2.728006935119629 | Test Acc 91.46666666666667\n",
      "Epoch 624 | Batch 0/5 | Loss 2.747836\n",
      "Epoch 624 | Test Acc = 90.40% +- 2.27%\n",
      "Epoch 624 | Train Loss 2.8470044136047363 | Test Acc 90.4\n",
      "Epoch 625 | Batch 0/5 | Loss 2.529698\n",
      "Epoch 625 | Test Acc = 88.53% +- 4.82%\n",
      "Epoch 625 | Train Loss 2.712706279754639 | Test Acc 88.53333333333333\n",
      "Epoch 626 | Batch 0/5 | Loss 2.726571\n",
      "Epoch 626 | Test Acc = 84.27% +- 14.24%\n",
      "Epoch 626 | Train Loss 2.752750205993652 | Test Acc 84.26666666666668\n",
      "Epoch 627 | Batch 0/5 | Loss 2.882646\n",
      "Epoch 627 | Test Acc = 88.80% +- 6.80%\n",
      "Epoch 627 | Train Loss 2.8127874374389648 | Test Acc 88.80000000000001\n",
      "Epoch 628 | Batch 0/5 | Loss 2.764575\n",
      "Epoch 628 | Test Acc = 89.07% +- 7.84%\n",
      "Epoch 628 | Train Loss 2.777490758895874 | Test Acc 89.06666666666668\n",
      "Epoch 629 | Batch 0/5 | Loss 2.878033\n",
      "Epoch 629 | Test Acc = 86.93% +- 3.73%\n",
      "Epoch 629 | Train Loss 2.8229440212249757 | Test Acc 86.93333333333334\n",
      "Epoch 630 | Batch 0/5 | Loss 2.873535\n",
      "Epoch 630 | Test Acc = 87.47% +- 9.04%\n",
      "Epoch 630 | Train Loss 2.9091251850128175 | Test Acc 87.46666666666667\n",
      "Epoch 631 | Batch 0/5 | Loss 2.621511\n",
      "Epoch 631 | Test Acc = 81.87% +- 7.74%\n",
      "Epoch 631 | Train Loss 2.7933040618896485 | Test Acc 81.86666666666667\n",
      "Epoch 632 | Batch 0/5 | Loss 2.755648\n",
      "Epoch 632 | Test Acc = 84.53% +- 5.31%\n",
      "Epoch 632 | Train Loss 2.8457273483276366 | Test Acc 84.53333333333333\n",
      "Epoch 633 | Batch 0/5 | Loss 2.822619\n",
      "Epoch 633 | Test Acc = 89.60% +- 5.40%\n",
      "Epoch 633 | Train Loss 2.727867937088013 | Test Acc 89.6\n",
      "Epoch 634 | Batch 0/5 | Loss 2.535177\n",
      "Epoch 634 | Test Acc = 92.00% +- 6.49%\n",
      "Epoch 634 | Train Loss 2.7889848232269285 | Test Acc 92.0\n",
      "Epoch 635 | Batch 0/5 | Loss 2.771806\n",
      "Epoch 635 | Test Acc = 84.80% +- 1.90%\n",
      "Epoch 635 | Train Loss 2.8306503295898438 | Test Acc 84.8\n",
      "Epoch 636 | Batch 0/5 | Loss 2.699421\n",
      "Epoch 636 | Test Acc = 90.13% +- 3.35%\n",
      "Epoch 636 | Train Loss 2.7439009666442873 | Test Acc 90.13333333333333\n",
      "Epoch 637 | Batch 0/5 | Loss 2.974273\n",
      "Epoch 637 | Test Acc = 85.07% +- 7.52%\n",
      "Epoch 637 | Train Loss 2.835415506362915 | Test Acc 85.06666666666666\n",
      "Epoch 638 | Batch 0/5 | Loss 2.696889\n",
      "Epoch 638 | Test Acc = 87.20% +- 7.11%\n",
      "Epoch 638 | Train Loss 2.701238250732422 | Test Acc 87.2\n",
      "Epoch 639 | Batch 0/5 | Loss 2.856128\n",
      "Epoch 639 | Test Acc = 89.87% +- 8.41%\n",
      "Epoch 639 | Train Loss 2.872700881958008 | Test Acc 89.86666666666667\n",
      "Epoch 640 | Batch 0/5 | Loss 2.667378\n",
      "Epoch 640 | Test Acc = 84.00% +- 7.43%\n",
      "Epoch 640 | Train Loss 2.609022283554077 | Test Acc 84.00000000000001\n",
      "Epoch 641 | Batch 0/5 | Loss 2.569855\n",
      "Epoch 641 | Test Acc = 88.00% +- 5.33%\n",
      "Epoch 641 | Train Loss 2.638175439834595 | Test Acc 88.0\n",
      "Epoch 642 | Batch 0/5 | Loss 2.712202\n",
      "Epoch 642 | Test Acc = 79.47% +- 13.80%\n",
      "Epoch 642 | Train Loss 2.7187429904937743 | Test Acc 79.46666666666665\n",
      "Epoch 643 | Batch 0/5 | Loss 2.730820\n",
      "Epoch 643 | Test Acc = 78.93% +- 8.18%\n",
      "Epoch 643 | Train Loss 2.7364277839660645 | Test Acc 78.93333333333335\n",
      "Epoch 644 | Batch 0/5 | Loss 2.627730\n",
      "Epoch 644 | Test Acc = 88.53% +- 4.65%\n",
      "Epoch 644 | Train Loss 2.7477667331695557 | Test Acc 88.53333333333335\n",
      "Epoch 645 | Batch 0/5 | Loss 2.682276\n",
      "Epoch 645 | Test Acc = 87.20% +- 7.70%\n",
      "Epoch 645 | Train Loss 2.8771335601806642 | Test Acc 87.2\n",
      "Epoch 646 | Batch 0/5 | Loss 2.804831\n",
      "Epoch 646 | Test Acc = 83.20% +- 8.64%\n",
      "Epoch 646 | Train Loss 2.680419445037842 | Test Acc 83.2\n",
      "Epoch 647 | Batch 0/5 | Loss 2.439372\n",
      "Epoch 647 | Test Acc = 87.20% +- 3.35%\n",
      "Epoch 647 | Train Loss 2.620008182525635 | Test Acc 87.2\n",
      "Epoch 648 | Batch 0/5 | Loss 2.520931\n",
      "Epoch 648 | Test Acc = 90.93% +- 4.46%\n",
      "Epoch 648 | Train Loss 2.733103799819946 | Test Acc 90.93333333333334\n",
      "Epoch 649 | Batch 0/5 | Loss 2.786664\n",
      "Epoch 649 | Test Acc = 88.27% +- 2.01%\n",
      "Epoch 649 | Train Loss 2.6700082778930665 | Test Acc 88.26666666666667\n",
      "Epoch 650 | Batch 0/5 | Loss 2.728235\n",
      "Epoch 650 | Test Acc = 93.87% +- 7.95%\n",
      "Epoch 650 | Train Loss 2.7538967609405516 | Test Acc 93.86666666666667\n",
      "Epoch 651 | Batch 0/5 | Loss 2.765332\n",
      "Epoch 651 | Test Acc = 90.13% +- 4.09%\n",
      "Epoch 651 | Train Loss 2.7593589305877684 | Test Acc 90.13333333333334\n",
      "Epoch 652 | Batch 0/5 | Loss 2.537194\n",
      "Epoch 652 | Test Acc = 85.60% +- 9.24%\n",
      "Epoch 652 | Train Loss 2.7537839889526365 | Test Acc 85.6\n",
      "Epoch 653 | Batch 0/5 | Loss 2.705611\n",
      "Epoch 653 | Test Acc = 89.07% +- 2.80%\n",
      "Epoch 653 | Train Loss 2.780147409439087 | Test Acc 89.06666666666666\n",
      "Epoch 654 | Batch 0/5 | Loss 2.727812\n",
      "Epoch 654 | Test Acc = 83.20% +- 8.51%\n",
      "Epoch 654 | Train Loss 2.7924290180206297 | Test Acc 83.20000000000002\n",
      "Epoch 655 | Batch 0/5 | Loss 2.735315\n",
      "Epoch 655 | Test Acc = 91.20% +- 4.22%\n",
      "Epoch 655 | Train Loss 2.6996178150177004 | Test Acc 91.2\n",
      "Epoch 656 | Batch 0/5 | Loss 2.534447\n",
      "Epoch 656 | Test Acc = 91.47% +- 1.59%\n",
      "Epoch 656 | Train Loss 2.6263787269592287 | Test Acc 91.46666666666667\n",
      "Epoch 657 | Batch 0/5 | Loss 2.812701\n",
      "Epoch 657 | Test Acc = 82.93% +- 9.87%\n",
      "Epoch 657 | Train Loss 2.7651750564575197 | Test Acc 82.93333333333334\n",
      "Epoch 658 | Batch 0/5 | Loss 2.596193\n",
      "Epoch 658 | Test Acc = 88.00% +- 2.67%\n",
      "Epoch 658 | Train Loss 2.6940049648284914 | Test Acc 88.0\n",
      "Epoch 659 | Batch 0/5 | Loss 2.654300\n",
      "Epoch 659 | Test Acc = 88.53% +- 5.26%\n",
      "Epoch 659 | Train Loss 2.72275390625 | Test Acc 88.53333333333335\n",
      "Epoch 660 | Batch 0/5 | Loss 2.673391\n",
      "Epoch 660 | Test Acc = 87.20% +- 8.76%\n",
      "Epoch 660 | Train Loss 2.7220151901245115 | Test Acc 87.2\n",
      "Epoch 661 | Batch 0/5 | Loss 2.898848\n",
      "Epoch 661 | Test Acc = 91.47% +- 9.25%\n",
      "Epoch 661 | Train Loss 2.831518316268921 | Test Acc 91.46666666666667\n",
      "Epoch 662 | Batch 0/5 | Loss 2.704055\n",
      "Epoch 662 | Test Acc = 88.27% +- 5.98%\n",
      "Epoch 662 | Train Loss 2.729056406021118 | Test Acc 88.26666666666668\n",
      "Epoch 663 | Batch 0/5 | Loss 2.777900\n",
      "Epoch 663 | Test Acc = 85.07% +- 7.94%\n",
      "Epoch 663 | Train Loss 2.7398520946502685 | Test Acc 85.06666666666668\n",
      "Epoch 664 | Batch 0/5 | Loss 2.651045\n",
      "Epoch 664 | Test Acc = 94.13% +- 3.95%\n",
      "Epoch 664 | Train Loss 2.774648571014404 | Test Acc 94.13333333333334\n",
      "Epoch 665 | Batch 0/5 | Loss 2.782499\n",
      "Epoch 665 | Test Acc = 91.73% +- 2.01%\n",
      "Epoch 665 | Train Loss 2.795591402053833 | Test Acc 91.73333333333332\n",
      "Epoch 666 | Batch 0/5 | Loss 2.805722\n",
      "Epoch 666 | Test Acc = 87.20% +- 10.52%\n",
      "Epoch 666 | Train Loss 2.7099819660186766 | Test Acc 87.2\n",
      "Epoch 667 | Batch 0/5 | Loss 2.952699\n",
      "Epoch 667 | Test Acc = 88.27% +- 3.87%\n",
      "Epoch 667 | Train Loss 2.7889573097229006 | Test Acc 88.26666666666668\n",
      "Epoch 668 | Batch 0/5 | Loss 2.924462\n",
      "Epoch 668 | Test Acc = 82.13% +- 7.04%\n",
      "Epoch 668 | Train Loss 2.7480777740478515 | Test Acc 82.13333333333335\n",
      "Epoch 669 | Batch 0/5 | Loss 2.765677\n",
      "Epoch 669 | Test Acc = 90.13% +- 3.88%\n",
      "Epoch 669 | Train Loss 2.683125305175781 | Test Acc 90.13333333333334\n",
      "Epoch 670 | Batch 0/5 | Loss 2.620723\n",
      "Epoch 670 | Test Acc = 87.73% +- 9.42%\n",
      "Epoch 670 | Train Loss 2.7449151039123536 | Test Acc 87.73333333333332\n",
      "Epoch 671 | Batch 0/5 | Loss 2.583685\n",
      "Epoch 671 | Test Acc = 90.67% +- 5.43%\n",
      "Epoch 671 | Train Loss 2.585733985900879 | Test Acc 90.66666666666666\n",
      "Epoch 672 | Batch 0/5 | Loss 2.801344\n",
      "Epoch 672 | Test Acc = 89.07% +- 2.80%\n",
      "Epoch 672 | Train Loss 2.680549907684326 | Test Acc 89.06666666666666\n",
      "Epoch 673 | Batch 0/5 | Loss 2.563729\n",
      "Epoch 673 | Test Acc = 87.20% +- 4.22%\n",
      "Epoch 673 | Train Loss 2.631551504135132 | Test Acc 87.20000000000002\n",
      "Epoch 674 | Batch 0/5 | Loss 2.771091\n",
      "Epoch 674 | Test Acc = 85.33% +- 5.91%\n",
      "Epoch 674 | Train Loss 2.651109743118286 | Test Acc 85.33333333333333\n",
      "Epoch 675 | Batch 0/5 | Loss 2.512575\n",
      "Epoch 675 | Test Acc = 91.73% +- 4.76%\n",
      "Epoch 675 | Train Loss 2.7513789176940917 | Test Acc 91.73333333333332\n",
      "Epoch 676 | Batch 0/5 | Loss 2.664884\n",
      "Epoch 676 | Test Acc = 93.60% +- 3.26%\n",
      "Epoch 676 | Train Loss 2.8349235534667967 | Test Acc 93.6\n",
      "Epoch 677 | Batch 0/5 | Loss 2.596223\n",
      "Epoch 677 | Test Acc = 91.20% +- 2.17%\n",
      "Epoch 677 | Train Loss 2.623619222640991 | Test Acc 91.19999999999999\n",
      "Epoch 678 | Batch 0/5 | Loss 2.775309\n",
      "Epoch 678 | Test Acc = 90.40% +- 4.34%\n",
      "Epoch 678 | Train Loss 2.8896368026733397 | Test Acc 90.4\n",
      "Epoch 679 | Batch 0/5 | Loss 2.740474\n",
      "Epoch 679 | Test Acc = 89.33% +- 7.50%\n",
      "Epoch 679 | Train Loss 2.7484981060028075 | Test Acc 89.33333333333334\n",
      "Epoch 680 | Batch 0/5 | Loss 2.721003\n",
      "Epoch 680 | Test Acc = 88.53% +- 7.41%\n",
      "Epoch 680 | Train Loss 2.866079568862915 | Test Acc 88.53333333333333\n",
      "Epoch 681 | Batch 0/5 | Loss 2.686056\n",
      "Epoch 681 | Test Acc = 84.53% +- 5.99%\n",
      "Epoch 681 | Train Loss 2.7416053771972657 | Test Acc 84.53333333333333\n",
      "Epoch 682 | Batch 0/5 | Loss 2.933265\n",
      "Epoch 682 | Test Acc = 89.60% +- 6.59%\n",
      "Epoch 682 | Train Loss 2.8184649467468263 | Test Acc 89.6\n",
      "Epoch 683 | Batch 0/5 | Loss 2.572106\n",
      "Epoch 683 | Test Acc = 88.80% +- 6.21%\n",
      "Epoch 683 | Train Loss 2.6867438316345216 | Test Acc 88.8\n",
      "Epoch 684 | Batch 0/5 | Loss 2.706630\n",
      "Epoch 684 | Test Acc = 79.20% +- 6.38%\n",
      "Epoch 684 | Train Loss 2.660518217086792 | Test Acc 79.2\n",
      "Epoch 685 | Batch 0/5 | Loss 2.765788\n",
      "Epoch 685 | Test Acc = 77.07% +- 8.66%\n",
      "Epoch 685 | Train Loss 2.7378009796142577 | Test Acc 77.06666666666665\n",
      "Epoch 686 | Batch 0/5 | Loss 2.849973\n",
      "Epoch 686 | Test Acc = 90.40% +- 4.87%\n",
      "Epoch 686 | Train Loss 2.676904249191284 | Test Acc 90.4\n",
      "Epoch 687 | Batch 0/5 | Loss 2.597098\n",
      "Epoch 687 | Test Acc = 83.73% +- 5.45%\n",
      "Epoch 687 | Train Loss 2.639211082458496 | Test Acc 83.73333333333332\n",
      "Epoch 688 | Batch 0/5 | Loss 2.619908\n",
      "Epoch 688 | Test Acc = 91.20% +- 3.51%\n",
      "Epoch 688 | Train Loss 2.6848066329956053 | Test Acc 91.2\n",
      "Epoch 689 | Batch 0/5 | Loss 2.715750\n",
      "Epoch 689 | Test Acc = 91.47% +- 3.81%\n",
      "Epoch 689 | Train Loss 2.66924786567688 | Test Acc 91.46666666666667\n",
      "Epoch 690 | Batch 0/5 | Loss 2.639051\n",
      "Epoch 690 | Test Acc = 87.47% +- 4.09%\n",
      "Epoch 690 | Train Loss 2.691807413101196 | Test Acc 87.46666666666667\n",
      "Epoch 691 | Batch 0/5 | Loss 2.653520\n",
      "Epoch 691 | Test Acc = 83.47% +- 5.61%\n",
      "Epoch 691 | Train Loss 2.71785626411438 | Test Acc 83.46666666666667\n",
      "Epoch 692 | Batch 0/5 | Loss 2.769633\n",
      "Epoch 692 | Test Acc = 89.60% +- 5.55%\n",
      "Epoch 692 | Train Loss 2.684623289108276 | Test Acc 89.6\n",
      "Epoch 693 | Batch 0/5 | Loss 2.682122\n",
      "Epoch 693 | Test Acc = 88.00% +- 5.33%\n",
      "Epoch 693 | Train Loss 2.594046401977539 | Test Acc 88.0\n",
      "Epoch 694 | Batch 0/5 | Loss 2.594131\n",
      "Epoch 694 | Test Acc = 90.40% +- 3.26%\n",
      "Epoch 694 | Train Loss 2.718250036239624 | Test Acc 90.4\n",
      "Epoch 695 | Batch 0/5 | Loss 2.826192\n",
      "Epoch 695 | Test Acc = 87.47% +- 9.37%\n",
      "Epoch 695 | Train Loss 2.7224090099334717 | Test Acc 87.46666666666665\n",
      "Epoch 696 | Batch 0/5 | Loss 2.866104\n",
      "Epoch 696 | Test Acc = 89.33% +- 6.10%\n",
      "Epoch 696 | Train Loss 2.6869171142578123 | Test Acc 89.33333333333334\n",
      "Epoch 697 | Batch 0/5 | Loss 2.781107\n",
      "Epoch 697 | Test Acc = 93.07% +- 3.80%\n",
      "Epoch 697 | Train Loss 2.624519968032837 | Test Acc 93.06666666666666\n",
      "Epoch 698 | Batch 0/5 | Loss 2.653471\n",
      "Epoch 698 | Test Acc = 86.93% +- 8.44%\n",
      "Epoch 698 | Train Loss 2.649769687652588 | Test Acc 86.93333333333334\n",
      "Epoch 699 | Batch 0/5 | Loss 2.590819\n",
      "Epoch 699 | Test Acc = 88.53% +- 7.45%\n",
      "Epoch 699 | Train Loss 2.648801326751709 | Test Acc 88.53333333333333\n",
      "Epoch 700 | Batch 0/5 | Loss 2.548861\n",
      "Epoch 700 | Test Acc = 86.40% +- 6.54%\n",
      "Epoch 700 | Train Loss 2.7760639667510985 | Test Acc 86.39999999999999\n",
      "Epoch 701 | Batch 0/5 | Loss 2.718988\n",
      "Epoch 701 | Test Acc = 92.53% +- 4.35%\n",
      "Epoch 701 | Train Loss 2.7392319679260253 | Test Acc 92.53333333333333\n",
      "Epoch 702 | Batch 0/5 | Loss 2.381464\n",
      "Epoch 702 | Test Acc = 84.53% +- 6.21%\n",
      "Epoch 702 | Train Loss 2.7144124507904053 | Test Acc 84.53333333333333\n",
      "Epoch 703 | Batch 0/5 | Loss 2.795031\n",
      "Epoch 703 | Test Acc = 90.67% +- 3.77%\n",
      "Epoch 703 | Train Loss 2.658115863800049 | Test Acc 90.66666666666667\n",
      "Epoch 704 | Batch 0/5 | Loss 2.612970\n",
      "Epoch 704 | Test Acc = 89.60% +- 4.27%\n",
      "Epoch 704 | Train Loss 2.6988521575927735 | Test Acc 89.6\n",
      "Epoch 705 | Batch 0/5 | Loss 2.647452\n",
      "Epoch 705 | Test Acc = 84.80% +- 10.83%\n",
      "Epoch 705 | Train Loss 2.5974995613098146 | Test Acc 84.8\n",
      "Epoch 706 | Batch 0/5 | Loss 2.707627\n",
      "Epoch 706 | Test Acc = 82.93% +- 10.95%\n",
      "Epoch 706 | Train Loss 2.72798113822937 | Test Acc 82.93333333333334\n",
      "Epoch 707 | Batch 0/5 | Loss 2.809254\n",
      "Epoch 707 | Test Acc = 90.40% +- 9.42%\n",
      "Epoch 707 | Train Loss 2.7141379833221437 | Test Acc 90.4\n",
      "Epoch 708 | Batch 0/5 | Loss 2.470533\n",
      "Epoch 708 | Test Acc = 88.27% +- 4.27%\n",
      "Epoch 708 | Train Loss 2.694107723236084 | Test Acc 88.26666666666668\n",
      "Epoch 709 | Batch 0/5 | Loss 2.660860\n",
      "Epoch 709 | Test Acc = 86.40% +- 6.67%\n",
      "Epoch 709 | Train Loss 2.6648940563201906 | Test Acc 86.4\n",
      "Epoch 710 | Batch 0/5 | Loss 2.741251\n",
      "Epoch 710 | Test Acc = 92.27% +- 4.14%\n",
      "Epoch 710 | Train Loss 2.7018545150756834 | Test Acc 92.26666666666667\n",
      "Epoch 711 | Batch 0/5 | Loss 2.550550\n",
      "Epoch 711 | Test Acc = 90.67% +- 3.70%\n",
      "Epoch 711 | Train Loss 2.553958034515381 | Test Acc 90.66666666666666\n",
      "Epoch 712 | Batch 0/5 | Loss 2.419770\n",
      "Epoch 712 | Test Acc = 86.67% +- 5.96%\n",
      "Epoch 712 | Train Loss 2.664864253997803 | Test Acc 86.66666666666666\n",
      "Epoch 713 | Batch 0/5 | Loss 2.835830\n",
      "Epoch 713 | Test Acc = 91.47% +- 5.71%\n",
      "Epoch 713 | Train Loss 2.7024892807006835 | Test Acc 91.46666666666667\n",
      "Epoch 714 | Batch 0/5 | Loss 2.448932\n",
      "Epoch 714 | Test Acc = 94.67% +- 4.43%\n",
      "Epoch 714 | Train Loss 2.588942623138428 | Test Acc 94.66666666666667\n",
      "Epoch 715 | Batch 0/5 | Loss 2.468536\n",
      "Epoch 715 | Test Acc = 86.67% +- 5.38%\n",
      "Epoch 715 | Train Loss 2.5136672496795653 | Test Acc 86.66666666666667\n",
      "Epoch 716 | Batch 0/5 | Loss 2.844969\n",
      "Epoch 716 | Test Acc = 89.33% +- 7.68%\n",
      "Epoch 716 | Train Loss 2.7048853397369386 | Test Acc 89.33333333333334\n",
      "Epoch 717 | Batch 0/5 | Loss 2.495396\n",
      "Epoch 717 | Test Acc = 93.60% +- 5.20%\n",
      "Epoch 717 | Train Loss 2.647598123550415 | Test Acc 93.60000000000001\n",
      "Epoch 718 | Batch 0/5 | Loss 2.602429\n",
      "Epoch 718 | Test Acc = 81.33% +- 9.44%\n",
      "Epoch 718 | Train Loss 2.5965767383575438 | Test Acc 81.33333333333333\n",
      "Epoch 719 | Batch 0/5 | Loss 2.773581\n",
      "Epoch 719 | Test Acc = 92.53% +- 3.95%\n",
      "Epoch 719 | Train Loss 2.720290422439575 | Test Acc 92.53333333333333\n",
      "Epoch 720 | Batch 0/5 | Loss 2.683975\n",
      "Epoch 720 | Test Acc = 89.33% +- 3.84%\n",
      "Epoch 720 | Train Loss 2.6062973499298097 | Test Acc 89.33333333333334\n",
      "Epoch 721 | Batch 0/5 | Loss 2.653091\n",
      "Epoch 721 | Test Acc = 93.87% +- 4.41%\n",
      "Epoch 721 | Train Loss 2.6353869915008543 | Test Acc 93.86666666666667\n",
      "Epoch 722 | Batch 0/5 | Loss 2.659521\n",
      "Epoch 722 | Test Acc = 89.07% +- 3.17%\n",
      "Epoch 722 | Train Loss 2.6732061386108397 | Test Acc 89.06666666666668\n",
      "Epoch 723 | Batch 0/5 | Loss 2.675229\n",
      "Epoch 723 | Test Acc = 84.27% +- 6.11%\n",
      "Epoch 723 | Train Loss 2.645592975616455 | Test Acc 84.26666666666668\n",
      "Epoch 724 | Batch 0/5 | Loss 2.467591\n",
      "Epoch 724 | Test Acc = 90.93% +- 6.20%\n",
      "Epoch 724 | Train Loss 2.5924986839294433 | Test Acc 90.93333333333334\n",
      "Epoch 725 | Batch 0/5 | Loss 2.912300\n",
      "Epoch 725 | Test Acc = 89.07% +- 6.20%\n",
      "Epoch 725 | Train Loss 2.633542776107788 | Test Acc 89.06666666666668\n",
      "Epoch 726 | Batch 0/5 | Loss 2.497584\n",
      "Epoch 726 | Test Acc = 86.13% +- 10.39%\n",
      "Epoch 726 | Train Loss 2.6071980953216554 | Test Acc 86.13333333333334\n",
      "Epoch 727 | Batch 0/5 | Loss 2.492435\n",
      "Epoch 727 | Test Acc = 89.07% +- 4.76%\n",
      "Epoch 727 | Train Loss 2.7878318786621095 | Test Acc 89.06666666666666\n",
      "Epoch 728 | Batch 0/5 | Loss 2.437004\n",
      "Epoch 728 | Test Acc = 91.47% +- 4.02%\n",
      "Epoch 728 | Train Loss 2.5455429553985596 | Test Acc 91.46666666666667\n",
      "Epoch 729 | Batch 0/5 | Loss 2.716476\n",
      "Epoch 729 | Test Acc = 86.67% +- 14.73%\n",
      "Epoch 729 | Train Loss 2.646307945251465 | Test Acc 86.66666666666666\n",
      "Epoch 730 | Batch 0/5 | Loss 2.721858\n",
      "Epoch 730 | Test Acc = 89.33% +- 6.85%\n",
      "Epoch 730 | Train Loss 2.5754239559173584 | Test Acc 89.33333333333333\n",
      "Epoch 731 | Batch 0/5 | Loss 2.565420\n",
      "Epoch 731 | Test Acc = 89.87% +- 5.36%\n",
      "Epoch 731 | Train Loss 2.568182134628296 | Test Acc 89.86666666666667\n",
      "Epoch 732 | Batch 0/5 | Loss 2.589635\n",
      "Epoch 732 | Test Acc = 88.53% +- 3.74%\n",
      "Epoch 732 | Train Loss 2.642738151550293 | Test Acc 88.53333333333333\n",
      "Epoch 733 | Batch 0/5 | Loss 2.545726\n",
      "Epoch 733 | Test Acc = 87.47% +- 4.77%\n",
      "Epoch 733 | Train Loss 2.7991256713867188 | Test Acc 87.46666666666665\n",
      "Epoch 734 | Batch 0/5 | Loss 2.766161\n",
      "Epoch 734 | Test Acc = 89.33% +- 3.05%\n",
      "Epoch 734 | Train Loss 2.6320149421691896 | Test Acc 89.33333333333333\n",
      "Epoch 735 | Batch 0/5 | Loss 2.617679\n",
      "Epoch 735 | Test Acc = 87.20% +- 3.95%\n",
      "Epoch 735 | Train Loss 2.76232705116272 | Test Acc 87.2\n",
      "Epoch 736 | Batch 0/5 | Loss 2.615350\n",
      "Epoch 736 | Test Acc = 86.13% +- 5.05%\n",
      "Epoch 736 | Train Loss 2.5038262367248536 | Test Acc 86.13333333333333\n",
      "Epoch 737 | Batch 0/5 | Loss 2.508735\n",
      "Epoch 737 | Test Acc = 90.93% +- 6.20%\n",
      "Epoch 737 | Train Loss 2.5881207942962647 | Test Acc 90.93333333333334\n",
      "Epoch 738 | Batch 0/5 | Loss 2.558067\n",
      "Epoch 738 | Test Acc = 92.53% +- 4.59%\n",
      "Epoch 738 | Train Loss 2.5887614250183106 | Test Acc 92.53333333333335\n",
      "Epoch 739 | Batch 0/5 | Loss 2.304949\n",
      "Epoch 739 | Test Acc = 89.87% +- 5.71%\n",
      "Epoch 739 | Train Loss 2.4778801918029787 | Test Acc 89.86666666666666\n",
      "Epoch 740 | Batch 0/5 | Loss 2.578464\n",
      "Epoch 740 | Test Acc = 89.07% +- 8.70%\n",
      "Epoch 740 | Train Loss 2.5994239807128907 | Test Acc 89.06666666666668\n",
      "Epoch 741 | Batch 0/5 | Loss 2.703124\n",
      "Epoch 741 | Test Acc = 87.73% +- 8.08%\n",
      "Epoch 741 | Train Loss 2.651529836654663 | Test Acc 87.73333333333335\n",
      "Epoch 742 | Batch 0/5 | Loss 2.724025\n",
      "Epoch 742 | Test Acc = 93.60% +- 2.27%\n",
      "Epoch 742 | Train Loss 2.7039029121398928 | Test Acc 93.6\n",
      "Epoch 743 | Batch 0/5 | Loss 2.534532\n",
      "Epoch 743 | Test Acc = 94.13% +- 5.10%\n",
      "Epoch 743 | Train Loss 2.634306001663208 | Test Acc 94.13333333333334\n",
      "Epoch 744 | Batch 0/5 | Loss 2.745729\n",
      "Epoch 744 | Test Acc = 83.47% +- 8.64%\n",
      "Epoch 744 | Train Loss 2.5879196166992187 | Test Acc 83.46666666666667\n",
      "Epoch 745 | Batch 0/5 | Loss 2.250603\n",
      "Epoch 745 | Test Acc = 94.40% +- 2.90%\n",
      "Epoch 745 | Train Loss 2.5514432907104494 | Test Acc 94.4\n",
      "Epoch 746 | Batch 0/5 | Loss 2.685121\n",
      "Epoch 746 | Test Acc = 84.80% +- 7.07%\n",
      "Epoch 746 | Train Loss 2.6547254085540772 | Test Acc 84.79999999999998\n",
      "Epoch 747 | Batch 0/5 | Loss 2.757154\n",
      "Epoch 747 | Test Acc = 88.80% +- 3.95%\n",
      "Epoch 747 | Train Loss 2.5527960777282717 | Test Acc 88.80000000000001\n",
      "Epoch 748 | Batch 0/5 | Loss 2.603609\n",
      "Epoch 748 | Test Acc = 90.13% +- 6.12%\n",
      "Epoch 748 | Train Loss 2.6337052822113036 | Test Acc 90.13333333333334\n",
      "Epoch 749 | Batch 0/5 | Loss 2.563025\n",
      "Epoch 749 | Test Acc = 84.53% +- 6.59%\n",
      "Epoch 749 | Train Loss 2.6386155128479003 | Test Acc 84.53333333333333\n",
      "Epoch 750 | Batch 0/5 | Loss 2.525874\n",
      "Epoch 750 | Test Acc = 92.53% +- 6.21%\n",
      "Epoch 750 | Train Loss 2.5304433822631838 | Test Acc 92.53333333333335\n",
      "Epoch 751 | Batch 0/5 | Loss 2.797818\n",
      "Epoch 751 | Test Acc = 85.07% +- 3.34%\n",
      "Epoch 751 | Train Loss 2.6340639114379885 | Test Acc 85.06666666666666\n",
      "Epoch 752 | Batch 0/5 | Loss 2.596003\n",
      "Epoch 752 | Test Acc = 89.60% +- 8.14%\n",
      "Epoch 752 | Train Loss 2.5823901653289796 | Test Acc 89.6\n",
      "Epoch 753 | Batch 0/5 | Loss 2.629000\n",
      "Epoch 753 | Test Acc = 90.13% +- 5.80%\n",
      "Epoch 753 | Train Loss 2.5781791687011717 | Test Acc 90.13333333333334\n",
      "Epoch 754 | Batch 0/5 | Loss 2.667886\n",
      "Epoch 754 | Test Acc = 90.93% +- 3.80%\n",
      "Epoch 754 | Train Loss 2.5774410724639893 | Test Acc 90.93333333333334\n",
      "Epoch 755 | Batch 0/5 | Loss 2.732243\n",
      "Epoch 755 | Test Acc = 88.27% +- 9.45%\n",
      "Epoch 755 | Train Loss 2.57306547164917 | Test Acc 88.26666666666668\n",
      "Epoch 756 | Batch 0/5 | Loss 2.610501\n",
      "Epoch 756 | Test Acc = 89.60% +- 9.79%\n",
      "Epoch 756 | Train Loss 2.608085012435913 | Test Acc 89.60000000000001\n",
      "Epoch 757 | Batch 0/5 | Loss 2.626971\n",
      "Epoch 757 | Test Acc = 89.33% +- 4.85%\n",
      "Epoch 757 | Train Loss 2.5630890369415282 | Test Acc 89.33333333333334\n",
      "Epoch 758 | Batch 0/5 | Loss 2.725429\n",
      "Epoch 758 | Test Acc = 91.20% +- 6.59%\n",
      "Epoch 758 | Train Loss 2.5791192054748535 | Test Acc 91.2\n",
      "Epoch 759 | Batch 0/5 | Loss 2.401277\n",
      "Epoch 759 | Test Acc = 92.27% +- 3.34%\n",
      "Epoch 759 | Train Loss 2.522442436218262 | Test Acc 92.26666666666667\n",
      "Epoch 760 | Batch 0/5 | Loss 2.541978\n",
      "Epoch 760 | Test Acc = 91.20% +- 3.74%\n",
      "Epoch 760 | Train Loss 2.6547061920166017 | Test Acc 91.2\n",
      "Epoch 761 | Batch 0/5 | Loss 2.446959\n",
      "Epoch 761 | Test Acc = 89.07% +- 6.50%\n",
      "Epoch 761 | Train Loss 2.5073301792144775 | Test Acc 89.06666666666666\n",
      "Epoch 762 | Batch 0/5 | Loss 2.554745\n",
      "Epoch 762 | Test Acc = 87.73% +- 9.15%\n",
      "Epoch 762 | Train Loss 2.6526042461395263 | Test Acc 87.73333333333332\n",
      "Epoch 763 | Batch 0/5 | Loss 2.438111\n",
      "Epoch 763 | Test Acc = 78.13% +- 6.88%\n",
      "Epoch 763 | Train Loss 2.6562539100646974 | Test Acc 78.13333333333333\n",
      "Epoch 764 | Batch 0/5 | Loss 2.622888\n",
      "Epoch 764 | Test Acc = 92.00% +- 5.96%\n",
      "Epoch 764 | Train Loss 2.594921588897705 | Test Acc 92.0\n",
      "Epoch 765 | Batch 0/5 | Loss 2.520998\n",
      "Epoch 765 | Test Acc = 89.87% +- 6.21%\n",
      "Epoch 765 | Train Loss 2.614359188079834 | Test Acc 89.86666666666666\n",
      "Epoch 766 | Batch 0/5 | Loss 2.568943\n",
      "Epoch 766 | Test Acc = 93.07% +- 3.58%\n",
      "Epoch 766 | Train Loss 2.554445171356201 | Test Acc 93.06666666666668\n",
      "Epoch 767 | Batch 0/5 | Loss 2.468735\n",
      "Epoch 767 | Test Acc = 87.20% +- 8.12%\n",
      "Epoch 767 | Train Loss 2.5792708873748778 | Test Acc 87.2\n",
      "Epoch 768 | Batch 0/5 | Loss 2.797550\n",
      "Epoch 768 | Test Acc = 78.40% +- 3.42%\n",
      "Epoch 768 | Train Loss 2.687531757354736 | Test Acc 78.4\n",
      "Epoch 769 | Batch 0/5 | Loss 2.531239\n",
      "Epoch 769 | Test Acc = 92.80% +- 4.94%\n",
      "Epoch 769 | Train Loss 2.530834674835205 | Test Acc 92.8\n",
      "Epoch 770 | Batch 0/5 | Loss 2.549461\n",
      "Epoch 770 | Test Acc = 81.33% +- 5.82%\n",
      "Epoch 770 | Train Loss 2.5019302368164062 | Test Acc 81.33333333333334\n",
      "Epoch 771 | Batch 0/5 | Loss 2.579870\n",
      "Epoch 771 | Test Acc = 87.20% +- 7.38%\n",
      "Epoch 771 | Train Loss 2.5928637981414795 | Test Acc 87.19999999999999\n",
      "Epoch 772 | Batch 0/5 | Loss 2.546322\n",
      "Epoch 772 | Test Acc = 90.93% +- 4.58%\n",
      "Epoch 772 | Train Loss 2.582621717453003 | Test Acc 90.93333333333334\n",
      "Epoch 773 | Batch 0/5 | Loss 2.500801\n",
      "Epoch 773 | Test Acc = 82.40% +- 2.27%\n",
      "Epoch 773 | Train Loss 2.581950378417969 | Test Acc 82.4\n",
      "Epoch 774 | Batch 0/5 | Loss 2.351497\n",
      "Epoch 774 | Test Acc = 89.33% +- 4.37%\n",
      "Epoch 774 | Train Loss 2.5310982704162597 | Test Acc 89.33333333333333\n",
      "Epoch 775 | Batch 0/5 | Loss 2.727775\n",
      "Epoch 775 | Test Acc = 91.47% +- 6.80%\n",
      "Epoch 775 | Train Loss 2.5852967739105224 | Test Acc 91.46666666666667\n",
      "Epoch 776 | Batch 0/5 | Loss 2.606448\n",
      "Epoch 776 | Test Acc = 81.87% +- 10.02%\n",
      "Epoch 776 | Train Loss 2.5516576290130617 | Test Acc 81.86666666666665\n",
      "Epoch 777 | Batch 0/5 | Loss 2.496057\n",
      "Epoch 777 | Test Acc = 87.20% +- 4.47%\n",
      "Epoch 777 | Train Loss 2.7052876949310303 | Test Acc 87.19999999999999\n",
      "Epoch 778 | Batch 0/5 | Loss 2.798339\n",
      "Epoch 778 | Test Acc = 89.60% +- 4.76%\n",
      "Epoch 778 | Train Loss 2.664955186843872 | Test Acc 89.6\n",
      "Epoch 779 | Batch 0/5 | Loss 2.502726\n",
      "Epoch 779 | Test Acc = 93.87% +- 2.62%\n",
      "Epoch 779 | Train Loss 2.4962917804718017 | Test Acc 93.86666666666666\n",
      "Epoch 780 | Batch 0/5 | Loss 2.463707\n",
      "Epoch 780 | Test Acc = 93.07% +- 4.14%\n",
      "Epoch 780 | Train Loss 2.5262444972991944 | Test Acc 93.06666666666668\n",
      "Epoch 781 | Batch 0/5 | Loss 2.610122\n",
      "Epoch 781 | Test Acc = 93.07% +- 1.36%\n",
      "Epoch 781 | Train Loss 2.5730923652648925 | Test Acc 93.06666666666666\n",
      "Epoch 782 | Batch 0/5 | Loss 2.321659\n",
      "Epoch 782 | Test Acc = 91.47% +- 3.59%\n",
      "Epoch 782 | Train Loss 2.6615482330322267 | Test Acc 91.46666666666667\n",
      "Epoch 783 | Batch 0/5 | Loss 2.446764\n",
      "Epoch 783 | Test Acc = 90.13% +- 7.00%\n",
      "Epoch 783 | Train Loss 2.534694242477417 | Test Acc 90.13333333333334\n",
      "Epoch 784 | Batch 0/5 | Loss 2.507917\n",
      "Epoch 784 | Test Acc = 94.13% +- 4.47%\n",
      "Epoch 784 | Train Loss 2.4886576175689696 | Test Acc 94.13333333333334\n",
      "Epoch 785 | Batch 0/5 | Loss 2.495685\n",
      "Epoch 785 | Test Acc = 83.20% +- 8.38%\n",
      "Epoch 785 | Train Loss 2.546260118484497 | Test Acc 83.2\n",
      "Epoch 786 | Batch 0/5 | Loss 2.623839\n",
      "Epoch 786 | Test Acc = 92.00% +- 4.37%\n",
      "Epoch 786 | Train Loss 2.5124621868133543 | Test Acc 92.0\n",
      "Epoch 787 | Batch 0/5 | Loss 2.400812\n",
      "Epoch 787 | Test Acc = 88.00% +- 6.65%\n",
      "Epoch 787 | Train Loss 2.518248271942139 | Test Acc 88.0\n",
      "Epoch 788 | Batch 0/5 | Loss 2.468716\n",
      "Epoch 788 | Test Acc = 85.07% +- 3.73%\n",
      "Epoch 788 | Train Loss 2.452736568450928 | Test Acc 85.06666666666666\n",
      "Epoch 789 | Batch 0/5 | Loss 2.437114\n",
      "Epoch 789 | Test Acc = 87.73% +- 3.42%\n",
      "Epoch 789 | Train Loss 2.5775349140167236 | Test Acc 87.73333333333333\n",
      "Epoch 790 | Batch 0/5 | Loss 2.594840\n",
      "Epoch 790 | Test Acc = 87.47% +- 6.43%\n",
      "Epoch 790 | Train Loss 2.5873193740844727 | Test Acc 87.46666666666667\n",
      "Epoch 791 | Batch 0/5 | Loss 2.487606\n",
      "Epoch 791 | Test Acc = 88.80% +- 8.86%\n",
      "Epoch 791 | Train Loss 2.548515796661377 | Test Acc 88.80000000000001\n",
      "Epoch 792 | Batch 0/5 | Loss 2.733372\n",
      "Epoch 792 | Test Acc = 88.80% +- 9.19%\n",
      "Epoch 792 | Train Loss 2.550777864456177 | Test Acc 88.8\n",
      "Epoch 793 | Batch 0/5 | Loss 2.412638\n",
      "Epoch 793 | Test Acc = 81.60% +- 7.84%\n",
      "Epoch 793 | Train Loss 2.5312713623046874 | Test Acc 81.6\n",
      "Epoch 794 | Batch 0/5 | Loss 2.657351\n",
      "Epoch 794 | Test Acc = 88.80% +- 4.65%\n",
      "Epoch 794 | Train Loss 2.507304573059082 | Test Acc 88.8\n",
      "Epoch 795 | Batch 0/5 | Loss 2.408233\n",
      "Epoch 795 | Test Acc = 93.07% +- 3.26%\n",
      "Epoch 795 | Train Loss 2.5294885635375977 | Test Acc 93.06666666666668\n",
      "Epoch 796 | Batch 0/5 | Loss 2.408839\n",
      "Epoch 796 | Test Acc = 83.20% +- 10.36%\n",
      "Epoch 796 | Train Loss 2.44533371925354 | Test Acc 83.2\n",
      "Epoch 797 | Batch 0/5 | Loss 2.559426\n",
      "Epoch 797 | Test Acc = 90.93% +- 6.11%\n",
      "Epoch 797 | Train Loss 2.4914026260375977 | Test Acc 90.93333333333332\n",
      "Epoch 798 | Batch 0/5 | Loss 2.534417\n",
      "Epoch 798 | Test Acc = 85.60% +- 10.77%\n",
      "Epoch 798 | Train Loss 2.541272735595703 | Test Acc 85.6\n",
      "Epoch 799 | Batch 0/5 | Loss 2.533773\n",
      "Epoch 799 | Test Acc = 91.73% +- 3.26%\n",
      "Epoch 799 | Train Loss 2.585580062866211 | Test Acc 91.73333333333332\n",
      "Epoch 800 | Batch 0/5 | Loss 2.552196\n",
      "Epoch 800 | Test Acc = 93.07% +- 3.94%\n",
      "Epoch 800 | Train Loss 2.54303879737854 | Test Acc 93.06666666666668\n",
      "Epoch 801 | Batch 0/5 | Loss 2.426676\n",
      "Epoch 801 | Test Acc = 92.00% +- 3.31%\n",
      "Epoch 801 | Train Loss 2.647469472885132 | Test Acc 91.99999999999999\n",
      "Epoch 802 | Batch 0/5 | Loss 2.671257\n",
      "Epoch 802 | Test Acc = 91.73% +- 3.26%\n",
      "Epoch 802 | Train Loss 2.551602363586426 | Test Acc 91.73333333333333\n",
      "Epoch 803 | Batch 0/5 | Loss 2.503085\n",
      "Epoch 803 | Test Acc = 87.47% +- 5.15%\n",
      "Epoch 803 | Train Loss 2.4920597076416016 | Test Acc 87.46666666666667\n",
      "Epoch 804 | Batch 0/5 | Loss 2.439928\n",
      "Epoch 804 | Test Acc = 88.00% +- 6.05%\n",
      "Epoch 804 | Train Loss 2.5175944328308106 | Test Acc 88.0\n",
      "Epoch 805 | Batch 0/5 | Loss 2.316182\n",
      "Epoch 805 | Test Acc = 85.33% +- 5.43%\n",
      "Epoch 805 | Train Loss 2.555730628967285 | Test Acc 85.33333333333333\n",
      "Epoch 806 | Batch 0/5 | Loss 2.504479\n",
      "Epoch 806 | Test Acc = 88.00% +- 5.63%\n",
      "Epoch 806 | Train Loss 2.482809066772461 | Test Acc 88.0\n",
      "Epoch 807 | Batch 0/5 | Loss 2.551351\n",
      "Epoch 807 | Test Acc = 96.27% +- 1.55%\n",
      "Epoch 807 | Train Loss 2.4921080112457275 | Test Acc 96.26666666666668\n",
      "Epoch 808 | Batch 0/5 | Loss 2.655947\n",
      "Epoch 808 | Test Acc = 93.33% +- 4.85%\n",
      "Epoch 808 | Train Loss 2.714817428588867 | Test Acc 93.33333333333334\n",
      "Epoch 809 | Batch 0/5 | Loss 2.617550\n",
      "Epoch 809 | Test Acc = 88.27% +- 11.84%\n",
      "Epoch 809 | Train Loss 2.5336875438690187 | Test Acc 88.26666666666667\n",
      "Epoch 810 | Batch 0/5 | Loss 2.452156\n",
      "Epoch 810 | Test Acc = 87.73% +- 6.79%\n",
      "Epoch 810 | Train Loss 2.514060115814209 | Test Acc 87.73333333333333\n",
      "Epoch 811 | Batch 0/5 | Loss 2.303119\n",
      "Epoch 811 | Test Acc = 92.27% +- 2.50%\n",
      "Epoch 811 | Train Loss 2.4578567504882813 | Test Acc 92.26666666666668\n",
      "Epoch 812 | Batch 0/5 | Loss 2.796478\n",
      "Epoch 812 | Test Acc = 88.80% +- 5.36%\n",
      "Epoch 812 | Train Loss 2.5276541709899902 | Test Acc 88.80000000000001\n",
      "Epoch 813 | Batch 0/5 | Loss 2.411358\n",
      "Epoch 813 | Test Acc = 87.73% +- 3.50%\n",
      "Epoch 813 | Train Loss 2.5593278884887694 | Test Acc 87.73333333333333\n",
      "Epoch 814 | Batch 0/5 | Loss 2.500124\n",
      "Epoch 814 | Test Acc = 92.27% +- 2.50%\n",
      "Epoch 814 | Train Loss 2.5156912803649902 | Test Acc 92.26666666666668\n",
      "Epoch 815 | Batch 0/5 | Loss 2.398814\n",
      "Epoch 815 | Test Acc = 91.73% +- 3.34%\n",
      "Epoch 815 | Train Loss 2.50068039894104 | Test Acc 91.73333333333335\n",
      "Epoch 816 | Batch 0/5 | Loss 2.442728\n",
      "Epoch 816 | Test Acc = 90.67% +- 7.65%\n",
      "Epoch 816 | Train Loss 2.539657545089722 | Test Acc 90.66666666666666\n",
      "Epoch 817 | Batch 0/5 | Loss 2.682603\n",
      "Epoch 817 | Test Acc = 89.33% +- 3.91%\n",
      "Epoch 817 | Train Loss 2.6026747703552244 | Test Acc 89.33333333333333\n",
      "Epoch 818 | Batch 0/5 | Loss 2.479000\n",
      "Epoch 818 | Test Acc = 87.73% +- 5.20%\n",
      "Epoch 818 | Train Loss 2.5651751518249513 | Test Acc 87.73333333333332\n",
      "Epoch 819 | Batch 0/5 | Loss 2.455456\n",
      "Epoch 819 | Test Acc = 87.20% +- 8.28%\n",
      "Epoch 819 | Train Loss 2.53460054397583 | Test Acc 87.2\n",
      "Epoch 820 | Batch 0/5 | Loss 2.575473\n",
      "Epoch 820 | Test Acc = 91.47% +- 8.38%\n",
      "Epoch 820 | Train Loss 2.602255916595459 | Test Acc 91.46666666666665\n",
      "Epoch 821 | Batch 0/5 | Loss 2.447684\n",
      "Epoch 821 | Test Acc = 91.47% +- 4.35%\n",
      "Epoch 821 | Train Loss 2.4752426147460938 | Test Acc 91.46666666666667\n",
      "Epoch 822 | Batch 0/5 | Loss 2.444772\n",
      "Epoch 822 | Test Acc = 89.07% +- 3.65%\n",
      "Epoch 822 | Train Loss 2.560581588745117 | Test Acc 89.06666666666668\n",
      "Epoch 823 | Batch 0/5 | Loss 2.560979\n",
      "Epoch 823 | Test Acc = 92.53% +- 2.17%\n",
      "Epoch 823 | Train Loss 2.5227306365966795 | Test Acc 92.53333333333333\n",
      "Epoch 824 | Batch 0/5 | Loss 2.526464\n",
      "Epoch 824 | Test Acc = 86.40% +- 7.14%\n",
      "Epoch 824 | Train Loss 2.540197992324829 | Test Acc 86.4\n",
      "Epoch 825 | Batch 0/5 | Loss 2.525161\n",
      "Epoch 825 | Test Acc = 91.20% +- 5.46%\n",
      "Epoch 825 | Train Loss 2.552076721191406 | Test Acc 91.2\n",
      "Epoch 826 | Batch 0/5 | Loss 2.459890\n",
      "Epoch 826 | Test Acc = 87.20% +- 10.88%\n",
      "Epoch 826 | Train Loss 2.4625012397766115 | Test Acc 87.2\n",
      "Epoch 827 | Batch 0/5 | Loss 2.693798\n",
      "Epoch 827 | Test Acc = 91.20% +- 7.07%\n",
      "Epoch 827 | Train Loss 2.721077537536621 | Test Acc 91.2\n",
      "Epoch 828 | Batch 0/5 | Loss 2.600259\n",
      "Epoch 828 | Test Acc = 86.13% +- 6.76%\n",
      "Epoch 828 | Train Loss 2.5337031364440916 | Test Acc 86.13333333333334\n",
      "Epoch 829 | Batch 0/5 | Loss 2.362073\n",
      "Epoch 829 | Test Acc = 84.27% +- 8.24%\n",
      "Epoch 829 | Train Loss 2.5605968475341796 | Test Acc 84.26666666666667\n",
      "Epoch 830 | Batch 0/5 | Loss 2.559377\n",
      "Epoch 830 | Test Acc = 90.40% +- 7.26%\n",
      "Epoch 830 | Train Loss 2.6614137172698973 | Test Acc 90.4\n",
      "Epoch 831 | Batch 0/5 | Loss 2.531009\n",
      "Epoch 831 | Test Acc = 88.53% +- 7.38%\n",
      "Epoch 831 | Train Loss 2.4585659980773924 | Test Acc 88.53333333333333\n",
      "Epoch 832 | Batch 0/5 | Loss 2.415626\n",
      "Epoch 832 | Test Acc = 81.87% +- 6.03%\n",
      "Epoch 832 | Train Loss 2.4071433544158936 | Test Acc 81.86666666666666\n",
      "Epoch 833 | Batch 0/5 | Loss 2.390975\n",
      "Epoch 833 | Test Acc = 89.60% +- 7.22%\n",
      "Epoch 833 | Train Loss 2.5233481407165526 | Test Acc 89.60000000000001\n",
      "Epoch 834 | Batch 0/5 | Loss 2.410918\n",
      "Epoch 834 | Test Acc = 91.73% +- 3.58%\n",
      "Epoch 834 | Train Loss 2.4919402599334717 | Test Acc 91.73333333333332\n",
      "Epoch 835 | Batch 0/5 | Loss 2.621858\n",
      "Epoch 835 | Test Acc = 95.73% +- 1.36%\n",
      "Epoch 835 | Train Loss 2.564833164215088 | Test Acc 95.73333333333335\n",
      "Epoch 836 | Batch 0/5 | Loss 2.440984\n",
      "Epoch 836 | Test Acc = 88.53% +- 6.88%\n",
      "Epoch 836 | Train Loss 2.5824358463287354 | Test Acc 88.53333333333333\n",
      "Epoch 837 | Batch 0/5 | Loss 2.546203\n",
      "Epoch 837 | Test Acc = 93.60% +- 3.80%\n",
      "Epoch 837 | Train Loss 2.464926815032959 | Test Acc 93.6\n",
      "Epoch 838 | Batch 0/5 | Loss 2.686078\n",
      "Epoch 838 | Test Acc = 91.73% +- 4.98%\n",
      "Epoch 838 | Train Loss 2.55158371925354 | Test Acc 91.73333333333332\n",
      "Epoch 839 | Batch 0/5 | Loss 2.425649\n",
      "Epoch 839 | Test Acc = 81.60% +- 13.80%\n",
      "Epoch 839 | Train Loss 2.4183746337890626 | Test Acc 81.6\n",
      "Epoch 840 | Batch 0/5 | Loss 2.404429\n",
      "Epoch 840 | Test Acc = 90.40% +- 8.44%\n",
      "Epoch 840 | Train Loss 2.512461614608765 | Test Acc 90.4\n",
      "Epoch 841 | Batch 0/5 | Loss 2.355301\n",
      "Epoch 841 | Test Acc = 89.33% +- 3.84%\n",
      "Epoch 841 | Train Loss 2.4606658458709716 | Test Acc 89.33333333333333\n",
      "Epoch 842 | Batch 0/5 | Loss 2.643986\n",
      "Epoch 842 | Test Acc = 88.53% +- 3.27%\n",
      "Epoch 842 | Train Loss 2.4745288372039793 | Test Acc 88.53333333333333\n",
      "Epoch 843 | Batch 0/5 | Loss 2.377160\n",
      "Epoch 843 | Test Acc = 89.60% +- 6.07%\n",
      "Epoch 843 | Train Loss 2.5474488735198975 | Test Acc 89.6\n",
      "Epoch 844 | Batch 0/5 | Loss 2.441256\n",
      "Epoch 844 | Test Acc = 92.00% +- 2.56%\n",
      "Epoch 844 | Train Loss 2.535638427734375 | Test Acc 91.99999999999999\n",
      "Epoch 845 | Batch 0/5 | Loss 2.568057\n",
      "Epoch 845 | Test Acc = 89.33% +- 9.49%\n",
      "Epoch 845 | Train Loss 2.3944793224334715 | Test Acc 89.33333333333334\n",
      "Epoch 846 | Batch 0/5 | Loss 2.668332\n",
      "Epoch 846 | Test Acc = 88.53% +- 6.08%\n",
      "Epoch 846 | Train Loss 2.528017044067383 | Test Acc 88.53333333333333\n",
      "Epoch 847 | Batch 0/5 | Loss 2.715027\n",
      "Epoch 847 | Test Acc = 97.07% +- 1.87%\n",
      "Epoch 847 | Train Loss 2.5174859523773194 | Test Acc 97.06666666666666\n",
      "Epoch 848 | Batch 0/5 | Loss 2.501755\n",
      "Epoch 848 | Test Acc = 92.27% +- 3.80%\n",
      "Epoch 848 | Train Loss 2.4658659934997558 | Test Acc 92.26666666666665\n",
      "Epoch 849 | Batch 0/5 | Loss 2.421059\n",
      "Epoch 849 | Test Acc = 89.33% +- 3.62%\n",
      "Epoch 849 | Train Loss 2.445656156539917 | Test Acc 89.33333333333334\n",
      "Epoch 850 | Batch 0/5 | Loss 2.890230\n",
      "Epoch 850 | Test Acc = 89.33% +- 6.73%\n",
      "Epoch 850 | Train Loss 2.579293155670166 | Test Acc 89.33333333333334\n",
      "Epoch 851 | Batch 0/5 | Loss 2.400004\n",
      "Epoch 851 | Test Acc = 92.27% +- 5.74%\n",
      "Epoch 851 | Train Loss 2.5037007331848145 | Test Acc 92.26666666666667\n",
      "Epoch 852 | Batch 0/5 | Loss 2.273319\n",
      "Epoch 852 | Test Acc = 94.13% +- 7.98%\n",
      "Epoch 852 | Train Loss 2.545094347000122 | Test Acc 94.13333333333334\n",
      "Epoch 853 | Batch 0/5 | Loss 2.575844\n",
      "Epoch 853 | Test Acc = 88.53% +- 7.23%\n",
      "Epoch 853 | Train Loss 2.4741652965545655 | Test Acc 88.53333333333335\n",
      "Epoch 854 | Batch 0/5 | Loss 2.477520\n",
      "Epoch 854 | Test Acc = 91.20% +- 1.75%\n",
      "Epoch 854 | Train Loss 2.4924270153045653 | Test Acc 91.2\n",
      "Epoch 855 | Batch 0/5 | Loss 2.302721\n",
      "Epoch 855 | Test Acc = 88.53% +- 5.66%\n",
      "Epoch 855 | Train Loss 2.482137155532837 | Test Acc 88.53333333333333\n",
      "Epoch 856 | Batch 0/5 | Loss 2.509721\n",
      "Epoch 856 | Test Acc = 92.80% +- 3.35%\n",
      "Epoch 856 | Train Loss 2.4924652576446533 | Test Acc 92.8\n",
      "Epoch 857 | Batch 0/5 | Loss 2.526398\n",
      "Epoch 857 | Test Acc = 94.40% +- 2.38%\n",
      "Epoch 857 | Train Loss 2.589158296585083 | Test Acc 94.4\n",
      "Epoch 858 | Batch 0/5 | Loss 2.259826\n",
      "Epoch 858 | Test Acc = 83.47% +- 5.89%\n",
      "Epoch 858 | Train Loss 2.510549259185791 | Test Acc 83.46666666666667\n",
      "Epoch 859 | Batch 0/5 | Loss 2.484582\n",
      "Epoch 859 | Test Acc = 85.60% +- 7.94%\n",
      "Epoch 859 | Train Loss 2.473838233947754 | Test Acc 85.6\n",
      "Epoch 860 | Batch 0/5 | Loss 2.440577\n",
      "Epoch 860 | Test Acc = 93.87% +- 4.94%\n",
      "Epoch 860 | Train Loss 2.4422494411468505 | Test Acc 93.86666666666667\n",
      "Epoch 861 | Batch 0/5 | Loss 2.347601\n",
      "Epoch 861 | Test Acc = 89.07% +- 7.07%\n",
      "Epoch 861 | Train Loss 2.4634868144989013 | Test Acc 89.06666666666666\n",
      "Epoch 862 | Batch 0/5 | Loss 2.332107\n",
      "Epoch 862 | Test Acc = 90.13% +- 3.01%\n",
      "Epoch 862 | Train Loss 2.4026480674743653 | Test Acc 90.13333333333334\n",
      "Epoch 863 | Batch 0/5 | Loss 2.376134\n",
      "Epoch 863 | Test Acc = 89.87% +- 4.02%\n",
      "Epoch 863 | Train Loss 2.4238181591033934 | Test Acc 89.86666666666666\n",
      "Epoch 864 | Batch 0/5 | Loss 2.460203\n",
      "Epoch 864 | Test Acc = 89.60% +- 4.64%\n",
      "Epoch 864 | Train Loss 2.5180985927581787 | Test Acc 89.6\n",
      "Epoch 865 | Batch 0/5 | Loss 2.405173\n",
      "Epoch 865 | Test Acc = 92.00% +- 3.91%\n",
      "Epoch 865 | Train Loss 2.4088809967041014 | Test Acc 92.0\n",
      "Epoch 866 | Batch 0/5 | Loss 2.440241\n",
      "Epoch 866 | Test Acc = 90.40% +- 2.50%\n",
      "Epoch 866 | Train Loss 2.4805477142333983 | Test Acc 90.4\n",
      "Epoch 867 | Batch 0/5 | Loss 2.702188\n",
      "Epoch 867 | Test Acc = 88.53% +- 3.67%\n",
      "Epoch 867 | Train Loss 2.474803113937378 | Test Acc 88.53333333333333\n",
      "Epoch 868 | Batch 0/5 | Loss 2.639223\n",
      "Epoch 868 | Test Acc = 93.07% +- 3.34%\n",
      "Epoch 868 | Train Loss 2.4322576999664305 | Test Acc 93.06666666666666\n",
      "Epoch 869 | Batch 0/5 | Loss 2.439668\n",
      "Epoch 869 | Test Acc = 83.20% +- 7.67%\n",
      "Epoch 869 | Train Loss 2.413871002197266 | Test Acc 83.19999999999999\n",
      "Epoch 870 | Batch 0/5 | Loss 2.570936\n",
      "Epoch 870 | Test Acc = 94.40% +- 2.01%\n",
      "Epoch 870 | Train Loss 2.5307069778442384 | Test Acc 94.4\n",
      "Epoch 871 | Batch 0/5 | Loss 2.372683\n",
      "Epoch 871 | Test Acc = 92.53% +- 10.83%\n",
      "Epoch 871 | Train Loss 2.4233325004577635 | Test Acc 92.53333333333333\n",
      "Epoch 872 | Batch 0/5 | Loss 2.616621\n",
      "Epoch 872 | Test Acc = 91.73% +- 4.08%\n",
      "Epoch 872 | Train Loss 2.4169538497924803 | Test Acc 91.73333333333332\n",
      "Epoch 873 | Batch 0/5 | Loss 2.713728\n",
      "Epoch 873 | Test Acc = 96.00% +- 2.77%\n",
      "Epoch 873 | Train Loss 2.4804780960083006 | Test Acc 96.00000000000001\n",
      "Epoch 874 | Batch 0/5 | Loss 2.389166\n",
      "Epoch 874 | Test Acc = 94.40% +- 3.50%\n",
      "Epoch 874 | Train Loss 2.4074541568756103 | Test Acc 94.4\n",
      "Epoch 875 | Batch 0/5 | Loss 2.477509\n",
      "Epoch 875 | Test Acc = 91.73% +- 4.58%\n",
      "Epoch 875 | Train Loss 2.487362194061279 | Test Acc 91.73333333333335\n",
      "Epoch 876 | Batch 0/5 | Loss 2.604805\n",
      "Epoch 876 | Test Acc = 91.47% +- 5.21%\n",
      "Epoch 876 | Train Loss 2.526075839996338 | Test Acc 91.46666666666667\n",
      "Epoch 877 | Batch 0/5 | Loss 2.312371\n",
      "Epoch 877 | Test Acc = 87.73% +- 9.39%\n",
      "Epoch 877 | Train Loss 2.452450466156006 | Test Acc 87.73333333333335\n",
      "Epoch 878 | Batch 0/5 | Loss 2.377889\n",
      "Epoch 878 | Test Acc = 93.87% +- 2.92%\n",
      "Epoch 878 | Train Loss 2.4524864673614504 | Test Acc 93.86666666666667\n",
      "Epoch 879 | Batch 0/5 | Loss 2.687175\n",
      "Epoch 879 | Test Acc = 94.40% +- 5.79%\n",
      "Epoch 879 | Train Loss 2.4810070037841796 | Test Acc 94.4\n",
      "Epoch 880 | Batch 0/5 | Loss 2.436144\n",
      "Epoch 880 | Test Acc = 92.53% +- 3.81%\n",
      "Epoch 880 | Train Loss 2.553537607192993 | Test Acc 92.53333333333333\n",
      "Epoch 881 | Batch 0/5 | Loss 2.471622\n",
      "Epoch 881 | Test Acc = 91.20% +- 3.59%\n",
      "Epoch 881 | Train Loss 2.4599533081054688 | Test Acc 91.2\n",
      "Epoch 882 | Batch 0/5 | Loss 2.422486\n",
      "Epoch 882 | Test Acc = 90.67% +- 3.54%\n",
      "Epoch 882 | Train Loss 2.46178879737854 | Test Acc 90.66666666666666\n",
      "Epoch 883 | Batch 0/5 | Loss 2.428296\n",
      "Epoch 883 | Test Acc = 91.73% +- 3.34%\n",
      "Epoch 883 | Train Loss 2.413814401626587 | Test Acc 91.73333333333332\n",
      "Epoch 884 | Batch 0/5 | Loss 2.580056\n",
      "Epoch 884 | Test Acc = 89.87% +- 5.56%\n",
      "Epoch 884 | Train Loss 2.5143600940704345 | Test Acc 89.86666666666667\n",
      "Epoch 885 | Batch 0/5 | Loss 2.380780\n",
      "Epoch 885 | Test Acc = 90.67% +- 5.23%\n",
      "Epoch 885 | Train Loss 2.485318422317505 | Test Acc 90.66666666666666\n",
      "Epoch 886 | Batch 0/5 | Loss 2.375251\n",
      "Epoch 886 | Test Acc = 90.67% +- 7.39%\n",
      "Epoch 886 | Train Loss 2.409690809249878 | Test Acc 90.66666666666666\n",
      "Epoch 887 | Batch 0/5 | Loss 2.664447\n",
      "Epoch 887 | Test Acc = 89.60% +- 6.33%\n",
      "Epoch 887 | Train Loss 2.507226037979126 | Test Acc 89.6\n",
      "Epoch 888 | Batch 0/5 | Loss 2.408480\n",
      "Epoch 888 | Test Acc = 89.07% +- 8.76%\n",
      "Epoch 888 | Train Loss 2.439924478530884 | Test Acc 89.06666666666668\n",
      "Epoch 889 | Batch 0/5 | Loss 2.297179\n",
      "Epoch 889 | Test Acc = 90.40% +- 2.27%\n",
      "Epoch 889 | Train Loss 2.3299458503723143 | Test Acc 90.4\n",
      "Epoch 890 | Batch 0/5 | Loss 2.188407\n",
      "Epoch 890 | Test Acc = 87.47% +- 5.05%\n",
      "Epoch 890 | Train Loss 2.35422420501709 | Test Acc 87.46666666666667\n",
      "Epoch 891 | Batch 0/5 | Loss 2.463206\n",
      "Epoch 891 | Test Acc = 86.93% +- 5.70%\n",
      "Epoch 891 | Train Loss 2.477773141860962 | Test Acc 86.93333333333335\n",
      "Epoch 892 | Batch 0/5 | Loss 2.311384\n",
      "Epoch 892 | Test Acc = 87.20% +- 6.88%\n",
      "Epoch 892 | Train Loss 2.375638818740845 | Test Acc 87.2\n",
      "Epoch 893 | Batch 0/5 | Loss 2.238521\n",
      "Epoch 893 | Test Acc = 88.27% +- 5.89%\n",
      "Epoch 893 | Train Loss 2.474159860610962 | Test Acc 88.26666666666668\n",
      "Epoch 894 | Batch 0/5 | Loss 2.473386\n",
      "Epoch 894 | Test Acc = 93.87% +- 5.10%\n",
      "Epoch 894 | Train Loss 2.3662325382232665 | Test Acc 93.86666666666666\n",
      "Epoch 895 | Batch 0/5 | Loss 2.442400\n",
      "Epoch 895 | Test Acc = 79.20% +- 8.64%\n",
      "Epoch 895 | Train Loss 2.4681774616241454 | Test Acc 79.2\n",
      "Epoch 896 | Batch 0/5 | Loss 2.488252\n",
      "Epoch 896 | Test Acc = 91.73% +- 6.50%\n",
      "Epoch 896 | Train Loss 2.3692384719848634 | Test Acc 91.73333333333332\n",
      "Epoch 897 | Batch 0/5 | Loss 2.248580\n",
      "Epoch 897 | Test Acc = 89.33% +- 3.62%\n",
      "Epoch 897 | Train Loss 2.3287750720977782 | Test Acc 89.33333333333334\n",
      "Epoch 898 | Batch 0/5 | Loss 2.470423\n",
      "Epoch 898 | Test Acc = 88.80% +- 6.51%\n",
      "Epoch 898 | Train Loss 2.4729827404022218 | Test Acc 88.8\n",
      "Epoch 899 | Batch 0/5 | Loss 2.465207\n",
      "Epoch 899 | Test Acc = 91.73% +- 4.46%\n",
      "Epoch 899 | Train Loss 2.5181633949279787 | Test Acc 91.73333333333332\n",
      "Epoch 900 | Batch 0/5 | Loss 2.350719\n",
      "Epoch 900 | Test Acc = 92.53% +- 5.66%\n",
      "Epoch 900 | Train Loss 2.3740628242492674 | Test Acc 92.53333333333335\n",
      "Epoch 901 | Batch 0/5 | Loss 2.493176\n",
      "Epoch 901 | Test Acc = 90.40% +- 4.27%\n",
      "Epoch 901 | Train Loss 2.4966878414154055 | Test Acc 90.4\n",
      "Epoch 902 | Batch 0/5 | Loss 2.460828\n",
      "Epoch 902 | Test Acc = 87.20% +- 6.30%\n",
      "Epoch 902 | Train Loss 2.393657350540161 | Test Acc 87.2\n",
      "Epoch 903 | Batch 0/5 | Loss 2.355841\n",
      "Epoch 903 | Test Acc = 89.33% +- 5.33%\n",
      "Epoch 903 | Train Loss 2.3805860042572022 | Test Acc 89.33333333333333\n",
      "Epoch 904 | Batch 0/5 | Loss 2.401724\n",
      "Epoch 904 | Test Acc = 89.87% +- 6.59%\n",
      "Epoch 904 | Train Loss 2.4172293186187743 | Test Acc 89.86666666666666\n",
      "Epoch 905 | Batch 0/5 | Loss 2.558702\n",
      "Epoch 905 | Test Acc = 86.93% +- 5.84%\n",
      "Epoch 905 | Train Loss 2.390684795379639 | Test Acc 86.93333333333334\n",
      "Epoch 906 | Batch 0/5 | Loss 2.346172\n",
      "Epoch 906 | Test Acc = 95.20% +- 1.59%\n",
      "Epoch 906 | Train Loss 2.356233596801758 | Test Acc 95.20000000000002\n",
      "Epoch 907 | Batch 0/5 | Loss 2.403132\n",
      "Epoch 907 | Test Acc = 89.60% +- 5.40%\n",
      "Epoch 907 | Train Loss 2.405525255203247 | Test Acc 89.6\n",
      "Epoch 908 | Batch 0/5 | Loss 2.228881\n",
      "Epoch 908 | Test Acc = 95.47% +- 3.35%\n",
      "Epoch 908 | Train Loss 2.3868918895721434 | Test Acc 95.46666666666667\n",
      "Epoch 909 | Batch 0/5 | Loss 2.474950\n",
      "Epoch 909 | Test Acc = 87.73% +- 7.80%\n",
      "Epoch 909 | Train Loss 2.3809525966644287 | Test Acc 87.73333333333333\n",
      "Epoch 910 | Batch 0/5 | Loss 2.480181\n",
      "Epoch 910 | Test Acc = 86.40% +- 10.82%\n",
      "Epoch 910 | Train Loss 2.455604076385498 | Test Acc 86.4\n",
      "Epoch 911 | Batch 0/5 | Loss 2.448737\n",
      "Epoch 911 | Test Acc = 94.93% +- 3.58%\n",
      "Epoch 911 | Train Loss 2.3812958717346193 | Test Acc 94.93333333333334\n",
      "Epoch 912 | Batch 0/5 | Loss 2.494142\n",
      "Epoch 912 | Test Acc = 86.67% +- 7.13%\n",
      "Epoch 912 | Train Loss 2.477718687057495 | Test Acc 86.66666666666666\n",
      "Epoch 913 | Batch 0/5 | Loss 2.514677\n",
      "Epoch 913 | Test Acc = 89.87% +- 4.94%\n",
      "Epoch 913 | Train Loss 2.4551109313964843 | Test Acc 89.86666666666667\n",
      "Epoch 914 | Batch 0/5 | Loss 2.317888\n",
      "Epoch 914 | Test Acc = 92.27% +- 5.09%\n",
      "Epoch 914 | Train Loss 2.4113776683807373 | Test Acc 92.26666666666668\n",
      "Epoch 915 | Batch 0/5 | Loss 2.355685\n",
      "Epoch 915 | Test Acc = 82.13% +- 9.54%\n",
      "Epoch 915 | Train Loss 2.3997991561889647 | Test Acc 82.13333333333334\n",
      "Epoch 916 | Batch 0/5 | Loss 2.462898\n",
      "Epoch 916 | Test Acc = 90.13% +- 6.12%\n",
      "Epoch 916 | Train Loss 2.4226405143737795 | Test Acc 90.13333333333333\n",
      "Epoch 917 | Batch 0/5 | Loss 2.441200\n",
      "Epoch 917 | Test Acc = 91.73% +- 2.27%\n",
      "Epoch 917 | Train Loss 2.414210844039917 | Test Acc 91.73333333333332\n",
      "Epoch 918 | Batch 0/5 | Loss 2.463710\n",
      "Epoch 918 | Test Acc = 92.00% +- 8.99%\n",
      "Epoch 918 | Train Loss 2.4094821929931642 | Test Acc 92.0\n",
      "Epoch 919 | Batch 0/5 | Loss 2.500597\n",
      "Epoch 919 | Test Acc = 83.47% +- 5.15%\n",
      "Epoch 919 | Train Loss 2.3917606353759764 | Test Acc 83.46666666666667\n",
      "Epoch 920 | Batch 0/5 | Loss 2.331304\n",
      "Epoch 920 | Test Acc = 88.27% +- 6.33%\n",
      "Epoch 920 | Train Loss 2.372004842758179 | Test Acc 88.26666666666668\n",
      "Epoch 921 | Batch 0/5 | Loss 2.365607\n",
      "Epoch 921 | Test Acc = 92.27% +- 2.99%\n",
      "Epoch 921 | Train Loss 2.3796436309814455 | Test Acc 92.26666666666668\n",
      "Epoch 922 | Batch 0/5 | Loss 2.792731\n",
      "Epoch 922 | Test Acc = 90.93% +- 3.65%\n",
      "Epoch 922 | Train Loss 2.4914953231811525 | Test Acc 90.93333333333334\n",
      "Epoch 923 | Batch 0/5 | Loss 2.470987\n",
      "Epoch 923 | Test Acc = 96.00% +- 2.22%\n",
      "Epoch 923 | Train Loss 2.4617669582366943 | Test Acc 96.0\n",
      "Epoch 924 | Batch 0/5 | Loss 2.415628\n",
      "Epoch 924 | Test Acc = 82.93% +- 13.19%\n",
      "Epoch 924 | Train Loss 2.4158037185668944 | Test Acc 82.93333333333334\n",
      "Epoch 925 | Batch 0/5 | Loss 2.681183\n",
      "Epoch 925 | Test Acc = 91.20% +- 5.05%\n",
      "Epoch 925 | Train Loss 2.4710342407226564 | Test Acc 91.2\n",
      "Epoch 926 | Batch 0/5 | Loss 2.338626\n",
      "Epoch 926 | Test Acc = 84.00% +- 11.47%\n",
      "Epoch 926 | Train Loss 2.4420230388641357 | Test Acc 84.00000000000001\n",
      "Epoch 927 | Batch 0/5 | Loss 2.300957\n",
      "Epoch 927 | Test Acc = 94.13% +- 3.19%\n",
      "Epoch 927 | Train Loss 2.379850149154663 | Test Acc 94.13333333333334\n",
      "Epoch 928 | Batch 0/5 | Loss 2.290297\n",
      "Epoch 928 | Test Acc = 93.33% +- 4.79%\n",
      "Epoch 928 | Train Loss 2.3877986431121827 | Test Acc 93.33333333333333\n",
      "Epoch 929 | Batch 0/5 | Loss 2.344562\n",
      "Epoch 929 | Test Acc = 86.67% +- 6.32%\n",
      "Epoch 929 | Train Loss 2.3683088302612303 | Test Acc 86.66666666666666\n",
      "Epoch 930 | Batch 0/5 | Loss 2.642712\n",
      "Epoch 930 | Test Acc = 86.13% +- 6.03%\n",
      "Epoch 930 | Train Loss 2.51814603805542 | Test Acc 86.13333333333335\n",
      "Epoch 931 | Batch 0/5 | Loss 2.522157\n",
      "Epoch 931 | Test Acc = 84.00% +- 7.50%\n",
      "Epoch 931 | Train Loss 2.3840842247009277 | Test Acc 84.0\n",
      "Epoch 932 | Batch 0/5 | Loss 2.628241\n",
      "Epoch 932 | Test Acc = 94.40% +- 4.08%\n",
      "Epoch 932 | Train Loss 2.536308765411377 | Test Acc 94.4\n",
      "Epoch 933 | Batch 0/5 | Loss 2.307844\n",
      "Epoch 933 | Test Acc = 95.20% +- 3.51%\n",
      "Epoch 933 | Train Loss 2.3733533382415772 | Test Acc 95.2\n",
      "Epoch 934 | Batch 0/5 | Loss 2.463705\n",
      "Epoch 934 | Test Acc = 94.93% +- 2.50%\n",
      "Epoch 934 | Train Loss 2.491806697845459 | Test Acc 94.93333333333332\n",
      "Epoch 935 | Batch 0/5 | Loss 2.557440\n",
      "Epoch 935 | Test Acc = 94.93% +- 2.60%\n",
      "Epoch 935 | Train Loss 2.458253860473633 | Test Acc 94.93333333333334\n",
      "Epoch 936 | Batch 0/5 | Loss 2.350264\n",
      "Epoch 936 | Test Acc = 93.07% +- 1.87%\n",
      "Epoch 936 | Train Loss 2.4536861896514894 | Test Acc 93.06666666666666\n",
      "Epoch 937 | Batch 0/5 | Loss 2.150843\n",
      "Epoch 937 | Test Acc = 88.80% +- 3.01%\n",
      "Epoch 937 | Train Loss 2.533806657791138 | Test Acc 88.8\n",
      "Epoch 938 | Batch 0/5 | Loss 2.250595\n",
      "Epoch 938 | Test Acc = 84.80% +- 6.12%\n",
      "Epoch 938 | Train Loss 2.315016031265259 | Test Acc 84.8\n",
      "Epoch 939 | Batch 0/5 | Loss 2.488561\n",
      "Epoch 939 | Test Acc = 87.20% +- 5.46%\n",
      "Epoch 939 | Train Loss 2.3780031204223633 | Test Acc 87.2\n",
      "Epoch 940 | Batch 0/5 | Loss 2.275897\n",
      "Epoch 940 | Test Acc = 89.87% +- 5.41%\n",
      "Epoch 940 | Train Loss 2.3256768703460695 | Test Acc 89.86666666666665\n",
      "Epoch 941 | Batch 0/5 | Loss 2.438324\n",
      "Epoch 941 | Test Acc = 93.87% +- 4.22%\n",
      "Epoch 941 | Train Loss 2.3243314266204833 | Test Acc 93.86666666666667\n",
      "Epoch 942 | Batch 0/5 | Loss 2.456849\n",
      "Epoch 942 | Test Acc = 88.27% +- 6.91%\n",
      "Epoch 942 | Train Loss 2.453237438201904 | Test Acc 88.26666666666667\n",
      "Epoch 943 | Batch 0/5 | Loss 2.400945\n",
      "Epoch 943 | Test Acc = 97.07% +- 2.50%\n",
      "Epoch 943 | Train Loss 2.3936360359191893 | Test Acc 97.06666666666668\n",
      "Epoch 944 | Batch 0/5 | Loss 2.480924\n",
      "Epoch 944 | Test Acc = 90.40% +- 5.40%\n",
      "Epoch 944 | Train Loss 2.434893989562988 | Test Acc 90.39999999999999\n",
      "Epoch 945 | Batch 0/5 | Loss 2.365084\n",
      "Epoch 945 | Test Acc = 91.20% +- 3.74%\n",
      "Epoch 945 | Train Loss 2.3775535106658934 | Test Acc 91.20000000000002\n",
      "Epoch 946 | Batch 0/5 | Loss 2.493514\n",
      "Epoch 946 | Test Acc = 89.60% +- 9.42%\n",
      "Epoch 946 | Train Loss 2.4260855197906492 | Test Acc 89.60000000000001\n",
      "Epoch 947 | Batch 0/5 | Loss 2.474170\n",
      "Epoch 947 | Test Acc = 89.33% +- 5.87%\n",
      "Epoch 947 | Train Loss 2.3508036136627197 | Test Acc 89.33333333333334\n",
      "Epoch 948 | Batch 0/5 | Loss 2.251069\n",
      "Epoch 948 | Test Acc = 92.27% +- 4.21%\n",
      "Epoch 948 | Train Loss 2.2842173099517824 | Test Acc 92.26666666666668\n",
      "Epoch 949 | Batch 0/5 | Loss 2.368642\n",
      "Epoch 949 | Test Acc = 93.87% +- 2.52%\n",
      "Epoch 949 | Train Loss 2.3498262405395507 | Test Acc 93.86666666666667\n",
      "Epoch 950 | Batch 0/5 | Loss 2.639604\n",
      "Epoch 950 | Test Acc = 89.33% +- 7.28%\n",
      "Epoch 950 | Train Loss 2.3995823860168457 | Test Acc 89.33333333333333\n",
      "Epoch 951 | Batch 0/5 | Loss 2.439663\n",
      "Epoch 951 | Test Acc = 94.93% +- 4.08%\n",
      "Epoch 951 | Train Loss 2.3941948890686033 | Test Acc 94.93333333333334\n",
      "Epoch 952 | Batch 0/5 | Loss 2.296413\n",
      "Epoch 952 | Test Acc = 88.27% +- 6.50%\n",
      "Epoch 952 | Train Loss 2.2970870971679687 | Test Acc 88.26666666666665\n",
      "Epoch 953 | Batch 0/5 | Loss 2.401165\n",
      "Epoch 953 | Test Acc = 92.80% +- 5.21%\n",
      "Epoch 953 | Train Loss 2.3755259990692137 | Test Acc 92.8\n",
      "Epoch 954 | Batch 0/5 | Loss 2.624362\n",
      "Epoch 954 | Test Acc = 83.20% +- 11.65%\n",
      "Epoch 954 | Train Loss 2.6179523944854735 | Test Acc 83.2\n",
      "Epoch 955 | Batch 0/5 | Loss 2.456777\n",
      "Epoch 955 | Test Acc = 91.47% +- 4.77%\n",
      "Epoch 955 | Train Loss 2.4369340419769285 | Test Acc 91.46666666666667\n",
      "Epoch 956 | Batch 0/5 | Loss 2.306600\n",
      "Epoch 956 | Test Acc = 92.00% +- 5.48%\n",
      "Epoch 956 | Train Loss 2.379204273223877 | Test Acc 92.00000000000001\n",
      "Epoch 957 | Batch 0/5 | Loss 2.654409\n",
      "Epoch 957 | Test Acc = 91.73% +- 3.80%\n",
      "Epoch 957 | Train Loss 2.453124761581421 | Test Acc 91.73333333333332\n",
      "Epoch 958 | Batch 0/5 | Loss 2.431188\n",
      "Epoch 958 | Test Acc = 96.00% +- 2.45%\n",
      "Epoch 958 | Train Loss 2.4380372524261475 | Test Acc 96.0\n",
      "Epoch 959 | Batch 0/5 | Loss 2.482247\n",
      "Epoch 959 | Test Acc = 95.47% +- 4.28%\n",
      "Epoch 959 | Train Loss 2.3892578125 | Test Acc 95.46666666666667\n",
      "Epoch 960 | Batch 0/5 | Loss 2.348845\n",
      "Epoch 960 | Test Acc = 90.40% +- 6.79%\n",
      "Epoch 960 | Train Loss 2.3139414310455324 | Test Acc 90.4\n",
      "Epoch 961 | Batch 0/5 | Loss 2.324013\n",
      "Epoch 961 | Test Acc = 92.00% +- 3.70%\n",
      "Epoch 961 | Train Loss 2.378918695449829 | Test Acc 92.0\n",
      "Epoch 962 | Batch 0/5 | Loss 2.455158\n",
      "Epoch 962 | Test Acc = 91.47% +- 4.71%\n",
      "Epoch 962 | Train Loss 2.4825198650360107 | Test Acc 91.46666666666667\n",
      "Epoch 963 | Batch 0/5 | Loss 2.396325\n",
      "Epoch 963 | Test Acc = 86.93% +- 8.41%\n",
      "Epoch 963 | Train Loss 2.444837045669556 | Test Acc 86.93333333333332\n",
      "Epoch 964 | Batch 0/5 | Loss 2.492550\n",
      "Epoch 964 | Test Acc = 94.40% +- 4.34%\n",
      "Epoch 964 | Train Loss 2.468427133560181 | Test Acc 94.4\n",
      "Epoch 965 | Batch 0/5 | Loss 2.188894\n",
      "Epoch 965 | Test Acc = 90.93% +- 3.26%\n",
      "Epoch 965 | Train Loss 2.3081490993499756 | Test Acc 90.93333333333334\n",
      "Epoch 966 | Batch 0/5 | Loss 2.250112\n",
      "Epoch 966 | Test Acc = 90.40% +- 5.09%\n",
      "Epoch 966 | Train Loss 2.3882541179656984 | Test Acc 90.4\n",
      "Epoch 967 | Batch 0/5 | Loss 2.338484\n",
      "Epoch 967 | Test Acc = 90.40% +- 6.87%\n",
      "Epoch 967 | Train Loss 2.4080300331115723 | Test Acc 90.4\n",
      "Epoch 968 | Batch 0/5 | Loss 2.340262\n",
      "Epoch 968 | Test Acc = 92.00% +- 6.73%\n",
      "Epoch 968 | Train Loss 2.383177614212036 | Test Acc 92.0\n",
      "Epoch 969 | Batch 0/5 | Loss 2.502750\n",
      "Epoch 969 | Test Acc = 89.33% +- 4.85%\n",
      "Epoch 969 | Train Loss 2.431165885925293 | Test Acc 89.33333333333333\n",
      "Epoch 970 | Batch 0/5 | Loss 2.190565\n",
      "Epoch 970 | Test Acc = 87.73% +- 9.18%\n",
      "Epoch 970 | Train Loss 2.4055922508239744 | Test Acc 87.73333333333332\n",
      "Epoch 971 | Batch 0/5 | Loss 2.356732\n",
      "Epoch 971 | Test Acc = 82.40% +- 11.63%\n",
      "Epoch 971 | Train Loss 2.5080374240875245 | Test Acc 82.4\n",
      "Epoch 972 | Batch 0/5 | Loss 2.330459\n",
      "Epoch 972 | Test Acc = 90.93% +- 4.70%\n",
      "Epoch 972 | Train Loss 2.3730210304260253 | Test Acc 90.93333333333332\n",
      "Epoch 973 | Batch 0/5 | Loss 2.245871\n",
      "Epoch 973 | Test Acc = 93.60% +- 6.02%\n",
      "Epoch 973 | Train Loss 2.28648042678833 | Test Acc 93.60000000000001\n",
      "Epoch 974 | Batch 0/5 | Loss 2.310427\n",
      "Epoch 974 | Test Acc = 88.53% +- 3.51%\n",
      "Epoch 974 | Train Loss 2.319283056259155 | Test Acc 88.53333333333333\n",
      "Epoch 975 | Batch 0/5 | Loss 2.492238\n",
      "Epoch 975 | Test Acc = 94.13% +- 2.82%\n",
      "Epoch 975 | Train Loss 2.428684377670288 | Test Acc 94.13333333333334\n",
      "Epoch 976 | Batch 0/5 | Loss 2.369971\n",
      "Epoch 976 | Test Acc = 90.93% +- 4.58%\n",
      "Epoch 976 | Train Loss 2.3830379962921144 | Test Acc 90.93333333333332\n",
      "Epoch 977 | Batch 0/5 | Loss 2.632115\n",
      "Epoch 977 | Test Acc = 90.13% +- 6.51%\n",
      "Epoch 977 | Train Loss 2.3913174152374266 | Test Acc 90.13333333333334\n",
      "Epoch 978 | Batch 0/5 | Loss 2.479805\n",
      "Epoch 978 | Test Acc = 89.87% +- 3.35%\n",
      "Epoch 978 | Train Loss 2.368574619293213 | Test Acc 89.86666666666666\n",
      "Epoch 979 | Batch 0/5 | Loss 2.367764\n",
      "Epoch 979 | Test Acc = 90.93% +- 4.87%\n",
      "Epoch 979 | Train Loss 2.3476097106933596 | Test Acc 90.93333333333334\n",
      "Epoch 980 | Batch 0/5 | Loss 2.575465\n",
      "Epoch 980 | Test Acc = 78.13% +- 11.03%\n",
      "Epoch 980 | Train Loss 2.3919846057891845 | Test Acc 78.13333333333334\n",
      "Epoch 981 | Batch 0/5 | Loss 2.268741\n",
      "Epoch 981 | Test Acc = 87.73% +- 6.33%\n",
      "Epoch 981 | Train Loss 2.305739498138428 | Test Acc 87.73333333333333\n",
      "Epoch 982 | Batch 0/5 | Loss 2.219665\n",
      "Epoch 982 | Test Acc = 90.13% +- 10.78%\n",
      "Epoch 982 | Train Loss 2.3995317459106444 | Test Acc 90.13333333333335\n",
      "Epoch 983 | Batch 0/5 | Loss 2.472210\n",
      "Epoch 983 | Test Acc = 88.53% +- 10.47%\n",
      "Epoch 983 | Train Loss 2.342359161376953 | Test Acc 88.53333333333333\n",
      "Epoch 984 | Batch 0/5 | Loss 2.380614\n",
      "Epoch 984 | Test Acc = 93.07% +- 3.42%\n",
      "Epoch 984 | Train Loss 2.3904794692993163 | Test Acc 93.06666666666666\n",
      "Epoch 985 | Batch 0/5 | Loss 2.418691\n",
      "Epoch 985 | Test Acc = 88.00% +- 5.33%\n",
      "Epoch 985 | Train Loss 2.327056312561035 | Test Acc 88.0\n",
      "Epoch 986 | Batch 0/5 | Loss 2.269264\n",
      "Epoch 986 | Test Acc = 84.53% +- 5.26%\n",
      "Epoch 986 | Train Loss 2.3365931510925293 | Test Acc 84.53333333333333\n",
      "Epoch 987 | Batch 0/5 | Loss 2.471192\n",
      "Epoch 987 | Test Acc = 94.13% +- 1.90%\n",
      "Epoch 987 | Train Loss 2.3356768608093263 | Test Acc 94.13333333333334\n",
      "Epoch 988 | Batch 0/5 | Loss 2.360707\n",
      "Epoch 988 | Test Acc = 85.60% +- 12.79%\n",
      "Epoch 988 | Train Loss 2.312386989593506 | Test Acc 85.6\n",
      "Epoch 989 | Batch 0/5 | Loss 2.467211\n",
      "Epoch 989 | Test Acc = 89.07% +- 2.90%\n",
      "Epoch 989 | Train Loss 2.337353515625 | Test Acc 89.06666666666668\n",
      "Epoch 990 | Batch 0/5 | Loss 2.273451\n",
      "Epoch 990 | Test Acc = 89.60% +- 7.37%\n",
      "Epoch 990 | Train Loss 2.4383244037628176 | Test Acc 89.6\n",
      "Epoch 991 | Batch 0/5 | Loss 2.380515\n",
      "Epoch 991 | Test Acc = 94.67% +- 2.67%\n",
      "Epoch 991 | Train Loss 2.360772180557251 | Test Acc 94.66666666666667\n",
      "Epoch 992 | Batch 0/5 | Loss 2.290103\n",
      "Epoch 992 | Test Acc = 85.33% +- 6.73%\n",
      "Epoch 992 | Train Loss 2.360869836807251 | Test Acc 85.33333333333334\n",
      "Epoch 993 | Batch 0/5 | Loss 2.450362\n",
      "Epoch 993 | Test Acc = 91.73% +- 7.33%\n",
      "Epoch 993 | Train Loss 2.317738246917725 | Test Acc 91.73333333333332\n",
      "Epoch 994 | Batch 0/5 | Loss 2.601833\n",
      "Epoch 994 | Test Acc = 84.00% +- 2.96%\n",
      "Epoch 994 | Train Loss 2.435438537597656 | Test Acc 84.0\n",
      "Epoch 995 | Batch 0/5 | Loss 2.340294\n",
      "Epoch 995 | Test Acc = 90.67% +- 5.17%\n",
      "Epoch 995 | Train Loss 2.320143795013428 | Test Acc 90.66666666666667\n",
      "Epoch 996 | Batch 0/5 | Loss 2.378007\n",
      "Epoch 996 | Test Acc = 92.00% +- 2.22%\n",
      "Epoch 996 | Train Loss 2.3164541244506838 | Test Acc 92.0\n",
      "Epoch 997 | Batch 0/5 | Loss 2.464661\n",
      "Epoch 997 | Test Acc = 91.20% +- 6.21%\n",
      "Epoch 997 | Train Loss 2.3866965770721436 | Test Acc 91.19999999999999\n",
      "Epoch 998 | Batch 0/5 | Loss 2.437550\n",
      "Epoch 998 | Test Acc = 91.47% +- 3.88%\n",
      "Epoch 998 | Train Loss 2.372130250930786 | Test Acc 91.46666666666667\n",
      "Epoch 999 | Batch 0/5 | Loss 2.601704\n",
      "Epoch 999 | Test Acc = 94.93% +- 3.17%\n",
      "Epoch 999 | Train Loss 2.3497820854187013 | Test Acc 94.93333333333334\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x250 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model(**parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fewshotbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
